{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41af919f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:29.470575Z",
     "iopub.status.busy": "2024-05-05T22:56:29.470237Z",
     "iopub.status.idle": "2024-05-05T22:56:30.206223Z",
     "shell.execute_reply": "2024-05-05T22:56:30.205242Z"
    },
    "papermill": {
     "duration": 0.746684,
     "end_time": "2024-05-05T22:56:30.208794",
     "exception": false,
     "start_time": "2024-05-05T22:56:29.462110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451e37a",
   "metadata": {
    "papermill": {
     "duration": 0.006273,
     "end_time": "2024-05-05T22:56:30.221825",
     "exception": false,
     "start_time": "2024-05-05T22:56:30.215552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb155d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:30.235386Z",
     "iopub.status.busy": "2024-05-05T22:56:30.234968Z",
     "iopub.status.idle": "2024-05-05T22:56:52.917868Z",
     "shell.execute_reply": "2024-05-05T22:56:52.916937Z"
    },
    "papermill": {
     "duration": 22.692668,
     "end_time": "2024-05-05T22:56:52.920522",
     "exception": false,
     "start_time": "2024-05-05T22:56:30.227854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\r\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: einops\r\n",
      "Successfully installed einops-0.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "from skimage.morphology import label\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa5e70",
   "metadata": {
    "papermill": {
     "duration": 0.007705,
     "end_time": "2024-05-05T22:56:52.936583",
     "exception": false,
     "start_time": "2024-05-05T22:56:52.928878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **UTILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72413d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:52.954599Z",
     "iopub.status.busy": "2024-05-05T22:56:52.954063Z",
     "iopub.status.idle": "2024-05-05T22:56:53.065662Z",
     "shell.execute_reply": "2024-05-05T22:56:53.064833Z"
    },
    "papermill": {
     "duration": 0.123662,
     "end_time": "2024-05-05T22:56:53.068142",
     "exception": false,
     "start_time": "2024-05-05T22:56:52.944480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNet\n",
    "Common utility functions and classes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Base Configuration class\n",
    "# Don't use this class directly. Instead, sub-class it and override\n",
    "\n",
    "class Config():\n",
    "    name = None\n",
    "\n",
    "    img_width = 256\n",
    "    img_height = 256\n",
    "\n",
    "    img_channel = 3\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    learning_momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    shuffle = False\n",
    "\n",
    "    def __init__(self):\n",
    "        self.IMAGE_SHAPE = np.array([\n",
    "            self.img_width, self.img_height, self.img_channel\n",
    "        ])\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Configurations\n",
    "\n",
    "class Option(Config):\n",
    "    \"\"\"Configuration for training on Kaggle Data Science Bowl 2018\n",
    "    Derived from the base Config class and overrides specific values\n",
    "    \"\"\"\n",
    "    name = \"DSB2018\"\n",
    "    img_height = 256\n",
    "    img_width = 256\n",
    "    # root dir of training and validation set\n",
    "    root_dir = 'combined'\n",
    "\n",
    "    # root dir of testing set\n",
    "    test_dir = 'testing_data'\n",
    "\n",
    "    # save segmenting results (prediction masks) to this folder\n",
    "    results_dir = 'results'\n",
    "\n",
    "    num_workers = 1  # number of threads for data loading\n",
    "    shuffle = True  # shuffle the data set\n",
    "    batch_size = 24  # GTX1060 3G Memory\n",
    "    epochs = 100  #number of epochs to train\n",
    "    is_train = True  # True for training, False for making prediction\n",
    "    save_model = True  # True for saving the model, False for not saving the model\n",
    "\n",
    "    n_gpu = 1  # number of GPUs\n",
    "\n",
    "    learning_rate = 0.0001  # learning rate\n",
    "    weight_decay = 1e-4  # weight decay\n",
    "\n",
    "    pin_memory = True  # use pinned (page-locked) memory. when using CUDA, set to True\n",
    "\n",
    "    is_cuda = torch.cuda.is_available()  # True --> GPU\n",
    "    num_gpus = torch.cuda.device_count()  # number of GPUs\n",
    "    checkpoint_dir = \"./checkpoint\"  # dir to save checkpoints\n",
    "    dtype = torch.cuda.FloatTensor if is_cuda else torch.Tensor  # data type\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Dataset orgnization:\n",
    "Read images and masks, combine separated mask into one\n",
    "Write images and combined masks into specific folder\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "    \"\"\"\n",
    "    Initialize image parameters from DSB2018Config class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stage1_train_src, stage1_train_dest, stage1_test_src, stage1_test_dest):\n",
    "        self.opt = Option\n",
    "        self.stage1_train_src = stage1_train_src\n",
    "        self.stage1_train_dest = stage1_train_dest\n",
    "        self.stage1_test_src = stage1_test_src\n",
    "        self.stage1_test_dest = stage1_test_dest\n",
    "\n",
    "    # Combine all separated masks into one mask\n",
    "    def assemble_masks(self, path):\n",
    "        # mask = np.zeros((self.config.IMG_HEIGHT, self.config.IMG_WIDTH), dtype=np.uint8)\n",
    "        mask = None\n",
    "        for i, mask_file in enumerate(next(os.walk(os.path.join(path, 'masks')))[2]):\n",
    "            mask_ = Image.open(os.path.join(path, 'masks', mask_file)).convert(\"RGB\")\n",
    "            # mask_ = mask_.resize((self.config.IMG_HEIGHT, self.config.IMG_WIDTH))\n",
    "            mask_ = np.asarray(mask_)\n",
    "            if i == 0:\n",
    "                mask = mask_\n",
    "                continue\n",
    "            mask = mask | mask_\n",
    "        # mask = np.expand_dims(mask, axis=-1)\n",
    "        return mask\n",
    "\n",
    "    # read all training data and save them to other folder\n",
    "    def prepare_training_data(self):\n",
    "        # get imageId\n",
    "        train_ids = next(os.walk(self.stage1_train_src))[1]\n",
    "\n",
    "        # read training data\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        print('reading training data starts...')\n",
    "        sys.stdout.flush()\n",
    "        for n, id_ in tqdm(enumerate(train_ids)):\n",
    "            path = os.path.join(self.stage1_train_src, id_)\n",
    "            dest = os.path.join(self.stage1_train_dest, id_)\n",
    "            img = Image.open(os.path.join(path, 'images', id_ + '.png')).convert(\"RGB\")\n",
    "            mask = self.assemble_masks(path)\n",
    "            os.mkdir(dest)\n",
    "            img.save(os.path.join(dest, 'image.png'))\n",
    "            Image.fromarray(mask).save(os.path.join(dest, 'mask.png'))\n",
    "\n",
    "        print('reading training data done...')\n",
    "\n",
    "    # read testing data and save them to other folder\n",
    "    def prepare_testing_data(self):\n",
    "        # get imageId\n",
    "        test_ids = next(os.walk(self.stage1_test_src))[1]\n",
    "        # read training data\n",
    "        print('reading testing data starts...')\n",
    "        sys.stdout.flush()\n",
    "        for n, id_ in tqdm(enumerate(test_ids)):\n",
    "            path = os.path.join(self.stage1_test_src, id_, 'images', id_ + '.png')\n",
    "            dest = os.path.join(self.stage1_test_dest, id_)\n",
    "            if not os.path.exists(dest):\n",
    "                os.mkdir(dest)\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            img.save(os.path.join(dest, 'image.png'))\n",
    "\n",
    "        print('reading testing data done...')\n",
    "\n",
    "\n",
    "def compute_iou(predictions, img_ids, val_loader):\n",
    "    \"\"\"\n",
    "    compute IOU between two combined masks, this does not follow kaggle's evaluation\n",
    "    :return: IOU, between 0 and 1\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    for i in range(0, len(img_ids)):\n",
    "        pred = predictions[i]\n",
    "        img_id = img_ids[i]\n",
    "        mask_path = os.path.join(Option.root_dir, img_id, 'mask.png')\n",
    "        mask = np.asarray(Image.open(mask_path).convert('L'), dtype=np.bool_)\n",
    "        union = np.sum(np.logical_or(mask, pred))\n",
    "        intersection = np.sum(np.logical_and(mask, pred))\n",
    "        iou = intersection / union\n",
    "        ious.append(iou)\n",
    "    df = pd.DataFrame({'img_id': img_ids, 'iou': ious})\n",
    "    df.to_csv('IOU.csv', index=False)\n",
    "\n",
    "\n",
    "# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def prob_to_rles(x, cutoff=0.5):\n",
    "    lab_img = label(x > cutoff)\n",
    "    for i in range(1, lab_img.max() + 1):\n",
    "        yield rle_encoding(lab_img == i)\n",
    "\n",
    "\n",
    "def encode_and_save(preds_test_upsampled, test_ids):\n",
    "    \"\"\"\n",
    "    Use run-length-encoding encode the prediction masks and save to csv file for submitting\n",
    "    :param preds_test_upsampled: list, for each elements, numpy array (Width, Height)\n",
    "    :param test_ids: list, for each elements, image id\n",
    "    :return:\n",
    "        save to csv file\n",
    "    \"\"\"\n",
    "    # save as imgs\n",
    "    for i in range(0, len(test_ids)):\n",
    "        path = os.path.join(Option.results_dir, test_ids[i])\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        # Image.fromarray(preds_test_upsampled[i]).save(os.path.join(path,'prediction.png'))\n",
    "        plt.imsave(os.path.join(path, 'prediction.png'), preds_test_upsampled[i], cmap='gray')\n",
    "    # save as encoding\n",
    "    new_test_ids = []\n",
    "    rles = []\n",
    "    for n, id_ in enumerate(test_ids):\n",
    "        rle = list(prob_to_rles(preds_test_upsampled[n]))\n",
    "        rles.extend(rle)\n",
    "        new_test_ids.extend([id_] * len(rle))\n",
    "\n",
    "    sub = pd.DataFrame()\n",
    "    sub['ImageId'] = new_test_ids\n",
    "    sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n",
    "    sub.to_csv('sub-dsbowl2018.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc281de",
   "metadata": {
    "papermill": {
     "duration": 0.006839,
     "end_time": "2024-05-05T22:56:53.082290",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.075451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b772274e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:53.098221Z",
     "iopub.status.busy": "2024-05-05T22:56:53.097561Z",
     "iopub.status.idle": "2024-05-05T22:56:53.115041Z",
     "shell.execute_reply": "2024-05-05T22:56:53.113996Z"
    },
    "papermill": {
     "duration": 0.027861,
     "end_time": "2024-05-05T22:56:53.117270",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.089409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# IMAGE_HEIGHT = 256\n",
    "# IMAGE_WIDTH = 256\n",
    "\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "\n",
    "opt = Option()\n",
    "\n",
    "transform0 = A.Compose([\n",
    "    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "    A.Normalize(\n",
    "        mean=[0.0, 0.0, 0.0],\n",
    "        std=[1.0, 1.0, 1.0],\n",
    "        max_pixel_value=255.0,\n",
    "    ),\n",
    "    # ToTensorV2()\n",
    "])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
    "    A.HorizontalFlip(p=0.25),\n",
    "    A.VerticalFlip(p=0.25),\n",
    "    A.Rotate(p=0.25),\n",
    "    A.Normalize(\n",
    "        mean=[0.0, 0.0, 0.0],\n",
    "        std=[1.0, 1.0, 1.0],\n",
    "        max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "class Dataset_Class(Dataset):\n",
    "    def __init__(self, root_dir, transform=transform0):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.folder_list = os.listdir(self.root_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.root_dir + '/' + self.folder_list[index] + '/' + \"image.png\")).convert(\"RGB\")\n",
    "        mask1 = Image.open(os.path.join(self.root_dir + '/' + self.folder_list[index] + '/' + \"mask.png\")).convert('L')\n",
    "        image = np.array(img, dtype=np.float32)\n",
    "        mask = np.array(mask1, dtype=np.float32)\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "        # mask1 = np.asarray(mask1)\n",
    "        # k = np.expand_dims(mask1, axis=-1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "        \n",
    "        image = torch.from_numpy(image)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        image = image.type(opt.dtype)\n",
    "        mask = mask.type(opt.dtype)\n",
    "        image = torch.reshape(image, (3, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        mask = torch.reshape(mask, (1, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        mask = mask/255.0\n",
    "        return image, mask\n",
    "    \n",
    "    \n",
    "def get_train_loader(root_training_dir='/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data'):\n",
    "    data = Dataset_Class(root_dir=root_training_dir)\n",
    "    \n",
    "    BATCH_SIZE = opt.batch_size\n",
    "    dataloader_train = DataLoader(\n",
    "        data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(\"TRAIN DATALOADER DONE\")\n",
    "    return dataloader_train\n",
    "\n",
    "\n",
    "def get_val_loader(root_val_dir='/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/validation_data'):\n",
    "    \n",
    "    data = Dataset_Class(root_dir=root_val_dir)\n",
    "    \n",
    "    BATCH_SIZE = opt.batch_size\n",
    "    dataloader_train = DataLoader(\n",
    "        data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(\"VAL DATALOADER DONE\")\n",
    "    return dataloader_train\n",
    "\n",
    "\n",
    "# def get_test_loader(root_test_dir='testing_data'):\n",
    "    \n",
    "#     data = Dataset_Class(root_dir=root_test_dir)\n",
    "#     BATCH_SIZE = opt.batch_size\n",
    "#     dataloader_train = DataLoader(\n",
    "#         data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f7758",
   "metadata": {
    "papermill": {
     "duration": 0.006816,
     "end_time": "2024-05-05T22:56:53.131194",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.124378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1294dd93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:53.146504Z",
     "iopub.status.busy": "2024-05-05T22:56:53.146080Z",
     "iopub.status.idle": "2024-05-05T22:56:53.221770Z",
     "shell.execute_reply": "2024-05-05T22:56:53.220947Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.085976,
     "end_time": "2024-05-05T22:56:53.223844",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.137868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "''' when filter kernel= 3x3, padding=1 makes in&out matrix same size'''\n",
    "\n",
    "class MHCA(nn.Module):\n",
    "    def __init__(self, n_feats, ratio):\n",
    "        \"\"\"\n",
    "        MHCA spatial-channel attention module.\n",
    "        :param n_feats: The number of filter of the input.\n",
    "        :param ratio: Channel reduction ratio.\n",
    "        \"\"\"\n",
    "        super(MHCA, self).__init__()\n",
    "\n",
    "        out_channels = int(n_feats // ratio)\n",
    "\n",
    "        head_1 = [\n",
    "            nn.Conv2d(in_channels=n_feats, out_channels=out_channels, kernel_size=1, padding=0, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=n_feats, kernel_size=1, padding=0, bias=True)\n",
    "        ]\n",
    "\n",
    "        kernel_size_sam = 3\n",
    "        head_2 = [\n",
    "            nn.Conv2d(in_channels=n_feats, out_channels=out_channels, kernel_size=kernel_size_sam, padding=0, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=out_channels, out_channels=n_feats, kernel_size=kernel_size_sam, padding=0, bias=True)\n",
    "        ]\n",
    "\n",
    "        kernel_size_sam_2 = 5\n",
    "        head_3 = [\n",
    "            nn.Conv2d(in_channels=n_feats, out_channels=out_channels, kernel_size=kernel_size_sam_2, padding=0, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=out_channels, out_channels=n_feats, kernel_size=kernel_size_sam_2, padding=0, bias=True)\n",
    "        ]\n",
    "\n",
    "        self.head_1 = nn.Sequential(*head_1)\n",
    "        self.head_2 = nn.Sequential(*head_2)\n",
    "        self.head_3 = nn.Sequential(*head_3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_h1 = self.head_1(x)\n",
    "        res_h2 = self.head_2(x)\n",
    "        res_h3 = self.head_3(x)\n",
    "        m_c = self.sigmoid(res_h1 + res_h2 + res_h3)\n",
    "        res = x * m_c\n",
    "        return res\n",
    "    \n",
    "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def down_pooling():\n",
    "    return nn.MaxPool2d(2)\n",
    "\n",
    "\n",
    "def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "# UNet class\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, input_channels=3, nclasses=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNet, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        in_channels = input_channels\n",
    "        out_channels = nclasses\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "class UNet2(nn.Module):\n",
    "    def __init__(self, input_channels, nclasses):\n",
    "        super().__init__()\n",
    "        # go down\n",
    "        self.conv1 = conv_bn_relu(input_channels, 16)\n",
    "        self.conv2 = conv_bn_relu(16, 32)\n",
    "        self.conv3 = conv_bn_relu(32, 64)\n",
    "        self.conv4 = conv_bn_relu(64, 128)\n",
    "        self.conv5 = conv_bn_relu(128, 256)\n",
    "        self.down_pooling = nn.MaxPool2d(2)\n",
    "\n",
    "        # go up\n",
    "        self.up_pool6 = up_pooling(256, 128)\n",
    "        self.conv6 = conv_bn_relu(256, 128)\n",
    "        self.up_pool7 = up_pooling(128, 64)\n",
    "        self.conv7 = conv_bn_relu(128, 64)\n",
    "        self.up_pool8 = up_pooling(64, 32)\n",
    "        self.conv8 = conv_bn_relu(64, 32)\n",
    "        self.up_pool9 = up_pooling(32, 16)\n",
    "        self.conv9 = conv_bn_relu(32, 16)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(16, nclasses, 1)\n",
    "\n",
    "        # test weight init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # normalize input data\n",
    "        x = x / 255.\n",
    "        # go down\n",
    "        x1 = self.conv1(x)\n",
    "        print(\"X1 shape: \", x1.size())\n",
    "        p1 = self.down_pooling(x1)\n",
    "        x2 = self.conv2(p1)\n",
    "        print(\"X2 Shape: \", x2.size())\n",
    "        p2 = self.down_pooling(x2)\n",
    "        x3 = self.conv3(p2)\n",
    "        print(\"X3 Shape: \", x3.size())\n",
    "        p3 = self.down_pooling(x3)\n",
    "        x4 = self.conv4(p3)\n",
    "        print(\"X4 Shape: \", x4.size())\n",
    "        p4 = self.down_pooling(x4)\n",
    "        x5 = self.conv5(p4)\n",
    "        print(\"X5 Shape: \", x5.size())\n",
    "\n",
    "        # go up\n",
    "        p6 = self.up_pool6(x5)\n",
    "        x6 = torch.cat([p6, x4], dim=1)\n",
    "        x6 = self.conv6(x6)\n",
    "\n",
    "        p7 = self.up_pool7(x6)\n",
    "        x7 = torch.cat([p7, x3], dim=1)\n",
    "        x7 = self.conv7(x7)\n",
    "\n",
    "        p8 = self.up_pool8(x7)\n",
    "        x8 = torch.cat([p8, x2], dim=1)\n",
    "        x8 = self.conv8(x8)\n",
    "\n",
    "        p9 = self.up_pool9(x8)\n",
    "        x9 = torch.cat([p9, x1], dim=1)\n",
    "        x9 = self.conv9(x9)\n",
    "\n",
    "        output = self.conv10(x9) \n",
    "        output = F.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Code for Attention UNET\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self, input_channels=3, nclasses=1):\n",
    "        super(AttU_Net, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=input_channels, ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64, ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128, ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256, ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512, ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
    "        self.Att5 = Attention_block(F_g=512, F_l=512, F_int=256)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
    "        self.Att4 = Attention_block(F_g=256, F_l=256, F_int=128)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
    "        self.Att3 = Attention_block(F_g=128, F_l=128, F_int=64)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
    "        self.Att2 = Attention_block(F_g=64, F_l=64, F_int=32)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, nclasses, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5, x=x4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=x3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=x2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=x1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Code for R2_UNET\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self, ch_out, t=2):\n",
    "        super(Recurrent_block, self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i == 0:\n",
    "                x1 = self.conv(x)\n",
    "\n",
    "            x1 = self.conv(x + x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "class RRCNN_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, t=2):\n",
    "        super(RRCNN_block, self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out, t=t),\n",
    "            Recurrent_block(ch_out, t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x + x1\n",
    "\n",
    "\n",
    "class R2U_Net(nn.Module):\n",
    "    def __init__(self, input_channels=3, nclasses=1, t=2):\n",
    "        super(R2U_Net, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.RRCNN1 = RRCNN_block(ch_in=input_channels, ch_out=64, t=t)\n",
    "\n",
    "        self.RRCNN2 = RRCNN_block(ch_in=64, ch_out=128, t=t)\n",
    "\n",
    "        self.RRCNN3 = RRCNN_block(ch_in=128, ch_out=256, t=t)\n",
    "\n",
    "        self.RRCNN4 = RRCNN_block(ch_in=256, ch_out=512, t=t)\n",
    "\n",
    "        self.RRCNN5 = RRCNN_block(ch_in=512, ch_out=1024, t=t)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
    "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512, t=t)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
    "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256, t=t)\n",
    "\n",
    "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
    "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128, t=t)\n",
    "\n",
    "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
    "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64, t=t)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, nclasses, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding path\n",
    "        x1 = self.RRCNN1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.RRCNN2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.RRCNN3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.RRCNN4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.RRCNN5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_RRCNN5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_RRCNN4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_RRCNN3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_RRCNN2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4868b1f",
   "metadata": {
    "papermill": {
     "duration": 0.006441,
     "end_time": "2024-05-05T22:56:53.237611",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.231170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trans-UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ebd10c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:53.252647Z",
     "iopub.status.busy": "2024-05-05T22:56:53.252332Z",
     "iopub.status.idle": "2024-05-05T22:56:53.318635Z",
     "shell.execute_reply": "2024-05-05T22:56:53.317859Z"
    },
    "papermill": {
     "duration": 0.076451,
     "end_time": "2024-05-05T22:56:53.320729",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.244278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modules\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_k = nn.Linear(dim, inner_dim , bias=False)\n",
    "        self.to_v = nn.Linear(dim, inner_dim , bias = False)\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x_qkv):\n",
    "        b, n, _, h = *x_qkv.shape, self.heads\n",
    "\n",
    "        k = self.to_k(x_qkv)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        v = self.to_v(x_qkv)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        q = self.to_q(x_qkv[:, 0].unsqueeze(1))\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "Vision Transformer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_num = head_num\n",
    "        self.dk = (embedding_dim // head_num) ** (1 / 2)\n",
    "\n",
    "        self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
    "        self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = self.qkv_layer(x)\n",
    "\n",
    "        query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.head_num))\n",
    "        energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n",
    "\n",
    "        x = rearrange(x, \"b h t d -> b t (h d)\")\n",
    "        x = self.out_attention(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_dim, embedding_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp_layers(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
    "        self.mlp = MLP(embedding_dim, mlp_dim)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _x = self.multi_head_attention(x)\n",
    "        _x = self.dropout(_x)\n",
    "        x = x + _x\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        _x = self.mlp(x)\n",
    "        x = x + _x\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_blocks = nn.ModuleList(\n",
    "            [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_block in self.layer_blocks:\n",
    "            x = layer_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, embedding_dim, head_num, mlp_dim,\n",
    "                 block_num, patch_dim, classification=True, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_dim = patch_dim\n",
    "        self.classification = classification\n",
    "        self.num_tokens = (img_dim // patch_dim) ** 2\n",
    "        print(f'Num of tokens: {self.num_tokens}')\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "        print(f'Token dimension: {self.token_dim}')\n",
    "\n",
    "        self.projection = nn.Linear(self.token_dim, embedding_dim)\n",
    "        self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n",
    "        print(f'Embedding shape: {self.embedding.shape}')\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n",
    "\n",
    "        if self.classification:\n",
    "            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_patches = rearrange(x,\n",
    "                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.patch_dim, patch_y=self.patch_dim)\n",
    "\n",
    "        batch_size, tokens, _ = img_patches.shape\n",
    "\n",
    "        project = self.projection(img_patches)\n",
    "        token = repeat(self.cls_token, 'b ... -> (b batch_size) ...',\n",
    "                       batch_size=batch_size)\n",
    "\n",
    "        patches = torch.cat([token, project], dim=1)\n",
    "        patches += self.embedding[:tokens + 1, :]\n",
    "\n",
    "        x = self.dropout(patches)\n",
    "        x = self.transformer(x)\n",
    "        x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Trans-UNet\n",
    "\"\"\"\n",
    "class EncoderBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        width = int(out_channels * (base_width / 64))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_down = self.downsample(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = x + x_down\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_concat=None):\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        if x_concat is not None:\n",
    "            x = torch.cat([x_concat, x], dim=1)\n",
    "\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
    "        self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
    "        self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
    "\n",
    "        self.vit_img_dim = img_dim // patch_dim\n",
    "        print(f'vit_img_dim = img_dim//patch_dim\\n={img_dim}/{patch_dim} = {self.vit_img_dim}')\n",
    "        self.vit = ViT(self.vit_img_dim, out_channels * 8, out_channels * 8,\n",
    "                       head_num, mlp_dim, block_num, patch_dim=1, classification=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(f'Shape of x: {x.shape}')\n",
    "        x = self.conv1(x)\n",
    "#         print(f'Shape of x after conv1: {x.shape}')\n",
    "        x = self.norm1(x)\n",
    "#         print(f'Shape after norm1: {x.shape}')\n",
    "        x1 = self.relu(x)\n",
    "\n",
    "        x2 = self.encoder1(x1)\n",
    "#         print(f'Shape of x after encoder1: {x.shape}')\n",
    "        x3 = self.encoder2(x2)\n",
    "#         print(f'Shape of x after encoder2: {x.shape}')\n",
    "        x = self.encoder3(x3)\n",
    "#         print(f'Shape of x after encoder3: {x.shape}')\n",
    "        x = self.vit(x)\n",
    "#         print(f'Shape of x after vit: {x.shape}')\n",
    "        x = rearrange(x, \"b (x y) c -> b c x y\", x=self.vit_img_dim, y=self.vit_img_dim)\n",
    "#         print(f'Shape of x after vit rearrangement: {x.shape}')\n",
    "        x = self.conv2(x)\n",
    "#         print(f'Shape of x after conv2 on vit-rearranged output: {x.shape}')\n",
    "        x = self.norm2(x)\n",
    "#         print(f'Shape of x after norm2: {x.shape}')\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x, x1, x2, x3\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels, class_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n",
    "        self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n",
    "        self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n",
    "        self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, x1, x2, x3):\n",
    "#         print('Entered Decoder')\n",
    "        x = self.decoder1(x, x3)\n",
    "        x = self.decoder2(x, x2)\n",
    "        x = self.decoder3(x, x1)\n",
    "        x = self.decoder4(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(img_dim, in_channels, out_channels,\n",
    "                               head_num, mlp_dim, block_num, patch_dim)\n",
    "\n",
    "        self.decoder = Decoder(out_channels, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x1, x2, x3 = self.encoder(x)\n",
    "        x = self.decoder(x, x1, x2, x3)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fa4bf",
   "metadata": {
    "papermill": {
     "duration": 0.006838,
     "end_time": "2024-05-05T22:56:53.334495",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.327657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metrics Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677eca35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:53.350871Z",
     "iopub.status.busy": "2024-05-05T22:56:53.350260Z",
     "iopub.status.idle": "2024-05-05T22:56:53.366111Z",
     "shell.execute_reply": "2024-05-05T22:56:53.365339Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.026605,
     "end_time": "2024-05-05T22:56:53.368126",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.341521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accuracy(SR, GT, threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT == torch.max(GT)\n",
    "    corr = torch.sum(SR == GT)\n",
    "    tensor_size = SR.size(0) * SR.size(1) * SR.size(2) * SR.size(3)\n",
    "    acc = float(corr) / float(tensor_size)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_sensitivity(SR, GT, threshold=0.5):\n",
    "    # Sensitivity == Recall\n",
    "    SR = SR > threshold\n",
    "    GT = GT > threshold\n",
    "\n",
    "    # TP : True Positive\n",
    "    # FN : False Negative\n",
    "    TP = torch.logical_and(SR, GT)\n",
    "    FN = torch.logical_and((SR == 0), (GT == 1))\n",
    "\n",
    "    SE = float(torch.sum(TP)) / (float(torch.sum(TP + FN)) + 1e-6)\n",
    "\n",
    "    return SE\n",
    "\n",
    "\n",
    "def get_specificity(SR, GT, threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT > threshold\n",
    "\n",
    "    # TN : True Negative\n",
    "    # FP : False Positive\n",
    "    TN = torch.logical_and((SR == 0), (GT == 0))\n",
    "    FP = torch.logical_and((SR == 1), (GT == 0))\n",
    "\n",
    "    SP = float(torch.sum(TN)) / (float(torch.sum(TN + FP)) + 1e-6)\n",
    "\n",
    "    return SP\n",
    "\n",
    "\n",
    "def get_precision(SR, GT, threshold=0.5):\n",
    "    SR = SR > threshold\n",
    "    GT = GT > threshold\n",
    "\n",
    "    # TP : True Positive\n",
    "    # FP : False Positive\n",
    "    TP = torch.logical_and(SR, GT)\n",
    "    P = SR\n",
    "\n",
    "    PC = float(torch.sum(TP)) / (float(torch.sum(P)) + 1e-6)\n",
    "\n",
    "    return PC\n",
    "\n",
    "\n",
    "def get_F1(SR, GT, threshold=0.5):\n",
    "    # Sensitivity == Recall\n",
    "    SE = get_sensitivity(SR, GT, threshold=threshold)\n",
    "    PC = get_precision(SR, GT, threshold=threshold)\n",
    "\n",
    "    F1 = 2 * SE * PC / (SE + PC + 1e-6)\n",
    "\n",
    "    return F1\n",
    "\n",
    "\n",
    "def get_JS(SR, GT, threshold=0.5):\n",
    "    # JS : Jaccard similarity\n",
    "    SR = SR > threshold\n",
    "    GT = GT > threshold\n",
    "    Inter = torch.sum(torch.logical_and(SR, GT))\n",
    "    Union = torch.sum(torch.logical_or(SR, GT))\n",
    "\n",
    "    JS = float(Inter) / (float(Union) + 1e-6)\n",
    "\n",
    "    return JS\n",
    "\n",
    "\n",
    "def get_DC(SR, GT, threshold=0.5):\n",
    "    # DC : Dice Coefficient\n",
    "    SR = SR > threshold\n",
    "    GT = GT > threshold\n",
    "    Inter = torch.sum(torch.logical_and(SR, GT)).item()\n",
    "    Union = torch.sum(torch.logical_or(SR, GT)).item()\n",
    "    DC = float(2 * Inter) / (float(Inter + Union) + 1e-6)\n",
    "\n",
    "    return DC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770eabd",
   "metadata": {
    "papermill": {
     "duration": 0.00687,
     "end_time": "2024-05-05T22:56:53.382161",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.375291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4bd8344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-05T22:56:53.397742Z",
     "iopub.status.busy": "2024-05-05T22:56:53.397389Z",
     "iopub.status.idle": "2024-05-06T08:31:05.887549Z",
     "shell.execute_reply": "2024-05-06T08:31:05.886567Z"
    },
    "papermill": {
     "duration": 34452.501577,
     "end_time": "2024-05-06T08:31:05.890566",
     "exception": false,
     "start_time": "2024-05-05T22:56:53.388989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_img_dim = img_dim//patch_dim\n",
      "=128/16 = 8\n",
      "Num of tokens: 64\n",
      "Token dimension: 1024\n",
      "Embedding shape: torch.Size([65, 1024])\n",
      "TRAIN DATALOADER DONE\n",
      "VAL DATALOADER DONE\n",
      "Entering the loop\n",
      "Epoch [1/100], Step [1/84], Loss: 0.8346\n",
      "Epoch [1/100], Step [2/84], Loss: 0.7918\n",
      "Epoch [1/100], Step [3/84], Loss: 0.7659\n",
      "Epoch [1/100], Step [4/84], Loss: 0.7740\n",
      "Epoch [1/100], Step [5/84], Loss: 0.7153\n",
      "Epoch [1/100], Step [6/84], Loss: 0.7386\n",
      "Epoch [1/100], Step [7/84], Loss: 0.7112\n",
      "Epoch [1/100], Step [8/84], Loss: 0.6837\n",
      "Epoch [1/100], Step [9/84], Loss: 0.6935\n",
      "Epoch [1/100], Step [10/84], Loss: 0.6648\n",
      "Epoch [1/100], Step [11/84], Loss: 0.6703\n",
      "Epoch [1/100], Step [12/84], Loss: 0.6406\n",
      "Epoch [1/100], Step [13/84], Loss: 0.6644\n",
      "Epoch [1/100], Step [14/84], Loss: 0.6135\n",
      "Epoch [1/100], Step [15/84], Loss: 0.6225\n",
      "Epoch [1/100], Step [16/84], Loss: 0.6193\n",
      "Epoch [1/100], Step [17/84], Loss: 0.6135\n",
      "Epoch [1/100], Step [18/84], Loss: 0.6018\n",
      "Epoch [1/100], Step [19/84], Loss: 0.6073\n",
      "Epoch [1/100], Step [20/84], Loss: 0.5891\n",
      "Epoch [1/100], Step [21/84], Loss: 0.5825\n",
      "Epoch [1/100], Step [22/84], Loss: 0.6112\n",
      "Epoch [1/100], Step [23/84], Loss: 0.6089\n",
      "Epoch [1/100], Step [24/84], Loss: 0.6179\n",
      "Epoch [1/100], Step [25/84], Loss: 0.5953\n",
      "Epoch [1/100], Step [26/84], Loss: 0.6078\n",
      "Epoch [1/100], Step [27/84], Loss: 0.5793\n",
      "Epoch [1/100], Step [28/84], Loss: 0.6046\n",
      "Epoch [1/100], Step [29/84], Loss: 0.5643\n",
      "Epoch [1/100], Step [30/84], Loss: 0.5795\n",
      "Epoch [1/100], Step [31/84], Loss: 0.6091\n",
      "Epoch [1/100], Step [32/84], Loss: 0.6191\n",
      "Epoch [1/100], Step [33/84], Loss: 0.5707\n",
      "Epoch [1/100], Step [34/84], Loss: 0.5665\n",
      "Epoch [1/100], Step [35/84], Loss: 0.6024\n",
      "Epoch [1/100], Step [36/84], Loss: 0.5971\n",
      "Epoch [1/100], Step [37/84], Loss: 0.6117\n",
      "Epoch [1/100], Step [38/84], Loss: 0.5790\n",
      "Epoch [1/100], Step [39/84], Loss: 0.5769\n",
      "Epoch [1/100], Step [40/84], Loss: 0.5667\n",
      "Epoch [1/100], Step [41/84], Loss: 0.5840\n",
      "Epoch [1/100], Step [42/84], Loss: 0.5742\n",
      "Epoch [1/100], Step [43/84], Loss: 0.5708\n",
      "Epoch [1/100], Step [44/84], Loss: 0.5810\n",
      "Epoch [1/100], Step [45/84], Loss: 0.5980\n",
      "Epoch [1/100], Step [46/84], Loss: 0.5683\n",
      "Epoch [1/100], Step [47/84], Loss: 0.5663\n",
      "Epoch [1/100], Step [48/84], Loss: 0.5890\n",
      "Epoch [1/100], Step [49/84], Loss: 0.5726\n",
      "Epoch [1/100], Step [50/84], Loss: 0.5915\n",
      "Epoch [1/100], Step [51/84], Loss: 0.5923\n",
      "Epoch [1/100], Step [52/84], Loss: 0.5580\n",
      "Epoch [1/100], Step [53/84], Loss: 0.5573\n",
      "Epoch [1/100], Step [54/84], Loss: 0.5745\n",
      "Epoch [1/100], Step [55/84], Loss: 0.5620\n",
      "Epoch [1/100], Step [56/84], Loss: 0.5552\n",
      "Epoch [1/100], Step [57/84], Loss: 0.5895\n",
      "Epoch [1/100], Step [58/84], Loss: 0.5783\n",
      "Epoch [1/100], Step [59/84], Loss: 0.5979\n",
      "Epoch [1/100], Step [60/84], Loss: 0.5451\n",
      "Epoch [1/100], Step [61/84], Loss: 0.5602\n",
      "Epoch [1/100], Step [62/84], Loss: 0.5502\n",
      "Epoch [1/100], Step [63/84], Loss: 0.5427\n",
      "Epoch [1/100], Step [64/84], Loss: 0.5738\n",
      "Epoch [1/100], Step [65/84], Loss: 0.5470\n",
      "Epoch [1/100], Step [66/84], Loss: 0.5800\n",
      "Epoch [1/100], Step [67/84], Loss: 0.5602\n",
      "Epoch [1/100], Step [68/84], Loss: 0.5437\n",
      "Epoch [1/100], Step [69/84], Loss: 0.5496\n",
      "Epoch [1/100], Step [70/84], Loss: 0.5526\n",
      "Epoch [1/100], Step [71/84], Loss: 0.5700\n",
      "Epoch [1/100], Step [72/84], Loss: 0.5455\n",
      "Epoch [1/100], Step [73/84], Loss: 0.5650\n",
      "Epoch [1/100], Step [74/84], Loss: 0.5397\n",
      "Epoch [1/100], Step [75/84], Loss: 0.5567\n",
      "Epoch [1/100], Step [76/84], Loss: 0.5678\n",
      "Epoch [1/100], Step [77/84], Loss: 0.5773\n",
      "Epoch [1/100], Step [78/84], Loss: 0.5594\n",
      "Epoch [1/100], Step [79/84], Loss: 0.5443\n",
      "Epoch [1/100], Step [80/84], Loss: 0.5530\n",
      "Epoch [1/100], Step [81/84], Loss: 0.5462\n",
      "Epoch [1/100], Step [82/84], Loss: 0.5897\n",
      "Epoch [1/100], Step [83/84], Loss: 0.5456\n",
      "Epoch [1/100], Step [84/84], Loss: 0.5803\n",
      "Epoch [2/100], Step [1/84], Loss: 0.5502\n",
      "Epoch [2/100], Step [2/84], Loss: 0.5456\n",
      "Epoch [2/100], Step [3/84], Loss: 0.5389\n",
      "Epoch [2/100], Step [4/84], Loss: 0.5546\n",
      "Epoch [2/100], Step [5/84], Loss: 0.5688\n",
      "Epoch [2/100], Step [6/84], Loss: 0.5443\n",
      "Epoch [2/100], Step [7/84], Loss: 0.5260\n",
      "Epoch [2/100], Step [8/84], Loss: 0.5500\n",
      "Epoch [2/100], Step [9/84], Loss: 0.5539\n",
      "Epoch [2/100], Step [10/84], Loss: 0.5533\n",
      "Epoch [2/100], Step [11/84], Loss: 0.5596\n",
      "Epoch [2/100], Step [12/84], Loss: 0.5566\n",
      "Epoch [2/100], Step [13/84], Loss: 0.5454\n",
      "Epoch [2/100], Step [14/84], Loss: 0.5613\n",
      "Epoch [2/100], Step [15/84], Loss: 0.5580\n",
      "Epoch [2/100], Step [16/84], Loss: 0.5497\n",
      "Epoch [2/100], Step [17/84], Loss: 0.5330\n",
      "Epoch [2/100], Step [18/84], Loss: 0.5304\n",
      "Epoch [2/100], Step [19/84], Loss: 0.5698\n",
      "Epoch [2/100], Step [20/84], Loss: 0.5577\n",
      "Epoch [2/100], Step [21/84], Loss: 0.5478\n",
      "Epoch [2/100], Step [22/84], Loss: 0.5479\n",
      "Epoch [2/100], Step [23/84], Loss: 0.5352\n",
      "Epoch [2/100], Step [24/84], Loss: 0.5630\n",
      "Epoch [2/100], Step [25/84], Loss: 0.5456\n",
      "Epoch [2/100], Step [26/84], Loss: 0.5230\n",
      "Epoch [2/100], Step [27/84], Loss: 0.5327\n",
      "Epoch [2/100], Step [28/84], Loss: 0.5360\n",
      "Epoch [2/100], Step [29/84], Loss: 0.5309\n",
      "Epoch [2/100], Step [30/84], Loss: 0.5258\n",
      "Epoch [2/100], Step [31/84], Loss: 0.5240\n",
      "Epoch [2/100], Step [32/84], Loss: 0.5598\n",
      "Epoch [2/100], Step [33/84], Loss: 0.5315\n",
      "Epoch [2/100], Step [34/84], Loss: 0.5357\n",
      "Epoch [2/100], Step [35/84], Loss: 0.5253\n",
      "Epoch [2/100], Step [36/84], Loss: 0.5282\n",
      "Epoch [2/100], Step [37/84], Loss: 0.5193\n",
      "Epoch [2/100], Step [38/84], Loss: 0.5359\n",
      "Epoch [2/100], Step [39/84], Loss: 0.5205\n",
      "Epoch [2/100], Step [40/84], Loss: 0.5369\n",
      "Epoch [2/100], Step [41/84], Loss: 0.5392\n",
      "Epoch [2/100], Step [42/84], Loss: 0.5184\n",
      "Epoch [2/100], Step [43/84], Loss: 0.5237\n",
      "Epoch [2/100], Step [44/84], Loss: 0.5258\n",
      "Epoch [2/100], Step [45/84], Loss: 0.5158\n",
      "Epoch [2/100], Step [46/84], Loss: 0.5244\n",
      "Epoch [2/100], Step [47/84], Loss: 0.5582\n",
      "Epoch [2/100], Step [48/84], Loss: 0.5257\n",
      "Epoch [2/100], Step [49/84], Loss: 0.5263\n",
      "Epoch [2/100], Step [50/84], Loss: 0.5057\n",
      "Epoch [2/100], Step [51/84], Loss: 0.5661\n",
      "Epoch [2/100], Step [52/84], Loss: 0.5356\n",
      "Epoch [2/100], Step [53/84], Loss: 0.5666\n",
      "Epoch [2/100], Step [54/84], Loss: 0.5137\n",
      "Epoch [2/100], Step [55/84], Loss: 0.5296\n",
      "Epoch [2/100], Step [56/84], Loss: 0.5381\n",
      "Epoch [2/100], Step [57/84], Loss: 0.5299\n",
      "Epoch [2/100], Step [58/84], Loss: 0.5174\n",
      "Epoch [2/100], Step [59/84], Loss: 0.5690\n",
      "Epoch [2/100], Step [60/84], Loss: 0.5109\n",
      "Epoch [2/100], Step [61/84], Loss: 0.5279\n",
      "Epoch [2/100], Step [62/84], Loss: 0.5154\n",
      "Epoch [2/100], Step [63/84], Loss: 0.5267\n",
      "Epoch [2/100], Step [64/84], Loss: 0.5409\n",
      "Epoch [2/100], Step [65/84], Loss: 0.5276\n",
      "Epoch [2/100], Step [66/84], Loss: 0.5760\n",
      "Epoch [2/100], Step [67/84], Loss: 0.5305\n",
      "Epoch [2/100], Step [68/84], Loss: 0.5382\n",
      "Epoch [2/100], Step [69/84], Loss: 0.5243\n",
      "Epoch [2/100], Step [70/84], Loss: 0.4983\n",
      "Epoch [2/100], Step [71/84], Loss: 0.5756\n",
      "Epoch [2/100], Step [72/84], Loss: 0.5138\n",
      "Epoch [2/100], Step [73/84], Loss: 0.5013\n",
      "Epoch [2/100], Step [74/84], Loss: 0.5155\n",
      "Epoch [2/100], Step [75/84], Loss: 0.5348\n",
      "Epoch [2/100], Step [76/84], Loss: 0.5079\n",
      "Epoch [2/100], Step [77/84], Loss: 0.5069\n",
      "Epoch [2/100], Step [78/84], Loss: 0.5451\n",
      "Epoch [2/100], Step [79/84], Loss: 0.5036\n",
      "Epoch [2/100], Step [80/84], Loss: 0.5204\n",
      "Epoch [2/100], Step [81/84], Loss: 0.5311\n",
      "Epoch [2/100], Step [82/84], Loss: 0.5003\n",
      "Epoch [2/100], Step [83/84], Loss: 0.5627\n",
      "Epoch [2/100], Step [84/84], Loss: 0.5356\n",
      "Epoch [3/100], Step [1/84], Loss: 0.5193\n",
      "Epoch [3/100], Step [2/84], Loss: 0.5413\n",
      "Epoch [3/100], Step [3/84], Loss: 0.5014\n",
      "Epoch [3/100], Step [4/84], Loss: 0.5199\n",
      "Epoch [3/100], Step [5/84], Loss: 0.5152\n",
      "Epoch [3/100], Step [6/84], Loss: 0.5291\n",
      "Epoch [3/100], Step [7/84], Loss: 0.5269\n",
      "Epoch [3/100], Step [8/84], Loss: 0.5082\n",
      "Epoch [3/100], Step [9/84], Loss: 0.5026\n",
      "Epoch [3/100], Step [10/84], Loss: 0.5308\n",
      "Epoch [3/100], Step [11/84], Loss: 0.5106\n",
      "Epoch [3/100], Step [12/84], Loss: 0.4976\n",
      "Epoch [3/100], Step [13/84], Loss: 0.5015\n",
      "Epoch [3/100], Step [14/84], Loss: 0.5137\n",
      "Epoch [3/100], Step [15/84], Loss: 0.5038\n",
      "Epoch [3/100], Step [16/84], Loss: 0.5239\n",
      "Epoch [3/100], Step [17/84], Loss: 0.5108\n",
      "Epoch [3/100], Step [18/84], Loss: 0.5013\n",
      "Epoch [3/100], Step [19/84], Loss: 0.5145\n",
      "Epoch [3/100], Step [20/84], Loss: 0.5274\n",
      "Epoch [3/100], Step [21/84], Loss: 0.4950\n",
      "Epoch [3/100], Step [22/84], Loss: 0.5579\n",
      "Epoch [3/100], Step [23/84], Loss: 0.5087\n",
      "Epoch [3/100], Step [24/84], Loss: 0.5089\n",
      "Epoch [3/100], Step [25/84], Loss: 0.5315\n",
      "Epoch [3/100], Step [26/84], Loss: 0.5084\n",
      "Epoch [3/100], Step [27/84], Loss: 0.5095\n",
      "Epoch [3/100], Step [28/84], Loss: 0.5108\n",
      "Epoch [3/100], Step [29/84], Loss: 0.5153\n",
      "Epoch [3/100], Step [30/84], Loss: 0.5086\n",
      "Epoch [3/100], Step [31/84], Loss: 0.5362\n",
      "Epoch [3/100], Step [32/84], Loss: 0.5166\n",
      "Epoch [3/100], Step [33/84], Loss: 0.5158\n",
      "Epoch [3/100], Step [34/84], Loss: 0.5256\n",
      "Epoch [3/100], Step [35/84], Loss: 0.5245\n",
      "Epoch [3/100], Step [36/84], Loss: 0.5099\n",
      "Epoch [3/100], Step [37/84], Loss: 0.5029\n",
      "Epoch [3/100], Step [38/84], Loss: 0.5063\n",
      "Epoch [3/100], Step [39/84], Loss: 0.5171\n",
      "Epoch [3/100], Step [40/84], Loss: 0.5050\n",
      "Epoch [3/100], Step [41/84], Loss: 0.5108\n",
      "Epoch [3/100], Step [42/84], Loss: 0.5143\n",
      "Epoch [3/100], Step [43/84], Loss: 0.5250\n",
      "Epoch [3/100], Step [44/84], Loss: 0.5047\n",
      "Epoch [3/100], Step [45/84], Loss: 0.5172\n",
      "Epoch [3/100], Step [46/84], Loss: 0.5035\n",
      "Epoch [3/100], Step [47/84], Loss: 0.5031\n",
      "Epoch [3/100], Step [48/84], Loss: 0.4915\n",
      "Epoch [3/100], Step [49/84], Loss: 0.5128\n",
      "Epoch [3/100], Step [50/84], Loss: 0.5156\n",
      "Epoch [3/100], Step [51/84], Loss: 0.5147\n",
      "Epoch [3/100], Step [52/84], Loss: 0.5331\n",
      "Epoch [3/100], Step [53/84], Loss: 0.4841\n",
      "Epoch [3/100], Step [54/84], Loss: 0.4951\n",
      "Epoch [3/100], Step [55/84], Loss: 0.5062\n",
      "Epoch [3/100], Step [56/84], Loss: 0.5136\n",
      "Epoch [3/100], Step [57/84], Loss: 0.4988\n",
      "Epoch [3/100], Step [58/84], Loss: 0.5517\n",
      "Epoch [3/100], Step [59/84], Loss: 0.5033\n",
      "Epoch [3/100], Step [60/84], Loss: 0.4923\n",
      "Epoch [3/100], Step [61/84], Loss: 0.5100\n",
      "Epoch [3/100], Step [62/84], Loss: 0.4906\n",
      "Epoch [3/100], Step [63/84], Loss: 0.5272\n",
      "Epoch [3/100], Step [64/84], Loss: 0.4939\n",
      "Epoch [3/100], Step [65/84], Loss: 0.4994\n",
      "Epoch [3/100], Step [66/84], Loss: 0.4985\n",
      "Epoch [3/100], Step [67/84], Loss: 0.5148\n",
      "Epoch [3/100], Step [68/84], Loss: 0.5082\n",
      "Epoch [3/100], Step [69/84], Loss: 0.4930\n",
      "Epoch [3/100], Step [70/84], Loss: 0.4912\n",
      "Epoch [3/100], Step [71/84], Loss: 0.4991\n",
      "Epoch [3/100], Step [72/84], Loss: 0.5164\n",
      "Epoch [3/100], Step [73/84], Loss: 0.4896\n",
      "Epoch [3/100], Step [74/84], Loss: 0.5235\n",
      "Epoch [3/100], Step [75/84], Loss: 0.4942\n",
      "Epoch [3/100], Step [76/84], Loss: 0.5022\n",
      "Epoch [3/100], Step [77/84], Loss: 0.5475\n",
      "Epoch [3/100], Step [78/84], Loss: 0.4852\n",
      "Epoch [3/100], Step [79/84], Loss: 0.4914\n",
      "Epoch [3/100], Step [80/84], Loss: 0.5049\n",
      "Epoch [3/100], Step [81/84], Loss: 0.5048\n",
      "Epoch [3/100], Step [82/84], Loss: 0.4881\n",
      "Epoch [3/100], Step [83/84], Loss: 0.5558\n",
      "Epoch [3/100], Step [84/84], Loss: 0.4668\n",
      "Epoch [4/100], Step [1/84], Loss: 0.4973\n",
      "Epoch [4/100], Step [2/84], Loss: 0.4883\n",
      "Epoch [4/100], Step [3/84], Loss: 0.4919\n",
      "Epoch [4/100], Step [4/84], Loss: 0.4879\n",
      "Epoch [4/100], Step [5/84], Loss: 0.4864\n",
      "Epoch [4/100], Step [6/84], Loss: 0.4843\n",
      "Epoch [4/100], Step [7/84], Loss: 0.4995\n",
      "Epoch [4/100], Step [8/84], Loss: 0.5066\n",
      "Epoch [4/100], Step [9/84], Loss: 0.5450\n",
      "Epoch [4/100], Step [10/84], Loss: 0.5406\n",
      "Epoch [4/100], Step [11/84], Loss: 0.5173\n",
      "Epoch [4/100], Step [12/84], Loss: 0.4944\n",
      "Epoch [4/100], Step [13/84], Loss: 0.4819\n",
      "Epoch [4/100], Step [14/84], Loss: 0.4913\n",
      "Epoch [4/100], Step [15/84], Loss: 0.4982\n",
      "Epoch [4/100], Step [16/84], Loss: 0.4984\n",
      "Epoch [4/100], Step [17/84], Loss: 0.4887\n",
      "Epoch [4/100], Step [18/84], Loss: 0.4943\n",
      "Epoch [4/100], Step [19/84], Loss: 0.5061\n",
      "Epoch [4/100], Step [20/84], Loss: 0.5396\n",
      "Epoch [4/100], Step [21/84], Loss: 0.5017\n",
      "Epoch [4/100], Step [22/84], Loss: 0.5062\n",
      "Epoch [4/100], Step [23/84], Loss: 0.4896\n",
      "Epoch [4/100], Step [24/84], Loss: 0.4990\n",
      "Epoch [4/100], Step [25/84], Loss: 0.5191\n",
      "Epoch [4/100], Step [26/84], Loss: 0.4906\n",
      "Epoch [4/100], Step [27/84], Loss: 0.5071\n",
      "Epoch [4/100], Step [28/84], Loss: 0.4937\n",
      "Epoch [4/100], Step [29/84], Loss: 0.5103\n",
      "Epoch [4/100], Step [30/84], Loss: 0.4977\n",
      "Epoch [4/100], Step [31/84], Loss: 0.5037\n",
      "Epoch [4/100], Step [32/84], Loss: 0.4901\n",
      "Epoch [4/100], Step [33/84], Loss: 0.5019\n",
      "Epoch [4/100], Step [34/84], Loss: 0.4891\n",
      "Epoch [4/100], Step [35/84], Loss: 0.4831\n",
      "Epoch [4/100], Step [36/84], Loss: 0.4916\n",
      "Epoch [4/100], Step [37/84], Loss: 0.5053\n",
      "Epoch [4/100], Step [38/84], Loss: 0.5199\n",
      "Epoch [4/100], Step [39/84], Loss: 0.5160\n",
      "Epoch [4/100], Step [40/84], Loss: 0.4935\n",
      "Epoch [4/100], Step [41/84], Loss: 0.4736\n",
      "Epoch [4/100], Step [42/84], Loss: 0.4805\n",
      "Epoch [4/100], Step [43/84], Loss: 0.4850\n",
      "Epoch [4/100], Step [44/84], Loss: 0.4854\n",
      "Epoch [4/100], Step [45/84], Loss: 0.4694\n",
      "Epoch [4/100], Step [46/84], Loss: 0.4675\n",
      "Epoch [4/100], Step [47/84], Loss: 0.4780\n",
      "Epoch [4/100], Step [48/84], Loss: 0.4959\n",
      "Epoch [4/100], Step [49/84], Loss: 0.4908\n",
      "Epoch [4/100], Step [50/84], Loss: 0.4815\n",
      "Epoch [4/100], Step [51/84], Loss: 0.4938\n",
      "Epoch [4/100], Step [52/84], Loss: 0.4662\n",
      "Epoch [4/100], Step [53/84], Loss: 0.4590\n",
      "Epoch [4/100], Step [54/84], Loss: 0.4956\n",
      "Epoch [4/100], Step [55/84], Loss: 0.4858\n",
      "Epoch [4/100], Step [56/84], Loss: 0.5172\n",
      "Epoch [4/100], Step [57/84], Loss: 0.4917\n",
      "Epoch [4/100], Step [58/84], Loss: 0.4685\n",
      "Epoch [4/100], Step [59/84], Loss: 0.4991\n",
      "Epoch [4/100], Step [60/84], Loss: 0.4749\n",
      "Epoch [4/100], Step [61/84], Loss: 0.4672\n",
      "Epoch [4/100], Step [62/84], Loss: 0.4856\n",
      "Epoch [4/100], Step [63/84], Loss: 0.4870\n",
      "Epoch [4/100], Step [64/84], Loss: 0.4812\n",
      "Epoch [4/100], Step [65/84], Loss: 0.4717\n",
      "Epoch [4/100], Step [66/84], Loss: 0.4792\n",
      "Epoch [4/100], Step [67/84], Loss: 0.5198\n",
      "Epoch [4/100], Step [68/84], Loss: 0.4852\n",
      "Epoch [4/100], Step [69/84], Loss: 0.4655\n",
      "Epoch [4/100], Step [70/84], Loss: 0.4899\n",
      "Epoch [4/100], Step [71/84], Loss: 0.4640\n",
      "Epoch [4/100], Step [72/84], Loss: 0.4642\n",
      "Epoch [4/100], Step [73/84], Loss: 0.4741\n",
      "Epoch [4/100], Step [74/84], Loss: 0.4793\n",
      "Epoch [4/100], Step [75/84], Loss: 0.4984\n",
      "Epoch [4/100], Step [76/84], Loss: 0.4834\n",
      "Epoch [4/100], Step [77/84], Loss: 0.4570\n",
      "Epoch [4/100], Step [78/84], Loss: 0.5039\n",
      "Epoch [4/100], Step [79/84], Loss: 0.4909\n",
      "Epoch [4/100], Step [80/84], Loss: 0.5217\n",
      "Epoch [4/100], Step [81/84], Loss: 0.5183\n",
      "Epoch [4/100], Step [82/84], Loss: 0.4604\n",
      "Epoch [4/100], Step [83/84], Loss: 0.4859\n",
      "Epoch [4/100], Step [84/84], Loss: 0.4482\n",
      "Epoch [5/100], Step [1/84], Loss: 0.4644\n",
      "Epoch [5/100], Step [2/84], Loss: 0.4719\n",
      "Epoch [5/100], Step [3/84], Loss: 0.4812\n",
      "Epoch [5/100], Step [4/84], Loss: 0.4742\n",
      "Epoch [5/100], Step [5/84], Loss: 0.4615\n",
      "Epoch [5/100], Step [6/84], Loss: 0.4611\n",
      "Epoch [5/100], Step [7/84], Loss: 0.4644\n",
      "Epoch [5/100], Step [8/84], Loss: 0.4627\n",
      "Epoch [5/100], Step [9/84], Loss: 0.4702\n",
      "Epoch [5/100], Step [10/84], Loss: 0.4812\n",
      "Epoch [5/100], Step [11/84], Loss: 0.4881\n",
      "Epoch [5/100], Step [12/84], Loss: 0.4901\n",
      "Epoch [5/100], Step [13/84], Loss: 0.4528\n",
      "Epoch [5/100], Step [14/84], Loss: 0.5153\n",
      "Epoch [5/100], Step [15/84], Loss: 0.4646\n",
      "Epoch [5/100], Step [16/84], Loss: 0.4745\n",
      "Epoch [5/100], Step [17/84], Loss: 0.4800\n",
      "Epoch [5/100], Step [18/84], Loss: 0.4704\n",
      "Epoch [5/100], Step [19/84], Loss: 0.4729\n",
      "Epoch [5/100], Step [20/84], Loss: 0.4643\n",
      "Epoch [5/100], Step [21/84], Loss: 0.4930\n",
      "Epoch [5/100], Step [22/84], Loss: 0.4690\n",
      "Epoch [5/100], Step [23/84], Loss: 0.4789\n",
      "Epoch [5/100], Step [24/84], Loss: 0.4866\n",
      "Epoch [5/100], Step [25/84], Loss: 0.4488\n",
      "Epoch [5/100], Step [26/84], Loss: 0.4588\n",
      "Epoch [5/100], Step [27/84], Loss: 0.4749\n",
      "Epoch [5/100], Step [28/84], Loss: 0.4629\n",
      "Epoch [5/100], Step [29/84], Loss: 0.4807\n",
      "Epoch [5/100], Step [30/84], Loss: 0.5212\n",
      "Epoch [5/100], Step [31/84], Loss: 0.4614\n",
      "Epoch [5/100], Step [32/84], Loss: 0.4841\n",
      "Epoch [5/100], Step [33/84], Loss: 0.4611\n",
      "Epoch [5/100], Step [34/84], Loss: 0.4716\n",
      "Epoch [5/100], Step [35/84], Loss: 0.4760\n",
      "Epoch [5/100], Step [36/84], Loss: 0.4644\n",
      "Epoch [5/100], Step [37/84], Loss: 0.4498\n",
      "Epoch [5/100], Step [38/84], Loss: 0.4704\n",
      "Epoch [5/100], Step [39/84], Loss: 0.4862\n",
      "Epoch [5/100], Step [40/84], Loss: 0.4706\n",
      "Epoch [5/100], Step [41/84], Loss: 0.4712\n",
      "Epoch [5/100], Step [42/84], Loss: 0.5007\n",
      "Epoch [5/100], Step [43/84], Loss: 0.4798\n",
      "Epoch [5/100], Step [44/84], Loss: 0.4468\n",
      "Epoch [5/100], Step [45/84], Loss: 0.4739\n",
      "Epoch [5/100], Step [46/84], Loss: 0.4958\n",
      "Epoch [5/100], Step [47/84], Loss: 0.4780\n",
      "Epoch [5/100], Step [48/84], Loss: 0.4669\n",
      "Epoch [5/100], Step [49/84], Loss: 0.4690\n",
      "Epoch [5/100], Step [50/84], Loss: 0.4694\n",
      "Epoch [5/100], Step [51/84], Loss: 0.4633\n",
      "Epoch [5/100], Step [52/84], Loss: 0.4777\n",
      "Epoch [5/100], Step [53/84], Loss: 0.4521\n",
      "Epoch [5/100], Step [54/84], Loss: 0.4820\n",
      "Epoch [5/100], Step [55/84], Loss: 0.4751\n",
      "Epoch [5/100], Step [56/84], Loss: 0.4706\n",
      "Epoch [5/100], Step [57/84], Loss: 0.4757\n",
      "Epoch [5/100], Step [58/84], Loss: 0.4851\n",
      "Epoch [5/100], Step [59/84], Loss: 0.4510\n",
      "Epoch [5/100], Step [60/84], Loss: 0.5021\n",
      "Epoch [5/100], Step [61/84], Loss: 0.4614\n",
      "Epoch [5/100], Step [62/84], Loss: 0.4784\n",
      "Epoch [5/100], Step [63/84], Loss: 0.4817\n",
      "Epoch [5/100], Step [64/84], Loss: 0.4603\n",
      "Epoch [5/100], Step [65/84], Loss: 0.4764\n",
      "Epoch [5/100], Step [66/84], Loss: 0.4502\n",
      "Epoch [5/100], Step [67/84], Loss: 0.4669\n",
      "Epoch [5/100], Step [68/84], Loss: 0.4678\n",
      "Epoch [5/100], Step [69/84], Loss: 0.4727\n",
      "Epoch [5/100], Step [70/84], Loss: 0.4564\n",
      "Epoch [5/100], Step [71/84], Loss: 0.4530\n",
      "Epoch [5/100], Step [72/84], Loss: 0.4579\n",
      "Epoch [5/100], Step [73/84], Loss: 0.5097\n",
      "Epoch [5/100], Step [74/84], Loss: 0.4572\n",
      "Epoch [5/100], Step [75/84], Loss: 0.4784\n",
      "Epoch [5/100], Step [76/84], Loss: 0.4589\n",
      "Epoch [5/100], Step [77/84], Loss: 0.4560\n",
      "Epoch [5/100], Step [78/84], Loss: 0.4652\n",
      "Epoch [5/100], Step [79/84], Loss: 0.4627\n",
      "Epoch [5/100], Step [80/84], Loss: 0.4458\n",
      "Epoch [5/100], Step [81/84], Loss: 0.4798\n",
      "Epoch [5/100], Step [82/84], Loss: 0.4518\n",
      "Epoch [5/100], Step [83/84], Loss: 0.4998\n",
      "Epoch [5/100], Step [84/84], Loss: 0.4866\n",
      "Epoch [6/100], Step [1/84], Loss: 0.4725\n",
      "Epoch [6/100], Step [2/84], Loss: 0.4479\n",
      "Epoch [6/100], Step [3/84], Loss: 0.4423\n",
      "Epoch [6/100], Step [4/84], Loss: 0.4507\n",
      "Epoch [6/100], Step [5/84], Loss: 0.4578\n",
      "Epoch [6/100], Step [6/84], Loss: 0.4736\n",
      "Epoch [6/100], Step [7/84], Loss: 0.4678\n",
      "Epoch [6/100], Step [8/84], Loss: 0.4624\n",
      "Epoch [6/100], Step [9/84], Loss: 0.4559\n",
      "Epoch [6/100], Step [10/84], Loss: 0.4578\n",
      "Epoch [6/100], Step [11/84], Loss: 0.4465\n",
      "Epoch [6/100], Step [12/84], Loss: 0.4771\n",
      "Epoch [6/100], Step [13/84], Loss: 0.4672\n",
      "Epoch [6/100], Step [14/84], Loss: 0.4656\n",
      "Epoch [6/100], Step [15/84], Loss: 0.4736\n",
      "Epoch [6/100], Step [16/84], Loss: 0.4838\n",
      "Epoch [6/100], Step [17/84], Loss: 0.5057\n",
      "Epoch [6/100], Step [18/84], Loss: 0.4566\n",
      "Epoch [6/100], Step [19/84], Loss: 0.4575\n",
      "Epoch [6/100], Step [20/84], Loss: 0.4675\n",
      "Epoch [6/100], Step [21/84], Loss: 0.4852\n",
      "Epoch [6/100], Step [22/84], Loss: 0.4580\n",
      "Epoch [6/100], Step [23/84], Loss: 0.4548\n",
      "Epoch [6/100], Step [24/84], Loss: 0.4596\n",
      "Epoch [6/100], Step [25/84], Loss: 0.4519\n",
      "Epoch [6/100], Step [26/84], Loss: 0.4667\n",
      "Epoch [6/100], Step [27/84], Loss: 0.4691\n",
      "Epoch [6/100], Step [28/84], Loss: 0.4602\n",
      "Epoch [6/100], Step [29/84], Loss: 0.4366\n",
      "Epoch [6/100], Step [30/84], Loss: 0.4407\n",
      "Epoch [6/100], Step [31/84], Loss: 0.4804\n",
      "Epoch [6/100], Step [32/84], Loss: 0.4570\n",
      "Epoch [6/100], Step [33/84], Loss: 0.4341\n",
      "Epoch [6/100], Step [34/84], Loss: 0.4548\n",
      "Epoch [6/100], Step [35/84], Loss: 0.4472\n",
      "Epoch [6/100], Step [36/84], Loss: 0.4705\n",
      "Epoch [6/100], Step [37/84], Loss: 0.4574\n",
      "Epoch [6/100], Step [38/84], Loss: 0.4572\n",
      "Epoch [6/100], Step [39/84], Loss: 0.4450\n",
      "Epoch [6/100], Step [40/84], Loss: 0.4557\n",
      "Epoch [6/100], Step [41/84], Loss: 0.4499\n",
      "Epoch [6/100], Step [42/84], Loss: 0.4669\n",
      "Epoch [6/100], Step [43/84], Loss: 0.4611\n",
      "Epoch [6/100], Step [44/84], Loss: 0.4349\n",
      "Epoch [6/100], Step [45/84], Loss: 0.4446\n",
      "Epoch [6/100], Step [46/84], Loss: 0.4624\n",
      "Epoch [6/100], Step [47/84], Loss: 0.4752\n",
      "Epoch [6/100], Step [48/84], Loss: 0.5147\n",
      "Epoch [6/100], Step [49/84], Loss: 0.4650\n",
      "Epoch [6/100], Step [50/84], Loss: 0.4432\n",
      "Epoch [6/100], Step [51/84], Loss: 0.4586\n",
      "Epoch [6/100], Step [52/84], Loss: 0.4454\n",
      "Epoch [6/100], Step [53/84], Loss: 0.4507\n",
      "Epoch [6/100], Step [54/84], Loss: 0.4528\n",
      "Epoch [6/100], Step [55/84], Loss: 0.4417\n",
      "Epoch [6/100], Step [56/84], Loss: 0.4769\n",
      "Epoch [6/100], Step [57/84], Loss: 0.4721\n",
      "Epoch [6/100], Step [58/84], Loss: 0.4379\n",
      "Epoch [6/100], Step [59/84], Loss: 0.4719\n",
      "Epoch [6/100], Step [60/84], Loss: 0.4400\n",
      "Epoch [6/100], Step [61/84], Loss: 0.4475\n",
      "Epoch [6/100], Step [62/84], Loss: 0.4631\n",
      "Epoch [6/100], Step [63/84], Loss: 0.4591\n",
      "Epoch [6/100], Step [64/84], Loss: 0.4527\n",
      "Epoch [6/100], Step [65/84], Loss: 0.4474\n",
      "Epoch [6/100], Step [66/84], Loss: 0.4429\n",
      "Epoch [6/100], Step [67/84], Loss: 0.4614\n",
      "Epoch [6/100], Step [68/84], Loss: 0.4593\n",
      "Epoch [6/100], Step [69/84], Loss: 0.4533\n",
      "Epoch [6/100], Step [70/84], Loss: 0.4441\n",
      "Epoch [6/100], Step [71/84], Loss: 0.4758\n",
      "Epoch [6/100], Step [72/84], Loss: 0.4434\n",
      "Epoch [6/100], Step [73/84], Loss: 0.4491\n",
      "Epoch [6/100], Step [74/84], Loss: 0.4601\n",
      "Epoch [6/100], Step [75/84], Loss: 0.4434\n",
      "Epoch [6/100], Step [76/84], Loss: 0.4522\n",
      "Epoch [6/100], Step [77/84], Loss: 0.4451\n",
      "Epoch [6/100], Step [78/84], Loss: 0.4370\n",
      "Epoch [6/100], Step [79/84], Loss: 0.4425\n",
      "Epoch [6/100], Step [80/84], Loss: 0.4459\n",
      "Epoch [6/100], Step [81/84], Loss: 0.4386\n",
      "Epoch [6/100], Step [82/84], Loss: 0.4356\n",
      "Epoch [6/100], Step [83/84], Loss: 0.4588\n",
      "Epoch [6/100], Step [84/84], Loss: 0.4883\n",
      "Epoch [7/100], Step [1/84], Loss: 0.4296\n",
      "Epoch [7/100], Step [2/84], Loss: 0.4630\n",
      "Epoch [7/100], Step [3/84], Loss: 0.4707\n",
      "Epoch [7/100], Step [4/84], Loss: 0.4308\n",
      "Epoch [7/100], Step [5/84], Loss: 0.4437\n",
      "Epoch [7/100], Step [6/84], Loss: 0.4400\n",
      "Epoch [7/100], Step [7/84], Loss: 0.4283\n",
      "Epoch [7/100], Step [8/84], Loss: 0.5080\n",
      "Epoch [7/100], Step [9/84], Loss: 0.4336\n",
      "Epoch [7/100], Step [10/84], Loss: 0.4322\n",
      "Epoch [7/100], Step [11/84], Loss: 0.4480\n",
      "Epoch [7/100], Step [12/84], Loss: 0.4560\n",
      "Epoch [7/100], Step [13/84], Loss: 0.4483\n",
      "Epoch [7/100], Step [14/84], Loss: 0.4257\n",
      "Epoch [7/100], Step [15/84], Loss: 0.4435\n",
      "Epoch [7/100], Step [16/84], Loss: 0.4286\n",
      "Epoch [7/100], Step [17/84], Loss: 0.4660\n",
      "Epoch [7/100], Step [18/84], Loss: 0.4271\n",
      "Epoch [7/100], Step [19/84], Loss: 0.4378\n",
      "Epoch [7/100], Step [20/84], Loss: 0.4434\n",
      "Epoch [7/100], Step [21/84], Loss: 0.4557\n",
      "Epoch [7/100], Step [22/84], Loss: 0.4710\n",
      "Epoch [7/100], Step [23/84], Loss: 0.4496\n",
      "Epoch [7/100], Step [24/84], Loss: 0.4557\n",
      "Epoch [7/100], Step [25/84], Loss: 0.4291\n",
      "Epoch [7/100], Step [26/84], Loss: 0.4487\n",
      "Epoch [7/100], Step [27/84], Loss: 0.4697\n",
      "Epoch [7/100], Step [28/84], Loss: 0.4553\n",
      "Epoch [7/100], Step [29/84], Loss: 0.4301\n",
      "Epoch [7/100], Step [30/84], Loss: 0.4439\n",
      "Epoch [7/100], Step [31/84], Loss: 0.4248\n",
      "Epoch [7/100], Step [32/84], Loss: 0.4315\n",
      "Epoch [7/100], Step [33/84], Loss: 0.4658\n",
      "Epoch [7/100], Step [34/84], Loss: 0.4378\n",
      "Epoch [7/100], Step [35/84], Loss: 0.4424\n",
      "Epoch [7/100], Step [36/84], Loss: 0.4402\n",
      "Epoch [7/100], Step [37/84], Loss: 0.4538\n",
      "Epoch [7/100], Step [38/84], Loss: 0.4674\n",
      "Epoch [7/100], Step [39/84], Loss: 0.4641\n",
      "Epoch [7/100], Step [40/84], Loss: 0.4337\n",
      "Epoch [7/100], Step [41/84], Loss: 0.4503\n",
      "Epoch [7/100], Step [42/84], Loss: 0.4490\n",
      "Epoch [7/100], Step [43/84], Loss: 0.4530\n",
      "Epoch [7/100], Step [44/84], Loss: 0.4362\n",
      "Epoch [7/100], Step [45/84], Loss: 0.4497\n",
      "Epoch [7/100], Step [46/84], Loss: 0.4235\n",
      "Epoch [7/100], Step [47/84], Loss: 0.4266\n",
      "Epoch [7/100], Step [48/84], Loss: 0.4642\n",
      "Epoch [7/100], Step [49/84], Loss: 0.4289\n",
      "Epoch [7/100], Step [50/84], Loss: 0.4493\n",
      "Epoch [7/100], Step [51/84], Loss: 0.4476\n",
      "Epoch [7/100], Step [52/84], Loss: 0.4162\n",
      "Epoch [7/100], Step [53/84], Loss: 0.4225\n",
      "Epoch [7/100], Step [54/84], Loss: 0.4567\n",
      "Epoch [7/100], Step [55/84], Loss: 0.4311\n",
      "Epoch [7/100], Step [56/84], Loss: 0.4188\n",
      "Epoch [7/100], Step [57/84], Loss: 0.4299\n",
      "Epoch [7/100], Step [58/84], Loss: 0.4391\n",
      "Epoch [7/100], Step [59/84], Loss: 0.4252\n",
      "Epoch [7/100], Step [60/84], Loss: 0.4432\n",
      "Epoch [7/100], Step [61/84], Loss: 0.4159\n",
      "Epoch [7/100], Step [62/84], Loss: 0.4512\n",
      "Epoch [7/100], Step [63/84], Loss: 0.4209\n",
      "Epoch [7/100], Step [64/84], Loss: 0.4214\n",
      "Epoch [7/100], Step [65/84], Loss: 0.4509\n",
      "Epoch [7/100], Step [66/84], Loss: 0.4169\n",
      "Epoch [7/100], Step [67/84], Loss: 0.4170\n",
      "Epoch [7/100], Step [68/84], Loss: 0.4367\n",
      "Epoch [7/100], Step [69/84], Loss: 0.4372\n",
      "Epoch [7/100], Step [70/84], Loss: 0.4427\n",
      "Epoch [7/100], Step [71/84], Loss: 0.4527\n",
      "Epoch [7/100], Step [72/84], Loss: 0.4108\n",
      "Epoch [7/100], Step [73/84], Loss: 0.4338\n",
      "Epoch [7/100], Step [74/84], Loss: 0.4476\n",
      "Epoch [7/100], Step [75/84], Loss: 0.4434\n",
      "Epoch [7/100], Step [76/84], Loss: 0.4042\n",
      "Epoch [7/100], Step [77/84], Loss: 0.4232\n",
      "Epoch [7/100], Step [78/84], Loss: 0.4456\n",
      "Epoch [7/100], Step [79/84], Loss: 0.4300\n",
      "Epoch [7/100], Step [80/84], Loss: 0.4295\n",
      "Epoch [7/100], Step [81/84], Loss: 0.4247\n",
      "Epoch [7/100], Step [82/84], Loss: 0.4381\n",
      "Epoch [7/100], Step [83/84], Loss: 0.4251\n",
      "Epoch [7/100], Step [84/84], Loss: 0.4624\n",
      "Epoch [8/100], Step [1/84], Loss: 0.4187\n",
      "Epoch [8/100], Step [2/84], Loss: 0.4539\n",
      "Epoch [8/100], Step [3/84], Loss: 0.4419\n",
      "Epoch [8/100], Step [4/84], Loss: 0.4167\n",
      "Epoch [8/100], Step [5/84], Loss: 0.4432\n",
      "Epoch [8/100], Step [6/84], Loss: 0.4423\n",
      "Epoch [8/100], Step [7/84], Loss: 0.4506\n",
      "Epoch [8/100], Step [8/84], Loss: 0.4065\n",
      "Epoch [8/100], Step [9/84], Loss: 0.4474\n",
      "Epoch [8/100], Step [10/84], Loss: 0.4131\n",
      "Epoch [8/100], Step [11/84], Loss: 0.4309\n",
      "Epoch [8/100], Step [12/84], Loss: 0.4396\n",
      "Epoch [8/100], Step [13/84], Loss: 0.4083\n",
      "Epoch [8/100], Step [14/84], Loss: 0.4293\n",
      "Epoch [8/100], Step [15/84], Loss: 0.4228\n",
      "Epoch [8/100], Step [16/84], Loss: 0.4263\n",
      "Epoch [8/100], Step [17/84], Loss: 0.4314\n",
      "Epoch [8/100], Step [18/84], Loss: 0.4244\n",
      "Epoch [8/100], Step [19/84], Loss: 0.4224\n",
      "Epoch [8/100], Step [20/84], Loss: 0.4222\n",
      "Epoch [8/100], Step [21/84], Loss: 0.4178\n",
      "Epoch [8/100], Step [22/84], Loss: 0.4130\n",
      "Epoch [8/100], Step [23/84], Loss: 0.4253\n",
      "Epoch [8/100], Step [24/84], Loss: 0.4386\n",
      "Epoch [8/100], Step [25/84], Loss: 0.4678\n",
      "Epoch [8/100], Step [26/84], Loss: 0.4427\n",
      "Epoch [8/100], Step [27/84], Loss: 0.4085\n",
      "Epoch [8/100], Step [28/84], Loss: 0.4336\n",
      "Epoch [8/100], Step [29/84], Loss: 0.4651\n",
      "Epoch [8/100], Step [30/84], Loss: 0.4442\n",
      "Epoch [8/100], Step [31/84], Loss: 0.4149\n",
      "Epoch [8/100], Step [32/84], Loss: 0.4207\n",
      "Epoch [8/100], Step [33/84], Loss: 0.4269\n",
      "Epoch [8/100], Step [34/84], Loss: 0.4245\n",
      "Epoch [8/100], Step [35/84], Loss: 0.4140\n",
      "Epoch [8/100], Step [36/84], Loss: 0.4042\n",
      "Epoch [8/100], Step [37/84], Loss: 0.4331\n",
      "Epoch [8/100], Step [38/84], Loss: 0.4307\n",
      "Epoch [8/100], Step [39/84], Loss: 0.4347\n",
      "Epoch [8/100], Step [40/84], Loss: 0.4480\n",
      "Epoch [8/100], Step [41/84], Loss: 0.4366\n",
      "Epoch [8/100], Step [42/84], Loss: 0.4462\n",
      "Epoch [8/100], Step [43/84], Loss: 0.4281\n",
      "Epoch [8/100], Step [44/84], Loss: 0.4128\n",
      "Epoch [8/100], Step [45/84], Loss: 0.4236\n",
      "Epoch [8/100], Step [46/84], Loss: 0.4261\n",
      "Epoch [8/100], Step [47/84], Loss: 0.4315\n",
      "Epoch [8/100], Step [48/84], Loss: 0.4132\n",
      "Epoch [8/100], Step [49/84], Loss: 0.4033\n",
      "Epoch [8/100], Step [50/84], Loss: 0.4309\n",
      "Epoch [8/100], Step [51/84], Loss: 0.4315\n",
      "Epoch [8/100], Step [52/84], Loss: 0.4185\n",
      "Epoch [8/100], Step [53/84], Loss: 0.4030\n",
      "Epoch [8/100], Step [54/84], Loss: 0.4011\n",
      "Epoch [8/100], Step [55/84], Loss: 0.4227\n",
      "Epoch [8/100], Step [56/84], Loss: 0.3928\n",
      "Epoch [8/100], Step [57/84], Loss: 0.4167\n",
      "Epoch [8/100], Step [58/84], Loss: 0.4009\n",
      "Epoch [8/100], Step [59/84], Loss: 0.3972\n",
      "Epoch [8/100], Step [60/84], Loss: 0.4016\n",
      "Epoch [8/100], Step [61/84], Loss: 0.4393\n",
      "Epoch [8/100], Step [62/84], Loss: 0.4251\n",
      "Epoch [8/100], Step [63/84], Loss: 0.4224\n",
      "Epoch [8/100], Step [64/84], Loss: 0.4040\n",
      "Epoch [8/100], Step [65/84], Loss: 0.4047\n",
      "Epoch [8/100], Step [66/84], Loss: 0.4094\n",
      "Epoch [8/100], Step [67/84], Loss: 0.3985\n",
      "Epoch [8/100], Step [68/84], Loss: 0.4092\n",
      "Epoch [8/100], Step [69/84], Loss: 0.4135\n",
      "Epoch [8/100], Step [70/84], Loss: 0.4507\n",
      "Epoch [8/100], Step [71/84], Loss: 0.4223\n",
      "Epoch [8/100], Step [72/84], Loss: 0.4505\n",
      "Epoch [8/100], Step [73/84], Loss: 0.4149\n",
      "Epoch [8/100], Step [74/84], Loss: 0.4290\n",
      "Epoch [8/100], Step [75/84], Loss: 0.4410\n",
      "Epoch [8/100], Step [76/84], Loss: 0.4221\n",
      "Epoch [8/100], Step [77/84], Loss: 0.4203\n",
      "Epoch [8/100], Step [78/84], Loss: 0.4000\n",
      "Epoch [8/100], Step [79/84], Loss: 0.4057\n",
      "Epoch [8/100], Step [80/84], Loss: 0.4106\n",
      "Epoch [8/100], Step [81/84], Loss: 0.4272\n",
      "Epoch [8/100], Step [82/84], Loss: 0.3917\n",
      "Epoch [8/100], Step [83/84], Loss: 0.4106\n",
      "Epoch [8/100], Step [84/84], Loss: 0.4002\n",
      "Epoch [9/100], Step [1/84], Loss: 0.4058\n",
      "Epoch [9/100], Step [2/84], Loss: 0.4243\n",
      "Epoch [9/100], Step [3/84], Loss: 0.4308\n",
      "Epoch [9/100], Step [4/84], Loss: 0.4130\n",
      "Epoch [9/100], Step [5/84], Loss: 0.3959\n",
      "Epoch [9/100], Step [6/84], Loss: 0.4097\n",
      "Epoch [9/100], Step [7/84], Loss: 0.4205\n",
      "Epoch [9/100], Step [8/84], Loss: 0.3943\n",
      "Epoch [9/100], Step [9/84], Loss: 0.4196\n",
      "Epoch [9/100], Step [10/84], Loss: 0.3908\n",
      "Epoch [9/100], Step [11/84], Loss: 0.4452\n",
      "Epoch [9/100], Step [12/84], Loss: 0.4215\n",
      "Epoch [9/100], Step [13/84], Loss: 0.4090\n",
      "Epoch [9/100], Step [14/84], Loss: 0.4298\n",
      "Epoch [9/100], Step [15/84], Loss: 0.4246\n",
      "Epoch [9/100], Step [16/84], Loss: 0.4247\n",
      "Epoch [9/100], Step [17/84], Loss: 0.4602\n",
      "Epoch [9/100], Step [18/84], Loss: 0.4076\n",
      "Epoch [9/100], Step [19/84], Loss: 0.3999\n",
      "Epoch [9/100], Step [20/84], Loss: 0.4128\n",
      "Epoch [9/100], Step [21/84], Loss: 0.4199\n",
      "Epoch [9/100], Step [22/84], Loss: 0.4439\n",
      "Epoch [9/100], Step [23/84], Loss: 0.4207\n",
      "Epoch [9/100], Step [24/84], Loss: 0.4062\n",
      "Epoch [9/100], Step [25/84], Loss: 0.4361\n",
      "Epoch [9/100], Step [26/84], Loss: 0.3941\n",
      "Epoch [9/100], Step [27/84], Loss: 0.4129\n",
      "Epoch [9/100], Step [28/84], Loss: 0.4110\n",
      "Epoch [9/100], Step [29/84], Loss: 0.4196\n",
      "Epoch [9/100], Step [30/84], Loss: 0.4039\n",
      "Epoch [9/100], Step [31/84], Loss: 0.4064\n",
      "Epoch [9/100], Step [32/84], Loss: 0.4125\n",
      "Epoch [9/100], Step [33/84], Loss: 0.4286\n",
      "Epoch [9/100], Step [34/84], Loss: 0.4297\n",
      "Epoch [9/100], Step [35/84], Loss: 0.3985\n",
      "Epoch [9/100], Step [36/84], Loss: 0.4245\n",
      "Epoch [9/100], Step [37/84], Loss: 0.3935\n",
      "Epoch [9/100], Step [38/84], Loss: 0.4306\n",
      "Epoch [9/100], Step [39/84], Loss: 0.4139\n",
      "Epoch [9/100], Step [40/84], Loss: 0.3889\n",
      "Epoch [9/100], Step [41/84], Loss: 0.4265\n",
      "Epoch [9/100], Step [42/84], Loss: 0.4327\n",
      "Epoch [9/100], Step [43/84], Loss: 0.4266\n",
      "Epoch [9/100], Step [44/84], Loss: 0.3993\n",
      "Epoch [9/100], Step [45/84], Loss: 0.4113\n",
      "Epoch [9/100], Step [46/84], Loss: 0.4064\n",
      "Epoch [9/100], Step [47/84], Loss: 0.4069\n",
      "Epoch [9/100], Step [48/84], Loss: 0.3933\n",
      "Epoch [9/100], Step [49/84], Loss: 0.4289\n",
      "Epoch [9/100], Step [50/84], Loss: 0.4173\n",
      "Epoch [9/100], Step [51/84], Loss: 0.4118\n",
      "Epoch [9/100], Step [52/84], Loss: 0.3968\n",
      "Epoch [9/100], Step [53/84], Loss: 0.3886\n",
      "Epoch [9/100], Step [54/84], Loss: 0.3818\n",
      "Epoch [9/100], Step [55/84], Loss: 0.4047\n",
      "Epoch [9/100], Step [56/84], Loss: 0.3990\n",
      "Epoch [9/100], Step [57/84], Loss: 0.4037\n",
      "Epoch [9/100], Step [58/84], Loss: 0.3923\n",
      "Epoch [9/100], Step [59/84], Loss: 0.3983\n",
      "Epoch [9/100], Step [60/84], Loss: 0.4119\n",
      "Epoch [9/100], Step [61/84], Loss: 0.4171\n",
      "Epoch [9/100], Step [62/84], Loss: 0.4080\n",
      "Epoch [9/100], Step [63/84], Loss: 0.3947\n",
      "Epoch [9/100], Step [64/84], Loss: 0.3982\n",
      "Epoch [9/100], Step [65/84], Loss: 0.4046\n",
      "Epoch [9/100], Step [66/84], Loss: 0.4103\n",
      "Epoch [9/100], Step [67/84], Loss: 0.4021\n",
      "Epoch [9/100], Step [68/84], Loss: 0.3960\n",
      "Epoch [9/100], Step [69/84], Loss: 0.4141\n",
      "Epoch [9/100], Step [70/84], Loss: 0.3873\n",
      "Epoch [9/100], Step [71/84], Loss: 0.4136\n",
      "Epoch [9/100], Step [72/84], Loss: 0.4419\n",
      "Epoch [9/100], Step [73/84], Loss: 0.3863\n",
      "Epoch [9/100], Step [74/84], Loss: 0.4054\n",
      "Epoch [9/100], Step [75/84], Loss: 0.3888\n",
      "Epoch [9/100], Step [76/84], Loss: 0.3808\n",
      "Epoch [9/100], Step [77/84], Loss: 0.4004\n",
      "Epoch [9/100], Step [78/84], Loss: 0.3770\n",
      "Epoch [9/100], Step [79/84], Loss: 0.3858\n",
      "Epoch [9/100], Step [80/84], Loss: 0.4304\n",
      "Epoch [9/100], Step [81/84], Loss: 0.3776\n",
      "Epoch [9/100], Step [82/84], Loss: 0.4062\n",
      "Epoch [9/100], Step [83/84], Loss: 0.4044\n",
      "Epoch [9/100], Step [84/84], Loss: 0.4110\n",
      "Epoch [10/100], Step [1/84], Loss: 0.3895\n",
      "Epoch [10/100], Step [2/84], Loss: 0.3907\n",
      "Epoch [10/100], Step [3/84], Loss: 0.3899\n",
      "Epoch [10/100], Step [4/84], Loss: 0.4051\n",
      "Epoch [10/100], Step [5/84], Loss: 0.3995\n",
      "Epoch [10/100], Step [6/84], Loss: 0.3754\n",
      "Epoch [10/100], Step [7/84], Loss: 0.3819\n",
      "Epoch [10/100], Step [8/84], Loss: 0.4060\n",
      "Epoch [10/100], Step [9/84], Loss: 0.3796\n",
      "Epoch [10/100], Step [10/84], Loss: 0.3849\n",
      "Epoch [10/100], Step [11/84], Loss: 0.4253\n",
      "Epoch [10/100], Step [12/84], Loss: 0.4116\n",
      "Epoch [10/100], Step [13/84], Loss: 0.3939\n",
      "Epoch [10/100], Step [14/84], Loss: 0.4017\n",
      "Epoch [10/100], Step [15/84], Loss: 0.3953\n",
      "Epoch [10/100], Step [16/84], Loss: 0.3681\n",
      "Epoch [10/100], Step [17/84], Loss: 0.4004\n",
      "Epoch [10/100], Step [18/84], Loss: 0.4231\n",
      "Epoch [10/100], Step [19/84], Loss: 0.4176\n",
      "Epoch [10/100], Step [20/84], Loss: 0.4066\n",
      "Epoch [10/100], Step [21/84], Loss: 0.4127\n",
      "Epoch [10/100], Step [22/84], Loss: 0.3832\n",
      "Epoch [10/100], Step [23/84], Loss: 0.4060\n",
      "Epoch [10/100], Step [24/84], Loss: 0.3908\n",
      "Epoch [10/100], Step [25/84], Loss: 0.4139\n",
      "Epoch [10/100], Step [26/84], Loss: 0.3927\n",
      "Epoch [10/100], Step [27/84], Loss: 0.4053\n",
      "Epoch [10/100], Step [28/84], Loss: 0.4100\n",
      "Epoch [10/100], Step [29/84], Loss: 0.4001\n",
      "Epoch [10/100], Step [30/84], Loss: 0.3765\n",
      "Epoch [10/100], Step [31/84], Loss: 0.5055\n",
      "Epoch [10/100], Step [32/84], Loss: 0.3890\n",
      "Epoch [10/100], Step [33/84], Loss: 0.3825\n",
      "Epoch [10/100], Step [34/84], Loss: 0.4441\n",
      "Epoch [10/100], Step [35/84], Loss: 0.3957\n",
      "Epoch [10/100], Step [36/84], Loss: 0.3886\n",
      "Epoch [10/100], Step [37/84], Loss: 0.3880\n",
      "Epoch [10/100], Step [38/84], Loss: 0.4002\n",
      "Epoch [10/100], Step [39/84], Loss: 0.4021\n",
      "Epoch [10/100], Step [40/84], Loss: 0.3791\n",
      "Epoch [10/100], Step [41/84], Loss: 0.3851\n",
      "Epoch [10/100], Step [42/84], Loss: 0.3885\n",
      "Epoch [10/100], Step [43/84], Loss: 0.3955\n",
      "Epoch [10/100], Step [44/84], Loss: 0.3851\n",
      "Epoch [10/100], Step [45/84], Loss: 0.3836\n",
      "Epoch [10/100], Step [46/84], Loss: 0.4126\n",
      "Epoch [10/100], Step [47/84], Loss: 0.4105\n",
      "Epoch [10/100], Step [48/84], Loss: 0.3905\n",
      "Epoch [10/100], Step [49/84], Loss: 0.3863\n",
      "Epoch [10/100], Step [50/84], Loss: 0.3848\n",
      "Epoch [10/100], Step [51/84], Loss: 0.4038\n",
      "Epoch [10/100], Step [52/84], Loss: 0.3724\n",
      "Epoch [10/100], Step [53/84], Loss: 0.4394\n",
      "Epoch [10/100], Step [54/84], Loss: 0.4138\n",
      "Epoch [10/100], Step [55/84], Loss: 0.3908\n",
      "Epoch [10/100], Step [56/84], Loss: 0.3914\n",
      "Epoch [10/100], Step [57/84], Loss: 0.4102\n",
      "Epoch [10/100], Step [58/84], Loss: 0.3810\n",
      "Epoch [10/100], Step [59/84], Loss: 0.3701\n",
      "Epoch [10/100], Step [60/84], Loss: 0.3875\n",
      "Epoch [10/100], Step [61/84], Loss: 0.3869\n",
      "Epoch [10/100], Step [62/84], Loss: 0.3902\n",
      "Epoch [10/100], Step [63/84], Loss: 0.3850\n",
      "Epoch [10/100], Step [64/84], Loss: 0.3819\n",
      "Epoch [10/100], Step [65/84], Loss: 0.3874\n",
      "Epoch [10/100], Step [66/84], Loss: 0.3663\n",
      "Epoch [10/100], Step [67/84], Loss: 0.4045\n",
      "Epoch [10/100], Step [68/84], Loss: 0.3836\n",
      "Epoch [10/100], Step [69/84], Loss: 0.3672\n",
      "Epoch [10/100], Step [70/84], Loss: 0.3967\n",
      "Epoch [10/100], Step [71/84], Loss: 0.3875\n",
      "Epoch [10/100], Step [72/84], Loss: 0.3757\n",
      "Epoch [10/100], Step [73/84], Loss: 0.3998\n",
      "Epoch [10/100], Step [74/84], Loss: 0.3900\n",
      "Epoch [10/100], Step [75/84], Loss: 0.3628\n",
      "Epoch [10/100], Step [76/84], Loss: 0.3829\n",
      "Epoch [10/100], Step [77/84], Loss: 0.3883\n",
      "Epoch [10/100], Step [78/84], Loss: 0.3817\n",
      "Epoch [10/100], Step [79/84], Loss: 0.4162\n",
      "Epoch [10/100], Step [80/84], Loss: 0.3912\n",
      "Epoch [10/100], Step [81/84], Loss: 0.3859\n",
      "Epoch [10/100], Step [82/84], Loss: 0.3927\n",
      "Epoch [10/100], Step [83/84], Loss: 0.3776\n",
      "Epoch [10/100], Step [84/84], Loss: 0.4580\n",
      "Epoch [11/100], Step [1/84], Loss: 0.3846\n",
      "Epoch [11/100], Step [2/84], Loss: 0.4161\n",
      "Epoch [11/100], Step [3/84], Loss: 0.4153\n",
      "Epoch [11/100], Step [4/84], Loss: 0.3877\n",
      "Epoch [11/100], Step [5/84], Loss: 0.3949\n",
      "Epoch [11/100], Step [6/84], Loss: 0.3956\n",
      "Epoch [11/100], Step [7/84], Loss: 0.3969\n",
      "Epoch [11/100], Step [8/84], Loss: 0.3957\n",
      "Epoch [11/100], Step [9/84], Loss: 0.3887\n",
      "Epoch [11/100], Step [10/84], Loss: 0.3886\n",
      "Epoch [11/100], Step [11/84], Loss: 0.3950\n",
      "Epoch [11/100], Step [12/84], Loss: 0.3838\n",
      "Epoch [11/100], Step [13/84], Loss: 0.3943\n",
      "Epoch [11/100], Step [14/84], Loss: 0.3723\n",
      "Epoch [11/100], Step [15/84], Loss: 0.3648\n",
      "Epoch [11/100], Step [16/84], Loss: 0.3702\n",
      "Epoch [11/100], Step [17/84], Loss: 0.3893\n",
      "Epoch [11/100], Step [18/84], Loss: 0.4328\n",
      "Epoch [11/100], Step [19/84], Loss: 0.3688\n",
      "Epoch [11/100], Step [20/84], Loss: 0.4130\n",
      "Epoch [11/100], Step [21/84], Loss: 0.3702\n",
      "Epoch [11/100], Step [22/84], Loss: 0.3882\n",
      "Epoch [11/100], Step [23/84], Loss: 0.3769\n",
      "Epoch [11/100], Step [24/84], Loss: 0.3868\n",
      "Epoch [11/100], Step [25/84], Loss: 0.3916\n",
      "Epoch [11/100], Step [26/84], Loss: 0.3974\n",
      "Epoch [11/100], Step [27/84], Loss: 0.4271\n",
      "Epoch [11/100], Step [28/84], Loss: 0.3911\n",
      "Epoch [11/100], Step [29/84], Loss: 0.3685\n",
      "Epoch [11/100], Step [30/84], Loss: 0.3843\n",
      "Epoch [11/100], Step [31/84], Loss: 0.3643\n",
      "Epoch [11/100], Step [32/84], Loss: 0.3872\n",
      "Epoch [11/100], Step [33/84], Loss: 0.3827\n",
      "Epoch [11/100], Step [34/84], Loss: 0.4014\n",
      "Epoch [11/100], Step [35/84], Loss: 0.3786\n",
      "Epoch [11/100], Step [36/84], Loss: 0.3628\n",
      "Epoch [11/100], Step [37/84], Loss: 0.4000\n",
      "Epoch [11/100], Step [38/84], Loss: 0.3940\n",
      "Epoch [11/100], Step [39/84], Loss: 0.4196\n",
      "Epoch [11/100], Step [40/84], Loss: 0.3639\n",
      "Epoch [11/100], Step [41/84], Loss: 0.3707\n",
      "Epoch [11/100], Step [42/84], Loss: 0.3677\n",
      "Epoch [11/100], Step [43/84], Loss: 0.3738\n",
      "Epoch [11/100], Step [44/84], Loss: 0.3790\n",
      "Epoch [11/100], Step [45/84], Loss: 0.3821\n",
      "Epoch [11/100], Step [46/84], Loss: 0.3756\n",
      "Epoch [11/100], Step [47/84], Loss: 0.3609\n",
      "Epoch [11/100], Step [48/84], Loss: 0.4001\n",
      "Epoch [11/100], Step [49/84], Loss: 0.3708\n",
      "Epoch [11/100], Step [50/84], Loss: 0.3865\n",
      "Epoch [11/100], Step [51/84], Loss: 0.3802\n",
      "Epoch [11/100], Step [52/84], Loss: 0.3589\n",
      "Epoch [11/100], Step [53/84], Loss: 0.3494\n",
      "Epoch [11/100], Step [54/84], Loss: 0.3878\n",
      "Epoch [11/100], Step [55/84], Loss: 0.3694\n",
      "Epoch [11/100], Step [56/84], Loss: 0.4462\n",
      "Epoch [11/100], Step [57/84], Loss: 0.3692\n",
      "Epoch [11/100], Step [58/84], Loss: 0.3937\n",
      "Epoch [11/100], Step [59/84], Loss: 0.3820\n",
      "Epoch [11/100], Step [60/84], Loss: 0.3676\n",
      "Epoch [11/100], Step [61/84], Loss: 0.4087\n",
      "Epoch [11/100], Step [62/84], Loss: 0.3850\n",
      "Epoch [11/100], Step [63/84], Loss: 0.3967\n",
      "Epoch [11/100], Step [64/84], Loss: 0.4068\n",
      "Epoch [11/100], Step [65/84], Loss: 0.3749\n",
      "Epoch [11/100], Step [66/84], Loss: 0.3740\n",
      "Epoch [11/100], Step [67/84], Loss: 0.3891\n",
      "Epoch [11/100], Step [68/84], Loss: 0.3630\n",
      "Epoch [11/100], Step [69/84], Loss: 0.3739\n",
      "Epoch [11/100], Step [70/84], Loss: 0.3968\n",
      "Epoch [11/100], Step [71/84], Loss: 0.3823\n",
      "Epoch [11/100], Step [72/84], Loss: 0.3809\n",
      "Epoch [11/100], Step [73/84], Loss: 0.3665\n",
      "Epoch [11/100], Step [74/84], Loss: 0.3584\n",
      "Epoch [11/100], Step [75/84], Loss: 0.3734\n",
      "Epoch [11/100], Step [76/84], Loss: 0.3767\n",
      "Epoch [11/100], Step [77/84], Loss: 0.3588\n",
      "Epoch [11/100], Step [78/84], Loss: 0.3864\n",
      "Epoch [11/100], Step [79/84], Loss: 0.4062\n",
      "Epoch [11/100], Step [80/84], Loss: 0.3915\n",
      "Epoch [11/100], Step [81/84], Loss: 0.3628\n",
      "Epoch [11/100], Step [82/84], Loss: 0.3797\n",
      "Epoch [11/100], Step [83/84], Loss: 0.3853\n",
      "Epoch [11/100], Step [84/84], Loss: 0.3968\n",
      "Epoch [12/100], Step [1/84], Loss: 0.3612\n",
      "Epoch [12/100], Step [2/84], Loss: 0.3734\n",
      "Epoch [12/100], Step [3/84], Loss: 0.3811\n",
      "Epoch [12/100], Step [4/84], Loss: 0.3678\n",
      "Epoch [12/100], Step [5/84], Loss: 0.3560\n",
      "Epoch [12/100], Step [6/84], Loss: 0.4167\n",
      "Epoch [12/100], Step [7/84], Loss: 0.3505\n",
      "Epoch [12/100], Step [8/84], Loss: 0.3600\n",
      "Epoch [12/100], Step [9/84], Loss: 0.3783\n",
      "Epoch [12/100], Step [10/84], Loss: 0.3812\n",
      "Epoch [12/100], Step [11/84], Loss: 0.3574\n",
      "Epoch [12/100], Step [12/84], Loss: 0.3782\n",
      "Epoch [12/100], Step [13/84], Loss: 0.3657\n",
      "Epoch [12/100], Step [14/84], Loss: 0.3806\n",
      "Epoch [12/100], Step [15/84], Loss: 0.3621\n",
      "Epoch [12/100], Step [16/84], Loss: 0.3934\n",
      "Epoch [12/100], Step [17/84], Loss: 0.3768\n",
      "Epoch [12/100], Step [18/84], Loss: 0.3884\n",
      "Epoch [12/100], Step [19/84], Loss: 0.3505\n",
      "Epoch [12/100], Step [20/84], Loss: 0.3432\n",
      "Epoch [12/100], Step [21/84], Loss: 0.3786\n",
      "Epoch [12/100], Step [22/84], Loss: 0.3428\n",
      "Epoch [12/100], Step [23/84], Loss: 0.3559\n",
      "Epoch [12/100], Step [24/84], Loss: 0.3965\n",
      "Epoch [12/100], Step [25/84], Loss: 0.3596\n",
      "Epoch [12/100], Step [26/84], Loss: 0.3663\n",
      "Epoch [12/100], Step [27/84], Loss: 0.3704\n",
      "Epoch [12/100], Step [28/84], Loss: 0.3737\n",
      "Epoch [12/100], Step [29/84], Loss: 0.3965\n",
      "Epoch [12/100], Step [30/84], Loss: 0.3814\n",
      "Epoch [12/100], Step [31/84], Loss: 0.3544\n",
      "Epoch [12/100], Step [32/84], Loss: 0.3537\n",
      "Epoch [12/100], Step [33/84], Loss: 0.3763\n",
      "Epoch [12/100], Step [34/84], Loss: 0.3694\n",
      "Epoch [12/100], Step [35/84], Loss: 0.3623\n",
      "Epoch [12/100], Step [36/84], Loss: 0.3571\n",
      "Epoch [12/100], Step [37/84], Loss: 0.4160\n",
      "Epoch [12/100], Step [38/84], Loss: 0.3713\n",
      "Epoch [12/100], Step [39/84], Loss: 0.3670\n",
      "Epoch [12/100], Step [40/84], Loss: 0.3632\n",
      "Epoch [12/100], Step [41/84], Loss: 0.3776\n",
      "Epoch [12/100], Step [42/84], Loss: 0.4043\n",
      "Epoch [12/100], Step [43/84], Loss: 0.3676\n",
      "Epoch [12/100], Step [44/84], Loss: 0.3545\n",
      "Epoch [12/100], Step [45/84], Loss: 0.3579\n",
      "Epoch [12/100], Step [46/84], Loss: 0.3524\n",
      "Epoch [12/100], Step [47/84], Loss: 0.3618\n",
      "Epoch [12/100], Step [48/84], Loss: 0.3439\n",
      "Epoch [12/100], Step [49/84], Loss: 0.3572\n",
      "Epoch [12/100], Step [50/84], Loss: 0.3602\n",
      "Epoch [12/100], Step [51/84], Loss: 0.3864\n",
      "Epoch [12/100], Step [52/84], Loss: 0.3589\n",
      "Epoch [12/100], Step [53/84], Loss: 0.3904\n",
      "Epoch [12/100], Step [54/84], Loss: 0.3606\n",
      "Epoch [12/100], Step [55/84], Loss: 0.3516\n",
      "Epoch [12/100], Step [56/84], Loss: 0.3597\n",
      "Epoch [12/100], Step [57/84], Loss: 0.3722\n",
      "Epoch [12/100], Step [58/84], Loss: 0.3868\n",
      "Epoch [12/100], Step [59/84], Loss: 0.3714\n",
      "Epoch [12/100], Step [60/84], Loss: 0.3583\n",
      "Epoch [12/100], Step [61/84], Loss: 0.3610\n",
      "Epoch [12/100], Step [62/84], Loss: 0.3618\n",
      "Epoch [12/100], Step [63/84], Loss: 0.3567\n",
      "Epoch [12/100], Step [64/84], Loss: 0.3432\n",
      "Epoch [12/100], Step [65/84], Loss: 0.3535\n",
      "Epoch [12/100], Step [66/84], Loss: 0.3881\n",
      "Epoch [12/100], Step [67/84], Loss: 0.3472\n",
      "Epoch [12/100], Step [68/84], Loss: 0.3641\n",
      "Epoch [12/100], Step [69/84], Loss: 0.3620\n",
      "Epoch [12/100], Step [70/84], Loss: 0.3472\n",
      "Epoch [12/100], Step [71/84], Loss: 0.3935\n",
      "Epoch [12/100], Step [72/84], Loss: 0.3669\n",
      "Epoch [12/100], Step [73/84], Loss: 0.3498\n",
      "Epoch [12/100], Step [74/84], Loss: 0.3802\n",
      "Epoch [12/100], Step [75/84], Loss: 0.3760\n",
      "Epoch [12/100], Step [76/84], Loss: 0.3524\n",
      "Epoch [12/100], Step [77/84], Loss: 0.3477\n",
      "Epoch [12/100], Step [78/84], Loss: 0.3787\n",
      "Epoch [12/100], Step [79/84], Loss: 0.3615\n",
      "Epoch [12/100], Step [80/84], Loss: 0.3717\n",
      "Epoch [12/100], Step [81/84], Loss: 0.3556\n",
      "Epoch [12/100], Step [82/84], Loss: 0.3593\n",
      "Epoch [12/100], Step [83/84], Loss: 0.3540\n",
      "Epoch [12/100], Step [84/84], Loss: 0.3443\n",
      "Epoch [13/100], Step [1/84], Loss: 0.3544\n",
      "Epoch [13/100], Step [2/84], Loss: 0.3948\n",
      "Epoch [13/100], Step [3/84], Loss: 0.3616\n",
      "Epoch [13/100], Step [4/84], Loss: 0.3574\n",
      "Epoch [13/100], Step [5/84], Loss: 0.3635\n",
      "Epoch [13/100], Step [6/84], Loss: 0.3541\n",
      "Epoch [13/100], Step [7/84], Loss: 0.3562\n",
      "Epoch [13/100], Step [8/84], Loss: 0.3806\n",
      "Epoch [13/100], Step [9/84], Loss: 0.3559\n",
      "Epoch [13/100], Step [10/84], Loss: 0.3786\n",
      "Epoch [13/100], Step [11/84], Loss: 0.3541\n",
      "Epoch [13/100], Step [12/84], Loss: 0.3532\n",
      "Epoch [13/100], Step [13/84], Loss: 0.3664\n",
      "Epoch [13/100], Step [14/84], Loss: 0.3422\n",
      "Epoch [13/100], Step [15/84], Loss: 0.3524\n",
      "Epoch [13/100], Step [16/84], Loss: 0.3487\n",
      "Epoch [13/100], Step [17/84], Loss: 0.3553\n",
      "Epoch [13/100], Step [18/84], Loss: 0.3553\n",
      "Epoch [13/100], Step [19/84], Loss: 0.3951\n",
      "Epoch [13/100], Step [20/84], Loss: 0.3410\n",
      "Epoch [13/100], Step [21/84], Loss: 0.3813\n",
      "Epoch [13/100], Step [22/84], Loss: 0.3686\n",
      "Epoch [13/100], Step [23/84], Loss: 0.3709\n",
      "Epoch [13/100], Step [24/84], Loss: 0.3512\n",
      "Epoch [13/100], Step [25/84], Loss: 0.3268\n",
      "Epoch [13/100], Step [26/84], Loss: 0.3555\n",
      "Epoch [13/100], Step [27/84], Loss: 0.3693\n",
      "Epoch [13/100], Step [28/84], Loss: 0.3407\n",
      "Epoch [13/100], Step [29/84], Loss: 0.3631\n",
      "Epoch [13/100], Step [30/84], Loss: 0.3594\n",
      "Epoch [13/100], Step [31/84], Loss: 0.3516\n",
      "Epoch [13/100], Step [32/84], Loss: 0.3738\n",
      "Epoch [13/100], Step [33/84], Loss: 0.3509\n",
      "Epoch [13/100], Step [34/84], Loss: 0.3439\n",
      "Epoch [13/100], Step [35/84], Loss: 0.3592\n",
      "Epoch [13/100], Step [36/84], Loss: 0.3967\n",
      "Epoch [13/100], Step [37/84], Loss: 0.3829\n",
      "Epoch [13/100], Step [38/84], Loss: 0.3272\n",
      "Epoch [13/100], Step [39/84], Loss: 0.3397\n",
      "Epoch [13/100], Step [40/84], Loss: 0.3825\n",
      "Epoch [13/100], Step [41/84], Loss: 0.3788\n",
      "Epoch [13/100], Step [42/84], Loss: 0.3445\n",
      "Epoch [13/100], Step [43/84], Loss: 0.3412\n",
      "Epoch [13/100], Step [44/84], Loss: 0.3563\n",
      "Epoch [13/100], Step [45/84], Loss: 0.3380\n",
      "Epoch [13/100], Step [46/84], Loss: 0.3670\n",
      "Epoch [13/100], Step [47/84], Loss: 0.3369\n",
      "Epoch [13/100], Step [48/84], Loss: 0.3716\n",
      "Epoch [13/100], Step [49/84], Loss: 0.3371\n",
      "Epoch [13/100], Step [50/84], Loss: 0.3468\n",
      "Epoch [13/100], Step [51/84], Loss: 0.3576\n",
      "Epoch [13/100], Step [52/84], Loss: 0.3595\n",
      "Epoch [13/100], Step [53/84], Loss: 0.3643\n",
      "Epoch [13/100], Step [54/84], Loss: 0.3568\n",
      "Epoch [13/100], Step [55/84], Loss: 0.3571\n",
      "Epoch [13/100], Step [56/84], Loss: 0.3414\n",
      "Epoch [13/100], Step [57/84], Loss: 0.3859\n",
      "Epoch [13/100], Step [58/84], Loss: 0.3325\n",
      "Epoch [13/100], Step [59/84], Loss: 0.3333\n",
      "Epoch [13/100], Step [60/84], Loss: 0.3475\n",
      "Epoch [13/100], Step [61/84], Loss: 0.3641\n",
      "Epoch [13/100], Step [62/84], Loss: 0.3445\n",
      "Epoch [13/100], Step [63/84], Loss: 0.3556\n",
      "Epoch [13/100], Step [64/84], Loss: 0.3520\n",
      "Epoch [13/100], Step [65/84], Loss: 0.3413\n",
      "Epoch [13/100], Step [66/84], Loss: 0.3425\n",
      "Epoch [13/100], Step [67/84], Loss: 0.3632\n",
      "Epoch [13/100], Step [68/84], Loss: 0.3559\n",
      "Epoch [13/100], Step [69/84], Loss: 0.3526\n",
      "Epoch [13/100], Step [70/84], Loss: 0.3447\n",
      "Epoch [13/100], Step [71/84], Loss: 0.3262\n",
      "Epoch [13/100], Step [72/84], Loss: 0.3324\n",
      "Epoch [13/100], Step [73/84], Loss: 0.3645\n",
      "Epoch [13/100], Step [74/84], Loss: 0.3844\n",
      "Epoch [13/100], Step [75/84], Loss: 0.3472\n",
      "Epoch [13/100], Step [76/84], Loss: 0.3380\n",
      "Epoch [13/100], Step [77/84], Loss: 0.3224\n",
      "Epoch [13/100], Step [78/84], Loss: 0.3420\n",
      "Epoch [13/100], Step [79/84], Loss: 0.3589\n",
      "Epoch [13/100], Step [80/84], Loss: 0.3559\n",
      "Epoch [13/100], Step [81/84], Loss: 0.3641\n",
      "Epoch [13/100], Step [82/84], Loss: 0.3764\n",
      "Epoch [13/100], Step [83/84], Loss: 0.3506\n",
      "Epoch [13/100], Step [84/84], Loss: 0.3475\n",
      "Epoch [14/100], Step [1/84], Loss: 0.3493\n",
      "Epoch [14/100], Step [2/84], Loss: 0.3392\n",
      "Epoch [14/100], Step [3/84], Loss: 0.3679\n",
      "Epoch [14/100], Step [4/84], Loss: 0.3324\n",
      "Epoch [14/100], Step [5/84], Loss: 0.3501\n",
      "Epoch [14/100], Step [6/84], Loss: 0.3460\n",
      "Epoch [14/100], Step [7/84], Loss: 0.3262\n",
      "Epoch [14/100], Step [8/84], Loss: 0.3546\n",
      "Epoch [14/100], Step [9/84], Loss: 0.3728\n",
      "Epoch [14/100], Step [10/84], Loss: 0.3287\n",
      "Epoch [14/100], Step [11/84], Loss: 0.3671\n",
      "Epoch [14/100], Step [12/84], Loss: 0.3504\n",
      "Epoch [14/100], Step [13/84], Loss: 0.3448\n",
      "Epoch [14/100], Step [14/84], Loss: 0.3458\n",
      "Epoch [14/100], Step [15/84], Loss: 0.3415\n",
      "Epoch [14/100], Step [16/84], Loss: 0.3434\n",
      "Epoch [14/100], Step [17/84], Loss: 0.3490\n",
      "Epoch [14/100], Step [18/84], Loss: 0.3998\n",
      "Epoch [14/100], Step [19/84], Loss: 0.3628\n",
      "Epoch [14/100], Step [20/84], Loss: 0.3461\n",
      "Epoch [14/100], Step [21/84], Loss: 0.3390\n",
      "Epoch [14/100], Step [22/84], Loss: 0.3342\n",
      "Epoch [14/100], Step [23/84], Loss: 0.3389\n",
      "Epoch [14/100], Step [24/84], Loss: 0.3372\n",
      "Epoch [14/100], Step [25/84], Loss: 0.3356\n",
      "Epoch [14/100], Step [26/84], Loss: 0.3563\n",
      "Epoch [14/100], Step [27/84], Loss: 0.3365\n",
      "Epoch [14/100], Step [28/84], Loss: 0.3292\n",
      "Epoch [14/100], Step [29/84], Loss: 0.3256\n",
      "Epoch [14/100], Step [30/84], Loss: 0.3818\n",
      "Epoch [14/100], Step [31/84], Loss: 0.3469\n",
      "Epoch [14/100], Step [32/84], Loss: 0.3568\n",
      "Epoch [14/100], Step [33/84], Loss: 0.3528\n",
      "Epoch [14/100], Step [34/84], Loss: 0.3547\n",
      "Epoch [14/100], Step [35/84], Loss: 0.3595\n",
      "Epoch [14/100], Step [36/84], Loss: 0.3579\n",
      "Epoch [14/100], Step [37/84], Loss: 0.3403\n",
      "Epoch [14/100], Step [38/84], Loss: 0.3149\n",
      "Epoch [14/100], Step [39/84], Loss: 0.3416\n",
      "Epoch [14/100], Step [40/84], Loss: 0.3429\n",
      "Epoch [14/100], Step [41/84], Loss: 0.3323\n",
      "Epoch [14/100], Step [42/84], Loss: 0.3361\n",
      "Epoch [14/100], Step [43/84], Loss: 0.3392\n",
      "Epoch [14/100], Step [44/84], Loss: 0.3349\n",
      "Epoch [14/100], Step [45/84], Loss: 0.3420\n",
      "Epoch [14/100], Step [46/84], Loss: 0.3420\n",
      "Epoch [14/100], Step [47/84], Loss: 0.3228\n",
      "Epoch [14/100], Step [48/84], Loss: 0.3752\n",
      "Epoch [14/100], Step [49/84], Loss: 0.3472\n",
      "Epoch [14/100], Step [50/84], Loss: 0.3521\n",
      "Epoch [14/100], Step [51/84], Loss: 0.3277\n",
      "Epoch [14/100], Step [52/84], Loss: 0.3453\n",
      "Epoch [14/100], Step [53/84], Loss: 0.3380\n",
      "Epoch [14/100], Step [54/84], Loss: 0.3514\n",
      "Epoch [14/100], Step [55/84], Loss: 0.3458\n",
      "Epoch [14/100], Step [56/84], Loss: 0.3993\n",
      "Epoch [14/100], Step [57/84], Loss: 0.3412\n",
      "Epoch [14/100], Step [58/84], Loss: 0.3597\n",
      "Epoch [14/100], Step [59/84], Loss: 0.3394\n",
      "Epoch [14/100], Step [60/84], Loss: 0.3372\n",
      "Epoch [14/100], Step [61/84], Loss: 0.3375\n",
      "Epoch [14/100], Step [62/84], Loss: 0.3509\n",
      "Epoch [14/100], Step [63/84], Loss: 0.3316\n",
      "Epoch [14/100], Step [64/84], Loss: 0.3365\n",
      "Epoch [14/100], Step [65/84], Loss: 0.3316\n",
      "Epoch [14/100], Step [66/84], Loss: 0.3860\n",
      "Epoch [14/100], Step [67/84], Loss: 0.3301\n",
      "Epoch [14/100], Step [68/84], Loss: 0.3312\n",
      "Epoch [14/100], Step [69/84], Loss: 0.3556\n",
      "Epoch [14/100], Step [70/84], Loss: 0.3656\n",
      "Epoch [14/100], Step [71/84], Loss: 0.3569\n",
      "Epoch [14/100], Step [72/84], Loss: 0.3284\n",
      "Epoch [14/100], Step [73/84], Loss: 0.3345\n",
      "Epoch [14/100], Step [74/84], Loss: 0.3590\n",
      "Epoch [14/100], Step [75/84], Loss: 0.3341\n",
      "Epoch [14/100], Step [76/84], Loss: 0.3607\n",
      "Epoch [14/100], Step [77/84], Loss: 0.3845\n",
      "Epoch [14/100], Step [78/84], Loss: 0.3355\n",
      "Epoch [14/100], Step [79/84], Loss: 0.3296\n",
      "Epoch [14/100], Step [80/84], Loss: 0.3407\n",
      "Epoch [14/100], Step [81/84], Loss: 0.3709\n",
      "Epoch [14/100], Step [82/84], Loss: 0.3346\n",
      "Epoch [14/100], Step [83/84], Loss: 0.3526\n",
      "Epoch [14/100], Step [84/84], Loss: 0.3482\n",
      "Epoch [15/100], Step [1/84], Loss: 0.3378\n",
      "Epoch [15/100], Step [2/84], Loss: 0.3502\n",
      "Epoch [15/100], Step [3/84], Loss: 0.3385\n",
      "Epoch [15/100], Step [4/84], Loss: 0.3402\n",
      "Epoch [15/100], Step [5/84], Loss: 0.3566\n",
      "Epoch [15/100], Step [6/84], Loss: 0.3258\n",
      "Epoch [15/100], Step [7/84], Loss: 0.3380\n",
      "Epoch [15/100], Step [8/84], Loss: 0.3511\n",
      "Epoch [15/100], Step [9/84], Loss: 0.3324\n",
      "Epoch [15/100], Step [10/84], Loss: 0.3248\n",
      "Epoch [15/100], Step [11/84], Loss: 0.3461\n",
      "Epoch [15/100], Step [12/84], Loss: 0.3298\n",
      "Epoch [15/100], Step [13/84], Loss: 0.3409\n",
      "Epoch [15/100], Step [14/84], Loss: 0.3231\n",
      "Epoch [15/100], Step [15/84], Loss: 0.3373\n",
      "Epoch [15/100], Step [16/84], Loss: 0.3170\n",
      "Epoch [15/100], Step [17/84], Loss: 0.3192\n",
      "Epoch [15/100], Step [18/84], Loss: 0.3465\n",
      "Epoch [15/100], Step [19/84], Loss: 0.3448\n",
      "Epoch [15/100], Step [20/84], Loss: 0.3238\n",
      "Epoch [15/100], Step [21/84], Loss: 0.3436\n",
      "Epoch [15/100], Step [22/84], Loss: 0.3308\n",
      "Epoch [15/100], Step [23/84], Loss: 0.3318\n",
      "Epoch [15/100], Step [24/84], Loss: 0.3448\n",
      "Epoch [15/100], Step [25/84], Loss: 0.3516\n",
      "Epoch [15/100], Step [26/84], Loss: 0.3530\n",
      "Epoch [15/100], Step [27/84], Loss: 0.3440\n",
      "Epoch [15/100], Step [28/84], Loss: 0.3426\n",
      "Epoch [15/100], Step [29/84], Loss: 0.3545\n",
      "Epoch [15/100], Step [30/84], Loss: 0.3383\n",
      "Epoch [15/100], Step [31/84], Loss: 0.3429\n",
      "Epoch [15/100], Step [32/84], Loss: 0.3311\n",
      "Epoch [15/100], Step [33/84], Loss: 0.3285\n",
      "Epoch [15/100], Step [34/84], Loss: 0.3533\n",
      "Epoch [15/100], Step [35/84], Loss: 0.3576\n",
      "Epoch [15/100], Step [36/84], Loss: 0.3516\n",
      "Epoch [15/100], Step [37/84], Loss: 0.3676\n",
      "Epoch [15/100], Step [38/84], Loss: 0.3384\n",
      "Epoch [15/100], Step [39/84], Loss: 0.3397\n",
      "Epoch [15/100], Step [40/84], Loss: 0.3274\n",
      "Epoch [15/100], Step [41/84], Loss: 0.3822\n",
      "Epoch [15/100], Step [42/84], Loss: 0.3152\n",
      "Epoch [15/100], Step [43/84], Loss: 0.3463\n",
      "Epoch [15/100], Step [44/84], Loss: 0.3338\n",
      "Epoch [15/100], Step [45/84], Loss: 0.3413\n",
      "Epoch [15/100], Step [46/84], Loss: 0.3578\n",
      "Epoch [15/100], Step [47/84], Loss: 0.3438\n",
      "Epoch [15/100], Step [48/84], Loss: 0.3138\n",
      "Epoch [15/100], Step [49/84], Loss: 0.3427\n",
      "Epoch [15/100], Step [50/84], Loss: 0.3047\n",
      "Epoch [15/100], Step [51/84], Loss: 0.3442\n",
      "Epoch [15/100], Step [52/84], Loss: 0.3049\n",
      "Epoch [15/100], Step [53/84], Loss: 0.3134\n",
      "Epoch [15/100], Step [54/84], Loss: 0.3744\n",
      "Epoch [15/100], Step [55/84], Loss: 0.3309\n",
      "Epoch [15/100], Step [56/84], Loss: 0.3207\n",
      "Epoch [15/100], Step [57/84], Loss: 0.3397\n",
      "Epoch [15/100], Step [58/84], Loss: 0.3429\n",
      "Epoch [15/100], Step [59/84], Loss: 0.3805\n",
      "Epoch [15/100], Step [60/84], Loss: 0.3370\n",
      "Epoch [15/100], Step [61/84], Loss: 0.3463\n",
      "Epoch [15/100], Step [62/84], Loss: 0.3202\n",
      "Epoch [15/100], Step [63/84], Loss: 0.3715\n",
      "Epoch [15/100], Step [64/84], Loss: 0.3268\n",
      "Epoch [15/100], Step [65/84], Loss: 0.3318\n",
      "Epoch [15/100], Step [66/84], Loss: 0.3252\n",
      "Epoch [15/100], Step [67/84], Loss: 0.3356\n",
      "Epoch [15/100], Step [68/84], Loss: 0.3268\n",
      "Epoch [15/100], Step [69/84], Loss: 0.3266\n",
      "Epoch [15/100], Step [70/84], Loss: 0.3147\n",
      "Epoch [15/100], Step [71/84], Loss: 0.3403\n",
      "Epoch [15/100], Step [72/84], Loss: 0.3367\n",
      "Epoch [15/100], Step [73/84], Loss: 0.3561\n",
      "Epoch [15/100], Step [74/84], Loss: 0.3282\n",
      "Epoch [15/100], Step [75/84], Loss: 0.3341\n",
      "Epoch [15/100], Step [76/84], Loss: 0.3200\n",
      "Epoch [15/100], Step [77/84], Loss: 0.3314\n",
      "Epoch [15/100], Step [78/84], Loss: 0.3133\n",
      "Epoch [15/100], Step [79/84], Loss: 0.3292\n",
      "Epoch [15/100], Step [80/84], Loss: 0.3231\n",
      "Epoch [15/100], Step [81/84], Loss: 0.3298\n",
      "Epoch [15/100], Step [82/84], Loss: 0.3331\n",
      "Epoch [15/100], Step [83/84], Loss: 0.3413\n",
      "Epoch [15/100], Step [84/84], Loss: 0.3475\n",
      "Epoch [16/100], Step [1/84], Loss: 0.3361\n",
      "Epoch [16/100], Step [2/84], Loss: 0.3408\n",
      "Epoch [16/100], Step [3/84], Loss: 0.3158\n",
      "Epoch [16/100], Step [4/84], Loss: 0.3401\n",
      "Epoch [16/100], Step [5/84], Loss: 0.3499\n",
      "Epoch [16/100], Step [6/84], Loss: 0.3283\n",
      "Epoch [16/100], Step [7/84], Loss: 0.3154\n",
      "Epoch [16/100], Step [8/84], Loss: 0.3184\n",
      "Epoch [16/100], Step [9/84], Loss: 0.3477\n",
      "Epoch [16/100], Step [10/84], Loss: 0.3187\n",
      "Epoch [16/100], Step [11/84], Loss: 0.3213\n",
      "Epoch [16/100], Step [12/84], Loss: 0.3575\n",
      "Epoch [16/100], Step [13/84], Loss: 0.3426\n",
      "Epoch [16/100], Step [14/84], Loss: 0.3256\n",
      "Epoch [16/100], Step [15/84], Loss: 0.3177\n",
      "Epoch [16/100], Step [16/84], Loss: 0.3360\n",
      "Epoch [16/100], Step [17/84], Loss: 0.3602\n",
      "Epoch [16/100], Step [18/84], Loss: 0.3311\n",
      "Epoch [16/100], Step [19/84], Loss: 0.3128\n",
      "Epoch [16/100], Step [20/84], Loss: 0.3142\n",
      "Epoch [16/100], Step [21/84], Loss: 0.3561\n",
      "Epoch [16/100], Step [22/84], Loss: 0.3180\n",
      "Epoch [16/100], Step [23/84], Loss: 0.3509\n",
      "Epoch [16/100], Step [24/84], Loss: 0.3374\n",
      "Epoch [16/100], Step [25/84], Loss: 0.3304\n",
      "Epoch [16/100], Step [26/84], Loss: 0.3359\n",
      "Epoch [16/100], Step [27/84], Loss: 0.3177\n",
      "Epoch [16/100], Step [28/84], Loss: 0.3183\n",
      "Epoch [16/100], Step [29/84], Loss: 0.3272\n",
      "Epoch [16/100], Step [30/84], Loss: 0.3255\n",
      "Epoch [16/100], Step [31/84], Loss: 0.3247\n",
      "Epoch [16/100], Step [32/84], Loss: 0.3380\n",
      "Epoch [16/100], Step [33/84], Loss: 0.3062\n",
      "Epoch [16/100], Step [34/84], Loss: 0.3355\n",
      "Epoch [16/100], Step [35/84], Loss: 0.3121\n",
      "Epoch [16/100], Step [36/84], Loss: 0.3055\n",
      "Epoch [16/100], Step [37/84], Loss: 0.3190\n",
      "Epoch [16/100], Step [38/84], Loss: 0.3327\n",
      "Epoch [16/100], Step [39/84], Loss: 0.3538\n",
      "Epoch [16/100], Step [40/84], Loss: 0.3250\n",
      "Epoch [16/100], Step [41/84], Loss: 0.3128\n",
      "Epoch [16/100], Step [42/84], Loss: 0.3488\n",
      "Epoch [16/100], Step [43/84], Loss: 0.3175\n",
      "Epoch [16/100], Step [44/84], Loss: 0.3284\n",
      "Epoch [16/100], Step [45/84], Loss: 0.3050\n",
      "Epoch [16/100], Step [46/84], Loss: 0.3524\n",
      "Epoch [16/100], Step [47/84], Loss: 0.3001\n",
      "Epoch [16/100], Step [48/84], Loss: 0.3223\n",
      "Epoch [16/100], Step [49/84], Loss: 0.3159\n",
      "Epoch [16/100], Step [50/84], Loss: 0.3164\n",
      "Epoch [16/100], Step [51/84], Loss: 0.3175\n",
      "Epoch [16/100], Step [52/84], Loss: 0.3135\n",
      "Epoch [16/100], Step [53/84], Loss: 0.3211\n",
      "Epoch [16/100], Step [54/84], Loss: 0.3092\n",
      "Epoch [16/100], Step [55/84], Loss: 0.3385\n",
      "Epoch [16/100], Step [56/84], Loss: 0.3026\n",
      "Epoch [16/100], Step [57/84], Loss: 0.3184\n",
      "Epoch [16/100], Step [58/84], Loss: 0.3139\n",
      "Epoch [16/100], Step [59/84], Loss: 0.3288\n",
      "Epoch [16/100], Step [60/84], Loss: 0.3069\n",
      "Epoch [16/100], Step [61/84], Loss: 0.3226\n",
      "Epoch [16/100], Step [62/84], Loss: 0.3125\n",
      "Epoch [16/100], Step [63/84], Loss: 0.3156\n",
      "Epoch [16/100], Step [64/84], Loss: 0.3284\n",
      "Epoch [16/100], Step [65/84], Loss: 0.3091\n",
      "Epoch [16/100], Step [66/84], Loss: 0.3170\n",
      "Epoch [16/100], Step [67/84], Loss: 0.3309\n",
      "Epoch [16/100], Step [68/84], Loss: 0.3444\n",
      "Epoch [16/100], Step [69/84], Loss: 0.3378\n",
      "Epoch [16/100], Step [70/84], Loss: 0.3268\n",
      "Epoch [16/100], Step [71/84], Loss: 0.3285\n",
      "Epoch [16/100], Step [72/84], Loss: 0.3248\n",
      "Epoch [16/100], Step [73/84], Loss: 0.3093\n",
      "Epoch [16/100], Step [74/84], Loss: 0.3238\n",
      "Epoch [16/100], Step [75/84], Loss: 0.3087\n",
      "Epoch [16/100], Step [76/84], Loss: 0.3285\n",
      "Epoch [16/100], Step [77/84], Loss: 0.3200\n",
      "Epoch [16/100], Step [78/84], Loss: 0.3116\n",
      "Epoch [16/100], Step [79/84], Loss: 0.3300\n",
      "Epoch [16/100], Step [80/84], Loss: 0.3180\n",
      "Epoch [16/100], Step [81/84], Loss: 0.3103\n",
      "Epoch [16/100], Step [82/84], Loss: 0.3159\n",
      "Epoch [16/100], Step [83/84], Loss: 0.3266\n",
      "Epoch [16/100], Step [84/84], Loss: 0.3154\n",
      "Epoch [17/100], Step [1/84], Loss: 0.3221\n",
      "Epoch [17/100], Step [2/84], Loss: 0.3306\n",
      "Epoch [17/100], Step [3/84], Loss: 0.2990\n",
      "Epoch [17/100], Step [4/84], Loss: 0.3118\n",
      "Epoch [17/100], Step [5/84], Loss: 0.3163\n",
      "Epoch [17/100], Step [6/84], Loss: 0.3031\n",
      "Epoch [17/100], Step [7/84], Loss: 0.3055\n",
      "Epoch [17/100], Step [8/84], Loss: 0.3127\n",
      "Epoch [17/100], Step [9/84], Loss: 0.3183\n",
      "Epoch [17/100], Step [10/84], Loss: 0.3000\n",
      "Epoch [17/100], Step [11/84], Loss: 0.3026\n",
      "Epoch [17/100], Step [12/84], Loss: 0.3146\n",
      "Epoch [17/100], Step [13/84], Loss: 0.3061\n",
      "Epoch [17/100], Step [14/84], Loss: 0.3061\n",
      "Epoch [17/100], Step [15/84], Loss: 0.3252\n",
      "Epoch [17/100], Step [16/84], Loss: 0.3162\n",
      "Epoch [17/100], Step [17/84], Loss: 0.3363\n",
      "Epoch [17/100], Step [18/84], Loss: 0.3127\n",
      "Epoch [17/100], Step [19/84], Loss: 0.3488\n",
      "Epoch [17/100], Step [20/84], Loss: 0.3115\n",
      "Epoch [17/100], Step [21/84], Loss: 0.3309\n",
      "Epoch [17/100], Step [22/84], Loss: 0.3041\n",
      "Epoch [17/100], Step [23/84], Loss: 0.3457\n",
      "Epoch [17/100], Step [24/84], Loss: 0.3104\n",
      "Epoch [17/100], Step [25/84], Loss: 0.2984\n",
      "Epoch [17/100], Step [26/84], Loss: 0.3389\n",
      "Epoch [17/100], Step [27/84], Loss: 0.3158\n",
      "Epoch [17/100], Step [28/84], Loss: 0.2940\n",
      "Epoch [17/100], Step [29/84], Loss: 0.3057\n",
      "Epoch [17/100], Step [30/84], Loss: 0.2996\n",
      "Epoch [17/100], Step [31/84], Loss: 0.3264\n",
      "Epoch [17/100], Step [32/84], Loss: 0.3740\n",
      "Epoch [17/100], Step [33/84], Loss: 0.3327\n",
      "Epoch [17/100], Step [34/84], Loss: 0.3021\n",
      "Epoch [17/100], Step [35/84], Loss: 0.3093\n",
      "Epoch [17/100], Step [36/84], Loss: 0.3180\n",
      "Epoch [17/100], Step [37/84], Loss: 0.2856\n",
      "Epoch [17/100], Step [38/84], Loss: 0.3273\n",
      "Epoch [17/100], Step [39/84], Loss: 0.3131\n",
      "Epoch [17/100], Step [40/84], Loss: 0.3143\n",
      "Epoch [17/100], Step [41/84], Loss: 0.3166\n",
      "Epoch [17/100], Step [42/84], Loss: 0.3237\n",
      "Epoch [17/100], Step [43/84], Loss: 0.3315\n",
      "Epoch [17/100], Step [44/84], Loss: 0.3089\n",
      "Epoch [17/100], Step [45/84], Loss: 0.2976\n",
      "Epoch [17/100], Step [46/84], Loss: 0.3292\n",
      "Epoch [17/100], Step [47/84], Loss: 0.3266\n",
      "Epoch [17/100], Step [48/84], Loss: 0.3129\n",
      "Epoch [17/100], Step [49/84], Loss: 0.2954\n",
      "Epoch [17/100], Step [50/84], Loss: 0.3336\n",
      "Epoch [17/100], Step [51/84], Loss: 0.3299\n",
      "Epoch [17/100], Step [52/84], Loss: 0.3088\n",
      "Epoch [17/100], Step [53/84], Loss: 0.3094\n",
      "Epoch [17/100], Step [54/84], Loss: 0.3230\n",
      "Epoch [17/100], Step [55/84], Loss: 0.3138\n",
      "Epoch [17/100], Step [56/84], Loss: 0.3045\n",
      "Epoch [17/100], Step [57/84], Loss: 0.3227\n",
      "Epoch [17/100], Step [58/84], Loss: 0.3177\n",
      "Epoch [17/100], Step [59/84], Loss: 0.3005\n",
      "Epoch [17/100], Step [60/84], Loss: 0.3127\n",
      "Epoch [17/100], Step [61/84], Loss: 0.3019\n",
      "Epoch [17/100], Step [62/84], Loss: 0.3034\n",
      "Epoch [17/100], Step [63/84], Loss: 0.3202\n",
      "Epoch [17/100], Step [64/84], Loss: 0.3252\n",
      "Epoch [17/100], Step [65/84], Loss: 0.3175\n",
      "Epoch [17/100], Step [66/84], Loss: 0.2902\n",
      "Epoch [17/100], Step [67/84], Loss: 0.3229\n",
      "Epoch [17/100], Step [68/84], Loss: 0.3210\n",
      "Epoch [17/100], Step [69/84], Loss: 0.3199\n",
      "Epoch [17/100], Step [70/84], Loss: 0.3241\n",
      "Epoch [17/100], Step [71/84], Loss: 0.3073\n",
      "Epoch [17/100], Step [72/84], Loss: 0.2988\n",
      "Epoch [17/100], Step [73/84], Loss: 0.2959\n",
      "Epoch [17/100], Step [74/84], Loss: 0.3189\n",
      "Epoch [17/100], Step [75/84], Loss: 0.3244\n",
      "Epoch [17/100], Step [76/84], Loss: 0.3175\n",
      "Epoch [17/100], Step [77/84], Loss: 0.3085\n",
      "Epoch [17/100], Step [78/84], Loss: 0.3081\n",
      "Epoch [17/100], Step [79/84], Loss: 0.2909\n",
      "Epoch [17/100], Step [80/84], Loss: 0.3469\n",
      "Epoch [17/100], Step [81/84], Loss: 0.3273\n",
      "Epoch [17/100], Step [82/84], Loss: 0.2912\n",
      "Epoch [17/100], Step [83/84], Loss: 0.2968\n",
      "Epoch [17/100], Step [84/84], Loss: 0.3711\n",
      "Epoch [18/100], Step [1/84], Loss: 0.3157\n",
      "Epoch [18/100], Step [2/84], Loss: 0.3158\n",
      "Epoch [18/100], Step [3/84], Loss: 0.2878\n",
      "Epoch [18/100], Step [4/84], Loss: 0.3028\n",
      "Epoch [18/100], Step [5/84], Loss: 0.3065\n",
      "Epoch [18/100], Step [6/84], Loss: 0.3360\n",
      "Epoch [18/100], Step [7/84], Loss: 0.3124\n",
      "Epoch [18/100], Step [8/84], Loss: 0.3351\n",
      "Epoch [18/100], Step [9/84], Loss: 0.3126\n",
      "Epoch [18/100], Step [10/84], Loss: 0.3156\n",
      "Epoch [18/100], Step [11/84], Loss: 0.3212\n",
      "Epoch [18/100], Step [12/84], Loss: 0.3096\n",
      "Epoch [18/100], Step [13/84], Loss: 0.3050\n",
      "Epoch [18/100], Step [14/84], Loss: 0.3245\n",
      "Epoch [18/100], Step [15/84], Loss: 0.2931\n",
      "Epoch [18/100], Step [16/84], Loss: 0.3700\n",
      "Epoch [18/100], Step [17/84], Loss: 0.3031\n",
      "Epoch [18/100], Step [18/84], Loss: 0.3161\n",
      "Epoch [18/100], Step [19/84], Loss: 0.3325\n",
      "Epoch [18/100], Step [20/84], Loss: 0.3069\n",
      "Epoch [18/100], Step [21/84], Loss: 0.3143\n",
      "Epoch [18/100], Step [22/84], Loss: 0.3357\n",
      "Epoch [18/100], Step [23/84], Loss: 0.3050\n",
      "Epoch [18/100], Step [24/84], Loss: 0.2950\n",
      "Epoch [18/100], Step [25/84], Loss: 0.3309\n",
      "Epoch [18/100], Step [26/84], Loss: 0.3068\n",
      "Epoch [18/100], Step [27/84], Loss: 0.3104\n",
      "Epoch [18/100], Step [28/84], Loss: 0.3099\n",
      "Epoch [18/100], Step [29/84], Loss: 0.2937\n",
      "Epoch [18/100], Step [30/84], Loss: 0.3326\n",
      "Epoch [18/100], Step [31/84], Loss: 0.3379\n",
      "Epoch [18/100], Step [32/84], Loss: 0.2961\n",
      "Epoch [18/100], Step [33/84], Loss: 0.3315\n",
      "Epoch [18/100], Step [34/84], Loss: 0.2945\n",
      "Epoch [18/100], Step [35/84], Loss: 0.3279\n",
      "Epoch [18/100], Step [36/84], Loss: 0.3285\n",
      "Epoch [18/100], Step [37/84], Loss: 0.3005\n",
      "Epoch [18/100], Step [38/84], Loss: 0.3320\n",
      "Epoch [18/100], Step [39/84], Loss: 0.2973\n",
      "Epoch [18/100], Step [40/84], Loss: 0.2971\n",
      "Epoch [18/100], Step [41/84], Loss: 0.3069\n",
      "Epoch [18/100], Step [42/84], Loss: 0.3164\n",
      "Epoch [18/100], Step [43/84], Loss: 0.3182\n",
      "Epoch [18/100], Step [44/84], Loss: 0.3329\n",
      "Epoch [18/100], Step [45/84], Loss: 0.2954\n",
      "Epoch [18/100], Step [46/84], Loss: 0.2883\n",
      "Epoch [18/100], Step [47/84], Loss: 0.3147\n",
      "Epoch [18/100], Step [48/84], Loss: 0.3169\n",
      "Epoch [18/100], Step [49/84], Loss: 0.2993\n",
      "Epoch [18/100], Step [50/84], Loss: 0.3173\n",
      "Epoch [18/100], Step [51/84], Loss: 0.2678\n",
      "Epoch [18/100], Step [52/84], Loss: 0.3034\n",
      "Epoch [18/100], Step [53/84], Loss: 0.2968\n",
      "Epoch [18/100], Step [54/84], Loss: 0.2841\n",
      "Epoch [18/100], Step [55/84], Loss: 0.2850\n",
      "Epoch [18/100], Step [56/84], Loss: 0.3487\n",
      "Epoch [18/100], Step [57/84], Loss: 0.3106\n",
      "Epoch [18/100], Step [58/84], Loss: 0.2930\n",
      "Epoch [18/100], Step [59/84], Loss: 0.3063\n",
      "Epoch [18/100], Step [60/84], Loss: 0.3033\n",
      "Epoch [18/100], Step [61/84], Loss: 0.2743\n",
      "Epoch [18/100], Step [62/84], Loss: 0.3273\n",
      "Epoch [18/100], Step [63/84], Loss: 0.2967\n",
      "Epoch [18/100], Step [64/84], Loss: 0.2975\n",
      "Epoch [18/100], Step [65/84], Loss: 0.3000\n",
      "Epoch [18/100], Step [66/84], Loss: 0.2842\n",
      "Epoch [18/100], Step [67/84], Loss: 0.2956\n",
      "Epoch [18/100], Step [68/84], Loss: 0.2917\n",
      "Epoch [18/100], Step [69/84], Loss: 0.2914\n",
      "Epoch [18/100], Step [70/84], Loss: 0.2942\n",
      "Epoch [18/100], Step [71/84], Loss: 0.2940\n",
      "Epoch [18/100], Step [72/84], Loss: 0.2904\n",
      "Epoch [18/100], Step [73/84], Loss: 0.3136\n",
      "Epoch [18/100], Step [74/84], Loss: 0.3030\n",
      "Epoch [18/100], Step [75/84], Loss: 0.2955\n",
      "Epoch [18/100], Step [76/84], Loss: 0.3148\n",
      "Epoch [18/100], Step [77/84], Loss: 0.3144\n",
      "Epoch [18/100], Step [78/84], Loss: 0.2752\n",
      "Epoch [18/100], Step [79/84], Loss: 0.2840\n",
      "Epoch [18/100], Step [80/84], Loss: 0.3127\n",
      "Epoch [18/100], Step [81/84], Loss: 0.2975\n",
      "Epoch [18/100], Step [82/84], Loss: 0.2916\n",
      "Epoch [18/100], Step [83/84], Loss: 0.3109\n",
      "Epoch [18/100], Step [84/84], Loss: 0.3075\n",
      "Epoch [19/100], Step [1/84], Loss: 0.2911\n",
      "Epoch [19/100], Step [2/84], Loss: 0.3019\n",
      "Epoch [19/100], Step [3/84], Loss: 0.2918\n",
      "Epoch [19/100], Step [4/84], Loss: 0.2942\n",
      "Epoch [19/100], Step [5/84], Loss: 0.3069\n",
      "Epoch [19/100], Step [6/84], Loss: 0.3033\n",
      "Epoch [19/100], Step [7/84], Loss: 0.3056\n",
      "Epoch [19/100], Step [8/84], Loss: 0.2877\n",
      "Epoch [19/100], Step [9/84], Loss: 0.2763\n",
      "Epoch [19/100], Step [10/84], Loss: 0.2962\n",
      "Epoch [19/100], Step [11/84], Loss: 0.3088\n",
      "Epoch [19/100], Step [12/84], Loss: 0.2818\n",
      "Epoch [19/100], Step [13/84], Loss: 0.2989\n",
      "Epoch [19/100], Step [14/84], Loss: 0.3108\n",
      "Epoch [19/100], Step [15/84], Loss: 0.2869\n",
      "Epoch [19/100], Step [16/84], Loss: 0.3112\n",
      "Epoch [19/100], Step [17/84], Loss: 0.2902\n",
      "Epoch [19/100], Step [18/84], Loss: 0.3182\n",
      "Epoch [19/100], Step [19/84], Loss: 0.3060\n",
      "Epoch [19/100], Step [20/84], Loss: 0.3159\n",
      "Epoch [19/100], Step [21/84], Loss: 0.3038\n",
      "Epoch [19/100], Step [22/84], Loss: 0.2999\n",
      "Epoch [19/100], Step [23/84], Loss: 0.2784\n",
      "Epoch [19/100], Step [24/84], Loss: 0.2999\n",
      "Epoch [19/100], Step [25/84], Loss: 0.2806\n",
      "Epoch [19/100], Step [26/84], Loss: 0.2994\n",
      "Epoch [19/100], Step [27/84], Loss: 0.2826\n",
      "Epoch [19/100], Step [28/84], Loss: 0.2757\n",
      "Epoch [19/100], Step [29/84], Loss: 0.3113\n",
      "Epoch [19/100], Step [30/84], Loss: 0.2622\n",
      "Epoch [19/100], Step [31/84], Loss: 0.3230\n",
      "Epoch [19/100], Step [32/84], Loss: 0.2875\n",
      "Epoch [19/100], Step [33/84], Loss: 0.3125\n",
      "Epoch [19/100], Step [34/84], Loss: 0.2731\n",
      "Epoch [19/100], Step [35/84], Loss: 0.3431\n",
      "Epoch [19/100], Step [36/84], Loss: 0.3069\n",
      "Epoch [19/100], Step [37/84], Loss: 0.2779\n",
      "Epoch [19/100], Step [38/84], Loss: 0.2773\n",
      "Epoch [19/100], Step [39/84], Loss: 0.3176\n",
      "Epoch [19/100], Step [40/84], Loss: 0.2807\n",
      "Epoch [19/100], Step [41/84], Loss: 0.2646\n",
      "Epoch [19/100], Step [42/84], Loss: 0.3003\n",
      "Epoch [19/100], Step [43/84], Loss: 0.2925\n",
      "Epoch [19/100], Step [44/84], Loss: 0.3165\n",
      "Epoch [19/100], Step [45/84], Loss: 0.3050\n",
      "Epoch [19/100], Step [46/84], Loss: 0.3127\n",
      "Epoch [19/100], Step [47/84], Loss: 0.2867\n",
      "Epoch [19/100], Step [48/84], Loss: 0.3151\n",
      "Epoch [19/100], Step [49/84], Loss: 0.2986\n",
      "Epoch [19/100], Step [50/84], Loss: 0.2789\n",
      "Epoch [19/100], Step [51/84], Loss: 0.2848\n",
      "Epoch [19/100], Step [52/84], Loss: 0.3024\n",
      "Epoch [19/100], Step [53/84], Loss: 0.2763\n",
      "Epoch [19/100], Step [54/84], Loss: 0.3306\n",
      "Epoch [19/100], Step [55/84], Loss: 0.2853\n",
      "Epoch [19/100], Step [56/84], Loss: 0.3420\n",
      "Epoch [19/100], Step [57/84], Loss: 0.2844\n",
      "Epoch [19/100], Step [58/84], Loss: 0.2843\n",
      "Epoch [19/100], Step [59/84], Loss: 0.3035\n",
      "Epoch [19/100], Step [60/84], Loss: 0.2933\n",
      "Epoch [19/100], Step [61/84], Loss: 0.3189\n",
      "Epoch [19/100], Step [62/84], Loss: 0.2716\n",
      "Epoch [19/100], Step [63/84], Loss: 0.3131\n",
      "Epoch [19/100], Step [64/84], Loss: 0.2993\n",
      "Epoch [19/100], Step [65/84], Loss: 0.3030\n",
      "Epoch [19/100], Step [66/84], Loss: 0.3301\n",
      "Epoch [19/100], Step [67/84], Loss: 0.2851\n",
      "Epoch [19/100], Step [68/84], Loss: 0.2989\n",
      "Epoch [19/100], Step [69/84], Loss: 0.2893\n",
      "Epoch [19/100], Step [70/84], Loss: 0.2758\n",
      "Epoch [19/100], Step [71/84], Loss: 0.2788\n",
      "Epoch [19/100], Step [72/84], Loss: 0.2963\n",
      "Epoch [19/100], Step [73/84], Loss: 0.2936\n",
      "Epoch [19/100], Step [74/84], Loss: 0.2927\n",
      "Epoch [19/100], Step [75/84], Loss: 0.3209\n",
      "Epoch [19/100], Step [76/84], Loss: 0.2999\n",
      "Epoch [19/100], Step [77/84], Loss: 0.2753\n",
      "Epoch [19/100], Step [78/84], Loss: 0.2791\n",
      "Epoch [19/100], Step [79/84], Loss: 0.2681\n",
      "Epoch [19/100], Step [80/84], Loss: 0.3031\n",
      "Epoch [19/100], Step [81/84], Loss: 0.2952\n",
      "Epoch [19/100], Step [82/84], Loss: 0.2774\n",
      "Epoch [19/100], Step [83/84], Loss: 0.2610\n",
      "Epoch [19/100], Step [84/84], Loss: 0.2906\n",
      "Epoch [20/100], Step [1/84], Loss: 0.2968\n",
      "Epoch [20/100], Step [2/84], Loss: 0.2888\n",
      "Epoch [20/100], Step [3/84], Loss: 0.3008\n",
      "Epoch [20/100], Step [4/84], Loss: 0.2845\n",
      "Epoch [20/100], Step [5/84], Loss: 0.3095\n",
      "Epoch [20/100], Step [6/84], Loss: 0.3011\n",
      "Epoch [20/100], Step [7/84], Loss: 0.2806\n",
      "Epoch [20/100], Step [8/84], Loss: 0.3095\n",
      "Epoch [20/100], Step [9/84], Loss: 0.3065\n",
      "Epoch [20/100], Step [10/84], Loss: 0.2759\n",
      "Epoch [20/100], Step [11/84], Loss: 0.2791\n",
      "Epoch [20/100], Step [12/84], Loss: 0.2847\n",
      "Epoch [20/100], Step [13/84], Loss: 0.2877\n",
      "Epoch [20/100], Step [14/84], Loss: 0.3037\n",
      "Epoch [20/100], Step [15/84], Loss: 0.3060\n",
      "Epoch [20/100], Step [16/84], Loss: 0.3176\n",
      "Epoch [20/100], Step [17/84], Loss: 0.3020\n",
      "Epoch [20/100], Step [18/84], Loss: 0.2802\n",
      "Epoch [20/100], Step [19/84], Loss: 0.3003\n",
      "Epoch [20/100], Step [20/84], Loss: 0.2722\n",
      "Epoch [20/100], Step [21/84], Loss: 0.2898\n",
      "Epoch [20/100], Step [22/84], Loss: 0.3004\n",
      "Epoch [20/100], Step [23/84], Loss: 0.2955\n",
      "Epoch [20/100], Step [24/84], Loss: 0.2959\n",
      "Epoch [20/100], Step [25/84], Loss: 0.2746\n",
      "Epoch [20/100], Step [26/84], Loss: 0.2842\n",
      "Epoch [20/100], Step [27/84], Loss: 0.3120\n",
      "Epoch [20/100], Step [28/84], Loss: 0.3018\n",
      "Epoch [20/100], Step [29/84], Loss: 0.2990\n",
      "Epoch [20/100], Step [30/84], Loss: 0.2642\n",
      "Epoch [20/100], Step [31/84], Loss: 0.2857\n",
      "Epoch [20/100], Step [32/84], Loss: 0.2809\n",
      "Epoch [20/100], Step [33/84], Loss: 0.3070\n",
      "Epoch [20/100], Step [34/84], Loss: 0.3038\n",
      "Epoch [20/100], Step [35/84], Loss: 0.2859\n",
      "Epoch [20/100], Step [36/84], Loss: 0.3032\n",
      "Epoch [20/100], Step [37/84], Loss: 0.2819\n",
      "Epoch [20/100], Step [38/84], Loss: 0.2743\n",
      "Epoch [20/100], Step [39/84], Loss: 0.2921\n",
      "Epoch [20/100], Step [40/84], Loss: 0.2879\n",
      "Epoch [20/100], Step [41/84], Loss: 0.2835\n",
      "Epoch [20/100], Step [42/84], Loss: 0.3283\n",
      "Epoch [20/100], Step [43/84], Loss: 0.2707\n",
      "Epoch [20/100], Step [44/84], Loss: 0.2718\n",
      "Epoch [20/100], Step [45/84], Loss: 0.3114\n",
      "Epoch [20/100], Step [46/84], Loss: 0.2922\n",
      "Epoch [20/100], Step [47/84], Loss: 0.2982\n",
      "Epoch [20/100], Step [48/84], Loss: 0.3010\n",
      "Epoch [20/100], Step [49/84], Loss: 0.2710\n",
      "Epoch [20/100], Step [50/84], Loss: 0.3169\n",
      "Epoch [20/100], Step [51/84], Loss: 0.2786\n",
      "Epoch [20/100], Step [52/84], Loss: 0.2798\n",
      "Epoch [20/100], Step [53/84], Loss: 0.3020\n",
      "Epoch [20/100], Step [54/84], Loss: 0.2793\n",
      "Epoch [20/100], Step [55/84], Loss: 0.2739\n",
      "Epoch [20/100], Step [56/84], Loss: 0.2818\n",
      "Epoch [20/100], Step [57/84], Loss: 0.2912\n",
      "Epoch [20/100], Step [58/84], Loss: 0.2690\n",
      "Epoch [20/100], Step [59/84], Loss: 0.2780\n",
      "Epoch [20/100], Step [60/84], Loss: 0.2763\n",
      "Epoch [20/100], Step [61/84], Loss: 0.3129\n",
      "Epoch [20/100], Step [62/84], Loss: 0.2870\n",
      "Epoch [20/100], Step [63/84], Loss: 0.2762\n",
      "Epoch [20/100], Step [64/84], Loss: 0.2994\n",
      "Epoch [20/100], Step [65/84], Loss: 0.2767\n",
      "Epoch [20/100], Step [66/84], Loss: 0.2865\n",
      "Epoch [20/100], Step [67/84], Loss: 0.2944\n",
      "Epoch [20/100], Step [68/84], Loss: 0.2695\n",
      "Epoch [20/100], Step [69/84], Loss: 0.2734\n",
      "Epoch [20/100], Step [70/84], Loss: 0.2937\n",
      "Epoch [20/100], Step [71/84], Loss: 0.3042\n",
      "Epoch [20/100], Step [72/84], Loss: 0.2838\n",
      "Epoch [20/100], Step [73/84], Loss: 0.2940\n",
      "Epoch [20/100], Step [74/84], Loss: 0.3008\n",
      "Epoch [20/100], Step [75/84], Loss: 0.2723\n",
      "Epoch [20/100], Step [76/84], Loss: 0.2597\n",
      "Epoch [20/100], Step [77/84], Loss: 0.2772\n",
      "Epoch [20/100], Step [78/84], Loss: 0.2941\n",
      "Epoch [20/100], Step [79/84], Loss: 0.2967\n",
      "Epoch [20/100], Step [80/84], Loss: 0.2800\n",
      "Epoch [20/100], Step [81/84], Loss: 0.2789\n",
      "Epoch [20/100], Step [82/84], Loss: 0.2843\n",
      "Epoch [20/100], Step [83/84], Loss: 0.3002\n",
      "Epoch [20/100], Step [84/84], Loss: 0.2894\n",
      "Epoch [21/100], Step [1/84], Loss: 0.2589\n",
      "Epoch [21/100], Step [2/84], Loss: 0.2741\n",
      "Epoch [21/100], Step [3/84], Loss: 0.2727\n",
      "Epoch [21/100], Step [4/84], Loss: 0.2792\n",
      "Epoch [21/100], Step [5/84], Loss: 0.2939\n",
      "Epoch [21/100], Step [6/84], Loss: 0.2617\n",
      "Epoch [21/100], Step [7/84], Loss: 0.2840\n",
      "Epoch [21/100], Step [8/84], Loss: 0.2780\n",
      "Epoch [21/100], Step [9/84], Loss: 0.2976\n",
      "Epoch [21/100], Step [10/84], Loss: 0.2704\n",
      "Epoch [21/100], Step [11/84], Loss: 0.2795\n",
      "Epoch [21/100], Step [12/84], Loss: 0.2786\n",
      "Epoch [21/100], Step [13/84], Loss: 0.2606\n",
      "Epoch [21/100], Step [14/84], Loss: 0.2677\n",
      "Epoch [21/100], Step [15/84], Loss: 0.3137\n",
      "Epoch [21/100], Step [16/84], Loss: 0.2886\n",
      "Epoch [21/100], Step [17/84], Loss: 0.2896\n",
      "Epoch [21/100], Step [18/84], Loss: 0.2875\n",
      "Epoch [21/100], Step [19/84], Loss: 0.2799\n",
      "Epoch [21/100], Step [20/84], Loss: 0.2878\n",
      "Epoch [21/100], Step [21/84], Loss: 0.2667\n",
      "Epoch [21/100], Step [22/84], Loss: 0.2582\n",
      "Epoch [21/100], Step [23/84], Loss: 0.2752\n",
      "Epoch [21/100], Step [24/84], Loss: 0.2699\n",
      "Epoch [21/100], Step [25/84], Loss: 0.2930\n",
      "Epoch [21/100], Step [26/84], Loss: 0.2607\n",
      "Epoch [21/100], Step [27/84], Loss: 0.2802\n",
      "Epoch [21/100], Step [28/84], Loss: 0.2821\n",
      "Epoch [21/100], Step [29/84], Loss: 0.3036\n",
      "Epoch [21/100], Step [30/84], Loss: 0.2934\n",
      "Epoch [21/100], Step [31/84], Loss: 0.3237\n",
      "Epoch [21/100], Step [32/84], Loss: 0.2681\n",
      "Epoch [21/100], Step [33/84], Loss: 0.2798\n",
      "Epoch [21/100], Step [34/84], Loss: 0.3017\n",
      "Epoch [21/100], Step [35/84], Loss: 0.2901\n",
      "Epoch [21/100], Step [36/84], Loss: 0.2673\n",
      "Epoch [21/100], Step [37/84], Loss: 0.2797\n",
      "Epoch [21/100], Step [38/84], Loss: 0.2907\n",
      "Epoch [21/100], Step [39/84], Loss: 0.2707\n",
      "Epoch [21/100], Step [40/84], Loss: 0.2643\n",
      "Epoch [21/100], Step [41/84], Loss: 0.2912\n",
      "Epoch [21/100], Step [42/84], Loss: 0.2997\n",
      "Epoch [21/100], Step [43/84], Loss: 0.2866\n",
      "Epoch [21/100], Step [44/84], Loss: 0.3448\n",
      "Epoch [21/100], Step [45/84], Loss: 0.2754\n",
      "Epoch [21/100], Step [46/84], Loss: 0.2790\n",
      "Epoch [21/100], Step [47/84], Loss: 0.2684\n",
      "Epoch [21/100], Step [48/84], Loss: 0.2776\n",
      "Epoch [21/100], Step [49/84], Loss: 0.3090\n",
      "Epoch [21/100], Step [50/84], Loss: 0.2791\n",
      "Epoch [21/100], Step [51/84], Loss: 0.2939\n",
      "Epoch [21/100], Step [52/84], Loss: 0.2943\n",
      "Epoch [21/100], Step [53/84], Loss: 0.2863\n",
      "Epoch [21/100], Step [54/84], Loss: 0.2928\n",
      "Epoch [21/100], Step [55/84], Loss: 0.2773\n",
      "Epoch [21/100], Step [56/84], Loss: 0.2559\n",
      "Epoch [21/100], Step [57/84], Loss: 0.2848\n",
      "Epoch [21/100], Step [58/84], Loss: 0.2816\n",
      "Epoch [21/100], Step [59/84], Loss: 0.2820\n",
      "Epoch [21/100], Step [60/84], Loss: 0.2788\n",
      "Epoch [21/100], Step [61/84], Loss: 0.2696\n",
      "Epoch [21/100], Step [62/84], Loss: 0.2738\n",
      "Epoch [21/100], Step [63/84], Loss: 0.3011\n",
      "Epoch [21/100], Step [64/84], Loss: 0.2872\n",
      "Epoch [21/100], Step [65/84], Loss: 0.2749\n",
      "Epoch [21/100], Step [66/84], Loss: 0.2847\n",
      "Epoch [21/100], Step [67/84], Loss: 0.2981\n",
      "Epoch [21/100], Step [68/84], Loss: 0.2798\n",
      "Epoch [21/100], Step [69/84], Loss: 0.2852\n",
      "Epoch [21/100], Step [70/84], Loss: 0.2759\n",
      "Epoch [21/100], Step [71/84], Loss: 0.2809\n",
      "Epoch [21/100], Step [72/84], Loss: 0.2742\n",
      "Epoch [21/100], Step [73/84], Loss: 0.2799\n",
      "Epoch [21/100], Step [74/84], Loss: 0.2771\n",
      "Epoch [21/100], Step [75/84], Loss: 0.2741\n",
      "Epoch [21/100], Step [76/84], Loss: 0.2690\n",
      "Epoch [21/100], Step [77/84], Loss: 0.2769\n",
      "Epoch [21/100], Step [78/84], Loss: 0.2797\n",
      "Epoch [21/100], Step [79/84], Loss: 0.2871\n",
      "Epoch [21/100], Step [80/84], Loss: 0.2559\n",
      "Epoch [21/100], Step [81/84], Loss: 0.2790\n",
      "Epoch [21/100], Step [82/84], Loss: 0.2993\n",
      "Epoch [21/100], Step [83/84], Loss: 0.2910\n",
      "Epoch [21/100], Step [84/84], Loss: 0.3133\n",
      "Epoch [22/100], Step [1/84], Loss: 0.3017\n",
      "Epoch [22/100], Step [2/84], Loss: 0.2731\n",
      "Epoch [22/100], Step [3/84], Loss: 0.2674\n",
      "Epoch [22/100], Step [4/84], Loss: 0.2622\n",
      "Epoch [22/100], Step [5/84], Loss: 0.2611\n",
      "Epoch [22/100], Step [6/84], Loss: 0.2529\n",
      "Epoch [22/100], Step [7/84], Loss: 0.2788\n",
      "Epoch [22/100], Step [8/84], Loss: 0.2620\n",
      "Epoch [22/100], Step [9/84], Loss: 0.2633\n",
      "Epoch [22/100], Step [10/84], Loss: 0.2971\n",
      "Epoch [22/100], Step [11/84], Loss: 0.2625\n",
      "Epoch [22/100], Step [12/84], Loss: 0.3042\n",
      "Epoch [22/100], Step [13/84], Loss: 0.2620\n",
      "Epoch [22/100], Step [14/84], Loss: 0.2663\n",
      "Epoch [22/100], Step [15/84], Loss: 0.2786\n",
      "Epoch [22/100], Step [16/84], Loss: 0.2862\n",
      "Epoch [22/100], Step [17/84], Loss: 0.2802\n",
      "Epoch [22/100], Step [18/84], Loss: 0.2870\n",
      "Epoch [22/100], Step [19/84], Loss: 0.2997\n",
      "Epoch [22/100], Step [20/84], Loss: 0.2631\n",
      "Epoch [22/100], Step [21/84], Loss: 0.2592\n",
      "Epoch [22/100], Step [22/84], Loss: 0.2611\n",
      "Epoch [22/100], Step [23/84], Loss: 0.2749\n",
      "Epoch [22/100], Step [24/84], Loss: 0.2512\n",
      "Epoch [22/100], Step [25/84], Loss: 0.2917\n",
      "Epoch [22/100], Step [26/84], Loss: 0.2900\n",
      "Epoch [22/100], Step [27/84], Loss: 0.2946\n",
      "Epoch [22/100], Step [28/84], Loss: 0.2704\n",
      "Epoch [22/100], Step [29/84], Loss: 0.2635\n",
      "Epoch [22/100], Step [30/84], Loss: 0.2588\n",
      "Epoch [22/100], Step [31/84], Loss: 0.2972\n",
      "Epoch [22/100], Step [32/84], Loss: 0.2691\n",
      "Epoch [22/100], Step [33/84], Loss: 0.2591\n",
      "Epoch [22/100], Step [34/84], Loss: 0.2793\n",
      "Epoch [22/100], Step [35/84], Loss: 0.2726\n",
      "Epoch [22/100], Step [36/84], Loss: 0.2859\n",
      "Epoch [22/100], Step [37/84], Loss: 0.2629\n",
      "Epoch [22/100], Step [38/84], Loss: 0.2530\n",
      "Epoch [22/100], Step [39/84], Loss: 0.2982\n",
      "Epoch [22/100], Step [40/84], Loss: 0.2650\n",
      "Epoch [22/100], Step [41/84], Loss: 0.2620\n",
      "Epoch [22/100], Step [42/84], Loss: 0.2686\n",
      "Epoch [22/100], Step [43/84], Loss: 0.2558\n",
      "Epoch [22/100], Step [44/84], Loss: 0.2432\n",
      "Epoch [22/100], Step [45/84], Loss: 0.2985\n",
      "Epoch [22/100], Step [46/84], Loss: 0.2767\n",
      "Epoch [22/100], Step [47/84], Loss: 0.2450\n",
      "Epoch [22/100], Step [48/84], Loss: 0.2673\n",
      "Epoch [22/100], Step [49/84], Loss: 0.2706\n",
      "Epoch [22/100], Step [50/84], Loss: 0.2978\n",
      "Epoch [22/100], Step [51/84], Loss: 0.2978\n",
      "Epoch [22/100], Step [52/84], Loss: 0.2782\n",
      "Epoch [22/100], Step [53/84], Loss: 0.2925\n",
      "Epoch [22/100], Step [54/84], Loss: 0.2731\n",
      "Epoch [22/100], Step [55/84], Loss: 0.2697\n",
      "Epoch [22/100], Step [56/84], Loss: 0.2707\n",
      "Epoch [22/100], Step [57/84], Loss: 0.2853\n",
      "Epoch [22/100], Step [58/84], Loss: 0.2740\n",
      "Epoch [22/100], Step [59/84], Loss: 0.3107\n",
      "Epoch [22/100], Step [60/84], Loss: 0.2852\n",
      "Epoch [22/100], Step [61/84], Loss: 0.2850\n",
      "Epoch [22/100], Step [62/84], Loss: 0.2995\n",
      "Epoch [22/100], Step [63/84], Loss: 0.2788\n",
      "Epoch [22/100], Step [64/84], Loss: 0.2883\n",
      "Epoch [22/100], Step [65/84], Loss: 0.2631\n",
      "Epoch [22/100], Step [66/84], Loss: 0.2722\n",
      "Epoch [22/100], Step [67/84], Loss: 0.2651\n",
      "Epoch [22/100], Step [68/84], Loss: 0.2839\n",
      "Epoch [22/100], Step [69/84], Loss: 0.2656\n",
      "Epoch [22/100], Step [70/84], Loss: 0.2732\n",
      "Epoch [22/100], Step [71/84], Loss: 0.2650\n",
      "Epoch [22/100], Step [72/84], Loss: 0.2973\n",
      "Epoch [22/100], Step [73/84], Loss: 0.2885\n",
      "Epoch [22/100], Step [74/84], Loss: 0.2692\n",
      "Epoch [22/100], Step [75/84], Loss: 0.2837\n",
      "Epoch [22/100], Step [76/84], Loss: 0.2571\n",
      "Epoch [22/100], Step [77/84], Loss: 0.2564\n",
      "Epoch [22/100], Step [78/84], Loss: 0.2818\n",
      "Epoch [22/100], Step [79/84], Loss: 0.2697\n",
      "Epoch [22/100], Step [80/84], Loss: 0.2555\n",
      "Epoch [22/100], Step [81/84], Loss: 0.2632\n",
      "Epoch [22/100], Step [82/84], Loss: 0.2767\n",
      "Epoch [22/100], Step [83/84], Loss: 0.2521\n",
      "Epoch [22/100], Step [84/84], Loss: 0.2817\n",
      "Epoch [23/100], Step [1/84], Loss: 0.2568\n",
      "Epoch [23/100], Step [2/84], Loss: 0.2959\n",
      "Epoch [23/100], Step [3/84], Loss: 0.2522\n",
      "Epoch [23/100], Step [4/84], Loss: 0.2723\n",
      "Epoch [23/100], Step [5/84], Loss: 0.2557\n",
      "Epoch [23/100], Step [6/84], Loss: 0.2986\n",
      "Epoch [23/100], Step [7/84], Loss: 0.2574\n",
      "Epoch [23/100], Step [8/84], Loss: 0.2783\n",
      "Epoch [23/100], Step [9/84], Loss: 0.2493\n",
      "Epoch [23/100], Step [10/84], Loss: 0.2561\n",
      "Epoch [23/100], Step [11/84], Loss: 0.2565\n",
      "Epoch [23/100], Step [12/84], Loss: 0.2867\n",
      "Epoch [23/100], Step [13/84], Loss: 0.2550\n",
      "Epoch [23/100], Step [14/84], Loss: 0.2766\n",
      "Epoch [23/100], Step [15/84], Loss: 0.2820\n",
      "Epoch [23/100], Step [16/84], Loss: 0.2589\n",
      "Epoch [23/100], Step [17/84], Loss: 0.2718\n",
      "Epoch [23/100], Step [18/84], Loss: 0.2808\n",
      "Epoch [23/100], Step [19/84], Loss: 0.2346\n",
      "Epoch [23/100], Step [20/84], Loss: 0.2732\n",
      "Epoch [23/100], Step [21/84], Loss: 0.2775\n",
      "Epoch [23/100], Step [22/84], Loss: 0.2532\n",
      "Epoch [23/100], Step [23/84], Loss: 0.2757\n",
      "Epoch [23/100], Step [24/84], Loss: 0.2706\n",
      "Epoch [23/100], Step [25/84], Loss: 0.2546\n",
      "Epoch [23/100], Step [26/84], Loss: 0.3282\n",
      "Epoch [23/100], Step [27/84], Loss: 0.2568\n",
      "Epoch [23/100], Step [28/84], Loss: 0.2399\n",
      "Epoch [23/100], Step [29/84], Loss: 0.2639\n",
      "Epoch [23/100], Step [30/84], Loss: 0.2634\n",
      "Epoch [23/100], Step [31/84], Loss: 0.3139\n",
      "Epoch [23/100], Step [32/84], Loss: 0.2763\n",
      "Epoch [23/100], Step [33/84], Loss: 0.2628\n",
      "Epoch [23/100], Step [34/84], Loss: 0.2567\n",
      "Epoch [23/100], Step [35/84], Loss: 0.2619\n",
      "Epoch [23/100], Step [36/84], Loss: 0.2772\n",
      "Epoch [23/100], Step [37/84], Loss: 0.2774\n",
      "Epoch [23/100], Step [38/84], Loss: 0.2628\n",
      "Epoch [23/100], Step [39/84], Loss: 0.2679\n",
      "Epoch [23/100], Step [40/84], Loss: 0.2577\n",
      "Epoch [23/100], Step [41/84], Loss: 0.2846\n",
      "Epoch [23/100], Step [42/84], Loss: 0.2806\n",
      "Epoch [23/100], Step [43/84], Loss: 0.2943\n",
      "Epoch [23/100], Step [44/84], Loss: 0.2526\n",
      "Epoch [23/100], Step [45/84], Loss: 0.2766\n",
      "Epoch [23/100], Step [46/84], Loss: 0.2410\n",
      "Epoch [23/100], Step [47/84], Loss: 0.2763\n",
      "Epoch [23/100], Step [48/84], Loss: 0.2753\n",
      "Epoch [23/100], Step [49/84], Loss: 0.2682\n",
      "Epoch [23/100], Step [50/84], Loss: 0.2853\n",
      "Epoch [23/100], Step [51/84], Loss: 0.2655\n",
      "Epoch [23/100], Step [52/84], Loss: 0.2499\n",
      "Epoch [23/100], Step [53/84], Loss: 0.2644\n",
      "Epoch [23/100], Step [54/84], Loss: 0.2669\n",
      "Epoch [23/100], Step [55/84], Loss: 0.2866\n",
      "Epoch [23/100], Step [56/84], Loss: 0.2774\n",
      "Epoch [23/100], Step [57/84], Loss: 0.2582\n",
      "Epoch [23/100], Step [58/84], Loss: 0.2686\n",
      "Epoch [23/100], Step [59/84], Loss: 0.2581\n",
      "Epoch [23/100], Step [60/84], Loss: 0.2755\n",
      "Epoch [23/100], Step [61/84], Loss: 0.2605\n",
      "Epoch [23/100], Step [62/84], Loss: 0.2923\n",
      "Epoch [23/100], Step [63/84], Loss: 0.2491\n",
      "Epoch [23/100], Step [64/84], Loss: 0.2557\n",
      "Epoch [23/100], Step [65/84], Loss: 0.2623\n",
      "Epoch [23/100], Step [66/84], Loss: 0.2467\n",
      "Epoch [23/100], Step [67/84], Loss: 0.2972\n",
      "Epoch [23/100], Step [68/84], Loss: 0.2794\n",
      "Epoch [23/100], Step [69/84], Loss: 0.2662\n",
      "Epoch [23/100], Step [70/84], Loss: 0.2908\n",
      "Epoch [23/100], Step [71/84], Loss: 0.2447\n",
      "Epoch [23/100], Step [72/84], Loss: 0.2670\n",
      "Epoch [23/100], Step [73/84], Loss: 0.2895\n",
      "Epoch [23/100], Step [74/84], Loss: 0.2566\n",
      "Epoch [23/100], Step [75/84], Loss: 0.2525\n",
      "Epoch [23/100], Step [76/84], Loss: 0.2806\n",
      "Epoch [23/100], Step [77/84], Loss: 0.2656\n",
      "Epoch [23/100], Step [78/84], Loss: 0.2940\n",
      "Epoch [23/100], Step [79/84], Loss: 0.2462\n",
      "Epoch [23/100], Step [80/84], Loss: 0.2793\n",
      "Epoch [23/100], Step [81/84], Loss: 0.2674\n",
      "Epoch [23/100], Step [82/84], Loss: 0.2747\n",
      "Epoch [23/100], Step [83/84], Loss: 0.3110\n",
      "Epoch [23/100], Step [84/84], Loss: 0.2668\n",
      "Epoch [24/100], Step [1/84], Loss: 0.2486\n",
      "Epoch [24/100], Step [2/84], Loss: 0.2567\n",
      "Epoch [24/100], Step [3/84], Loss: 0.3179\n",
      "Epoch [24/100], Step [4/84], Loss: 0.2486\n",
      "Epoch [24/100], Step [5/84], Loss: 0.2724\n",
      "Epoch [24/100], Step [6/84], Loss: 0.2697\n",
      "Epoch [24/100], Step [7/84], Loss: 0.2667\n",
      "Epoch [24/100], Step [8/84], Loss: 0.2735\n",
      "Epoch [24/100], Step [9/84], Loss: 0.2763\n",
      "Epoch [24/100], Step [10/84], Loss: 0.3104\n",
      "Epoch [24/100], Step [11/84], Loss: 0.2648\n",
      "Epoch [24/100], Step [12/84], Loss: 0.3087\n",
      "Epoch [24/100], Step [13/84], Loss: 0.2398\n",
      "Epoch [24/100], Step [14/84], Loss: 0.2601\n",
      "Epoch [24/100], Step [15/84], Loss: 0.2428\n",
      "Epoch [24/100], Step [16/84], Loss: 0.2527\n",
      "Epoch [24/100], Step [17/84], Loss: 0.2491\n",
      "Epoch [24/100], Step [18/84], Loss: 0.2603\n",
      "Epoch [24/100], Step [19/84], Loss: 0.2582\n",
      "Epoch [24/100], Step [20/84], Loss: 0.2455\n",
      "Epoch [24/100], Step [21/84], Loss: 0.2645\n",
      "Epoch [24/100], Step [22/84], Loss: 0.2609\n",
      "Epoch [24/100], Step [23/84], Loss: 0.2419\n",
      "Epoch [24/100], Step [24/84], Loss: 0.2415\n",
      "Epoch [24/100], Step [25/84], Loss: 0.2550\n",
      "Epoch [24/100], Step [26/84], Loss: 0.3029\n",
      "Epoch [24/100], Step [27/84], Loss: 0.2954\n",
      "Epoch [24/100], Step [28/84], Loss: 0.2738\n",
      "Epoch [24/100], Step [29/84], Loss: 0.2825\n",
      "Epoch [24/100], Step [30/84], Loss: 0.2414\n",
      "Epoch [24/100], Step [31/84], Loss: 0.2582\n",
      "Epoch [24/100], Step [32/84], Loss: 0.2584\n",
      "Epoch [24/100], Step [33/84], Loss: 0.2364\n",
      "Epoch [24/100], Step [34/84], Loss: 0.2693\n",
      "Epoch [24/100], Step [35/84], Loss: 0.2984\n",
      "Epoch [24/100], Step [36/84], Loss: 0.2630\n",
      "Epoch [24/100], Step [37/84], Loss: 0.2854\n",
      "Epoch [24/100], Step [38/84], Loss: 0.2664\n",
      "Epoch [24/100], Step [39/84], Loss: 0.2497\n",
      "Epoch [24/100], Step [40/84], Loss: 0.2885\n",
      "Epoch [24/100], Step [41/84], Loss: 0.2728\n",
      "Epoch [24/100], Step [42/84], Loss: 0.2562\n",
      "Epoch [24/100], Step [43/84], Loss: 0.2543\n",
      "Epoch [24/100], Step [44/84], Loss: 0.2426\n",
      "Epoch [24/100], Step [45/84], Loss: 0.2392\n",
      "Epoch [24/100], Step [46/84], Loss: 0.2771\n",
      "Epoch [24/100], Step [47/84], Loss: 0.2881\n",
      "Epoch [24/100], Step [48/84], Loss: 0.2623\n",
      "Epoch [24/100], Step [49/84], Loss: 0.2602\n",
      "Epoch [24/100], Step [50/84], Loss: 0.2462\n",
      "Epoch [24/100], Step [51/84], Loss: 0.2462\n",
      "Epoch [24/100], Step [52/84], Loss: 0.2641\n",
      "Epoch [24/100], Step [53/84], Loss: 0.2683\n",
      "Epoch [24/100], Step [54/84], Loss: 0.2688\n",
      "Epoch [24/100], Step [55/84], Loss: 0.2896\n",
      "Epoch [24/100], Step [56/84], Loss: 0.2465\n",
      "Epoch [24/100], Step [57/84], Loss: 0.2632\n",
      "Epoch [24/100], Step [58/84], Loss: 0.2582\n",
      "Epoch [24/100], Step [59/84], Loss: 0.2529\n",
      "Epoch [24/100], Step [60/84], Loss: 0.2549\n",
      "Epoch [24/100], Step [61/84], Loss: 0.2500\n",
      "Epoch [24/100], Step [62/84], Loss: 0.2644\n",
      "Epoch [24/100], Step [63/84], Loss: 0.2557\n",
      "Epoch [24/100], Step [64/84], Loss: 0.2726\n",
      "Epoch [24/100], Step [65/84], Loss: 0.2501\n",
      "Epoch [24/100], Step [66/84], Loss: 0.2376\n",
      "Epoch [24/100], Step [67/84], Loss: 0.2880\n",
      "Epoch [24/100], Step [68/84], Loss: 0.2726\n",
      "Epoch [24/100], Step [69/84], Loss: 0.2372\n",
      "Epoch [24/100], Step [70/84], Loss: 0.2509\n",
      "Epoch [24/100], Step [71/84], Loss: 0.2709\n",
      "Epoch [24/100], Step [72/84], Loss: 0.2421\n",
      "Epoch [24/100], Step [73/84], Loss: 0.2870\n",
      "Epoch [24/100], Step [74/84], Loss: 0.2508\n",
      "Epoch [24/100], Step [75/84], Loss: 0.2400\n",
      "Epoch [24/100], Step [76/84], Loss: 0.2572\n",
      "Epoch [24/100], Step [77/84], Loss: 0.2313\n",
      "Epoch [24/100], Step [78/84], Loss: 0.2737\n",
      "Epoch [24/100], Step [79/84], Loss: 0.2754\n",
      "Epoch [24/100], Step [80/84], Loss: 0.2778\n",
      "Epoch [24/100], Step [81/84], Loss: 0.2538\n",
      "Epoch [24/100], Step [82/84], Loss: 0.2375\n",
      "Epoch [24/100], Step [83/84], Loss: 0.2693\n",
      "Epoch [24/100], Step [84/84], Loss: 0.2521\n",
      "Epoch [25/100], Step [1/84], Loss: 0.2705\n",
      "Epoch [25/100], Step [2/84], Loss: 0.2573\n",
      "Epoch [25/100], Step [3/84], Loss: 0.2590\n",
      "Epoch [25/100], Step [4/84], Loss: 0.2807\n",
      "Epoch [25/100], Step [5/84], Loss: 0.2724\n",
      "Epoch [25/100], Step [6/84], Loss: 0.2675\n",
      "Epoch [25/100], Step [7/84], Loss: 0.2539\n",
      "Epoch [25/100], Step [8/84], Loss: 0.2937\n",
      "Epoch [25/100], Step [9/84], Loss: 0.2722\n",
      "Epoch [25/100], Step [10/84], Loss: 0.2594\n",
      "Epoch [25/100], Step [11/84], Loss: 0.2571\n",
      "Epoch [25/100], Step [12/84], Loss: 0.2739\n",
      "Epoch [25/100], Step [13/84], Loss: 0.2430\n",
      "Epoch [25/100], Step [14/84], Loss: 0.2621\n",
      "Epoch [25/100], Step [15/84], Loss: 0.2621\n",
      "Epoch [25/100], Step [16/84], Loss: 0.2450\n",
      "Epoch [25/100], Step [17/84], Loss: 0.2409\n",
      "Epoch [25/100], Step [18/84], Loss: 0.2409\n",
      "Epoch [25/100], Step [19/84], Loss: 0.2965\n",
      "Epoch [25/100], Step [20/84], Loss: 0.2550\n",
      "Epoch [25/100], Step [21/84], Loss: 0.2812\n",
      "Epoch [25/100], Step [22/84], Loss: 0.2626\n",
      "Epoch [25/100], Step [23/84], Loss: 0.2840\n",
      "Epoch [25/100], Step [24/84], Loss: 0.2434\n",
      "Epoch [25/100], Step [25/84], Loss: 0.2336\n",
      "Epoch [25/100], Step [26/84], Loss: 0.2790\n",
      "Epoch [25/100], Step [27/84], Loss: 0.2432\n",
      "Epoch [25/100], Step [28/84], Loss: 0.2457\n",
      "Epoch [25/100], Step [29/84], Loss: 0.2612\n",
      "Epoch [25/100], Step [30/84], Loss: 0.2484\n",
      "Epoch [25/100], Step [31/84], Loss: 0.2428\n",
      "Epoch [25/100], Step [32/84], Loss: 0.2293\n",
      "Epoch [25/100], Step [33/84], Loss: 0.2637\n",
      "Epoch [25/100], Step [34/84], Loss: 0.2440\n",
      "Epoch [25/100], Step [35/84], Loss: 0.2409\n",
      "Epoch [25/100], Step [36/84], Loss: 0.2239\n",
      "Epoch [25/100], Step [37/84], Loss: 0.2657\n",
      "Epoch [25/100], Step [38/84], Loss: 0.2768\n",
      "Epoch [25/100], Step [39/84], Loss: 0.2383\n",
      "Epoch [25/100], Step [40/84], Loss: 0.2724\n",
      "Epoch [25/100], Step [41/84], Loss: 0.2591\n",
      "Epoch [25/100], Step [42/84], Loss: 0.2424\n",
      "Epoch [25/100], Step [43/84], Loss: 0.2613\n",
      "Epoch [25/100], Step [44/84], Loss: 0.2471\n",
      "Epoch [25/100], Step [45/84], Loss: 0.2372\n",
      "Epoch [25/100], Step [46/84], Loss: 0.2544\n",
      "Epoch [25/100], Step [47/84], Loss: 0.2487\n",
      "Epoch [25/100], Step [48/84], Loss: 0.2656\n",
      "Epoch [25/100], Step [49/84], Loss: 0.2690\n",
      "Epoch [25/100], Step [50/84], Loss: 0.2634\n",
      "Epoch [25/100], Step [51/84], Loss: 0.2610\n",
      "Epoch [25/100], Step [52/84], Loss: 0.2438\n",
      "Epoch [25/100], Step [53/84], Loss: 0.2445\n",
      "Epoch [25/100], Step [54/84], Loss: 0.2556\n",
      "Epoch [25/100], Step [55/84], Loss: 0.2763\n",
      "Epoch [25/100], Step [56/84], Loss: 0.2575\n",
      "Epoch [25/100], Step [57/84], Loss: 0.2647\n",
      "Epoch [25/100], Step [58/84], Loss: 0.2637\n",
      "Epoch [25/100], Step [59/84], Loss: 0.2494\n",
      "Epoch [25/100], Step [60/84], Loss: 0.2700\n",
      "Epoch [25/100], Step [61/84], Loss: 0.2579\n",
      "Epoch [25/100], Step [62/84], Loss: 0.2447\n",
      "Epoch [25/100], Step [63/84], Loss: 0.2464\n",
      "Epoch [25/100], Step [64/84], Loss: 0.2619\n",
      "Epoch [25/100], Step [65/84], Loss: 0.2222\n",
      "Epoch [25/100], Step [66/84], Loss: 0.2663\n",
      "Epoch [25/100], Step [67/84], Loss: 0.2660\n",
      "Epoch [25/100], Step [68/84], Loss: 0.2775\n",
      "Epoch [25/100], Step [69/84], Loss: 0.2408\n",
      "Epoch [25/100], Step [70/84], Loss: 0.2482\n",
      "Epoch [25/100], Step [71/84], Loss: 0.2320\n",
      "Epoch [25/100], Step [72/84], Loss: 0.2614\n",
      "Epoch [25/100], Step [73/84], Loss: 0.2413\n",
      "Epoch [25/100], Step [74/84], Loss: 0.2388\n",
      "Epoch [25/100], Step [75/84], Loss: 0.2524\n",
      "Epoch [25/100], Step [76/84], Loss: 0.2370\n",
      "Epoch [25/100], Step [77/84], Loss: 0.2496\n",
      "Epoch [25/100], Step [78/84], Loss: 0.2556\n",
      "Epoch [25/100], Step [79/84], Loss: 0.2774\n",
      "Epoch [25/100], Step [80/84], Loss: 0.2610\n",
      "Epoch [25/100], Step [81/84], Loss: 0.2734\n",
      "Epoch [25/100], Step [82/84], Loss: 0.2439\n",
      "Epoch [25/100], Step [83/84], Loss: 0.2526\n",
      "Epoch [25/100], Step [84/84], Loss: 0.3035\n",
      "Epoch [26/100], Step [1/84], Loss: 0.2282\n",
      "Epoch [26/100], Step [2/84], Loss: 0.2683\n",
      "Epoch [26/100], Step [3/84], Loss: 0.2567\n",
      "Epoch [26/100], Step [4/84], Loss: 0.2462\n",
      "Epoch [26/100], Step [5/84], Loss: 0.2688\n",
      "Epoch [26/100], Step [6/84], Loss: 0.2565\n",
      "Epoch [26/100], Step [7/84], Loss: 0.2394\n",
      "Epoch [26/100], Step [8/84], Loss: 0.2362\n",
      "Epoch [26/100], Step [9/84], Loss: 0.2512\n",
      "Epoch [26/100], Step [10/84], Loss: 0.2421\n",
      "Epoch [26/100], Step [11/84], Loss: 0.2480\n",
      "Epoch [26/100], Step [12/84], Loss: 0.2465\n",
      "Epoch [26/100], Step [13/84], Loss: 0.2550\n",
      "Epoch [26/100], Step [14/84], Loss: 0.2490\n",
      "Epoch [26/100], Step [15/84], Loss: 0.2496\n",
      "Epoch [26/100], Step [16/84], Loss: 0.2387\n",
      "Epoch [26/100], Step [17/84], Loss: 0.2608\n",
      "Epoch [26/100], Step [18/84], Loss: 0.2408\n",
      "Epoch [26/100], Step [19/84], Loss: 0.2441\n",
      "Epoch [26/100], Step [20/84], Loss: 0.2751\n",
      "Epoch [26/100], Step [21/84], Loss: 0.2384\n",
      "Epoch [26/100], Step [22/84], Loss: 0.2450\n",
      "Epoch [26/100], Step [23/84], Loss: 0.2793\n",
      "Epoch [26/100], Step [24/84], Loss: 0.2302\n",
      "Epoch [26/100], Step [25/84], Loss: 0.2525\n",
      "Epoch [26/100], Step [26/84], Loss: 0.2533\n",
      "Epoch [26/100], Step [27/84], Loss: 0.2377\n",
      "Epoch [26/100], Step [28/84], Loss: 0.2483\n",
      "Epoch [26/100], Step [29/84], Loss: 0.2515\n",
      "Epoch [26/100], Step [30/84], Loss: 0.2523\n",
      "Epoch [26/100], Step [31/84], Loss: 0.2814\n",
      "Epoch [26/100], Step [32/84], Loss: 0.2518\n",
      "Epoch [26/100], Step [33/84], Loss: 0.2379\n",
      "Epoch [26/100], Step [34/84], Loss: 0.2493\n",
      "Epoch [26/100], Step [35/84], Loss: 0.2634\n",
      "Epoch [26/100], Step [36/84], Loss: 0.2501\n",
      "Epoch [26/100], Step [37/84], Loss: 0.2374\n",
      "Epoch [26/100], Step [38/84], Loss: 0.2567\n",
      "Epoch [26/100], Step [39/84], Loss: 0.2619\n",
      "Epoch [26/100], Step [40/84], Loss: 0.2354\n",
      "Epoch [26/100], Step [41/84], Loss: 0.2422\n",
      "Epoch [26/100], Step [42/84], Loss: 0.2731\n",
      "Epoch [26/100], Step [43/84], Loss: 0.2323\n",
      "Epoch [26/100], Step [44/84], Loss: 0.2454\n",
      "Epoch [26/100], Step [45/84], Loss: 0.2692\n",
      "Epoch [26/100], Step [46/84], Loss: 0.2547\n",
      "Epoch [26/100], Step [47/84], Loss: 0.2608\n",
      "Epoch [26/100], Step [48/84], Loss: 0.2559\n",
      "Epoch [26/100], Step [49/84], Loss: 0.2824\n",
      "Epoch [26/100], Step [50/84], Loss: 0.2328\n",
      "Epoch [26/100], Step [51/84], Loss: 0.2377\n",
      "Epoch [26/100], Step [52/84], Loss: 0.2333\n",
      "Epoch [26/100], Step [53/84], Loss: 0.2509\n",
      "Epoch [26/100], Step [54/84], Loss: 0.2408\n",
      "Epoch [26/100], Step [55/84], Loss: 0.2394\n",
      "Epoch [26/100], Step [56/84], Loss: 0.2740\n",
      "Epoch [26/100], Step [57/84], Loss: 0.2519\n",
      "Epoch [26/100], Step [58/84], Loss: 0.2528\n",
      "Epoch [26/100], Step [59/84], Loss: 0.2516\n",
      "Epoch [26/100], Step [60/84], Loss: 0.2387\n",
      "Epoch [26/100], Step [61/84], Loss: 0.2344\n",
      "Epoch [26/100], Step [62/84], Loss: 0.2724\n",
      "Epoch [26/100], Step [63/84], Loss: 0.2405\n",
      "Epoch [26/100], Step [64/84], Loss: 0.2499\n",
      "Epoch [26/100], Step [65/84], Loss: 0.2491\n",
      "Epoch [26/100], Step [66/84], Loss: 0.2595\n",
      "Epoch [26/100], Step [67/84], Loss: 0.2412\n",
      "Epoch [26/100], Step [68/84], Loss: 0.2333\n",
      "Epoch [26/100], Step [69/84], Loss: 0.2380\n",
      "Epoch [26/100], Step [70/84], Loss: 0.2330\n",
      "Epoch [26/100], Step [71/84], Loss: 0.2402\n",
      "Epoch [26/100], Step [72/84], Loss: 0.2318\n",
      "Epoch [26/100], Step [73/84], Loss: 0.2397\n",
      "Epoch [26/100], Step [74/84], Loss: 0.2280\n",
      "Epoch [26/100], Step [75/84], Loss: 0.2435\n",
      "Epoch [26/100], Step [76/84], Loss: 0.2387\n",
      "Epoch [26/100], Step [77/84], Loss: 0.2531\n",
      "Epoch [26/100], Step [78/84], Loss: 0.2290\n",
      "Epoch [26/100], Step [79/84], Loss: 0.2520\n",
      "Epoch [26/100], Step [80/84], Loss: 0.2153\n",
      "Epoch [26/100], Step [81/84], Loss: 0.2661\n",
      "Epoch [26/100], Step [82/84], Loss: 0.2605\n",
      "Epoch [26/100], Step [83/84], Loss: 0.2458\n",
      "Epoch [26/100], Step [84/84], Loss: 0.3592\n",
      "Epoch [27/100], Step [1/84], Loss: 0.2410\n",
      "Epoch [27/100], Step [2/84], Loss: 0.2499\n",
      "Epoch [27/100], Step [3/84], Loss: 0.2460\n",
      "Epoch [27/100], Step [4/84], Loss: 0.2500\n",
      "Epoch [27/100], Step [5/84], Loss: 0.2413\n",
      "Epoch [27/100], Step [6/84], Loss: 0.2535\n",
      "Epoch [27/100], Step [7/84], Loss: 0.2416\n",
      "Epoch [27/100], Step [8/84], Loss: 0.2525\n",
      "Epoch [27/100], Step [9/84], Loss: 0.2625\n",
      "Epoch [27/100], Step [10/84], Loss: 0.2609\n",
      "Epoch [27/100], Step [11/84], Loss: 0.2531\n",
      "Epoch [27/100], Step [12/84], Loss: 0.2359\n",
      "Epoch [27/100], Step [13/84], Loss: 0.2307\n",
      "Epoch [27/100], Step [14/84], Loss: 0.2286\n",
      "Epoch [27/100], Step [15/84], Loss: 0.2293\n",
      "Epoch [27/100], Step [16/84], Loss: 0.2612\n",
      "Epoch [27/100], Step [17/84], Loss: 0.2437\n",
      "Epoch [27/100], Step [18/84], Loss: 0.2725\n",
      "Epoch [27/100], Step [19/84], Loss: 0.2374\n",
      "Epoch [27/100], Step [20/84], Loss: 0.2434\n",
      "Epoch [27/100], Step [21/84], Loss: 0.2446\n",
      "Epoch [27/100], Step [22/84], Loss: 0.2485\n",
      "Epoch [27/100], Step [23/84], Loss: 0.2230\n",
      "Epoch [27/100], Step [24/84], Loss: 0.2505\n",
      "Epoch [27/100], Step [25/84], Loss: 0.2362\n",
      "Epoch [27/100], Step [26/84], Loss: 0.2711\n",
      "Epoch [27/100], Step [27/84], Loss: 0.2584\n",
      "Epoch [27/100], Step [28/84], Loss: 0.2432\n",
      "Epoch [27/100], Step [29/84], Loss: 0.2452\n",
      "Epoch [27/100], Step [30/84], Loss: 0.2509\n",
      "Epoch [27/100], Step [31/84], Loss: 0.2579\n",
      "Epoch [27/100], Step [32/84], Loss: 0.2304\n",
      "Epoch [27/100], Step [33/84], Loss: 0.2609\n",
      "Epoch [27/100], Step [34/84], Loss: 0.2290\n",
      "Epoch [27/100], Step [35/84], Loss: 0.2421\n",
      "Epoch [27/100], Step [36/84], Loss: 0.2385\n",
      "Epoch [27/100], Step [37/84], Loss: 0.2381\n",
      "Epoch [27/100], Step [38/84], Loss: 0.2354\n",
      "Epoch [27/100], Step [39/84], Loss: 0.2448\n",
      "Epoch [27/100], Step [40/84], Loss: 0.2336\n",
      "Epoch [27/100], Step [41/84], Loss: 0.2718\n",
      "Epoch [27/100], Step [42/84], Loss: 0.2253\n",
      "Epoch [27/100], Step [43/84], Loss: 0.2318\n",
      "Epoch [27/100], Step [44/84], Loss: 0.2408\n",
      "Epoch [27/100], Step [45/84], Loss: 0.2370\n",
      "Epoch [27/100], Step [46/84], Loss: 0.2383\n",
      "Epoch [27/100], Step [47/84], Loss: 0.2526\n",
      "Epoch [27/100], Step [48/84], Loss: 0.2417\n",
      "Epoch [27/100], Step [49/84], Loss: 0.2337\n",
      "Epoch [27/100], Step [50/84], Loss: 0.2579\n",
      "Epoch [27/100], Step [51/84], Loss: 0.2495\n",
      "Epoch [27/100], Step [52/84], Loss: 0.2328\n",
      "Epoch [27/100], Step [53/84], Loss: 0.2501\n",
      "Epoch [27/100], Step [54/84], Loss: 0.2800\n",
      "Epoch [27/100], Step [55/84], Loss: 0.2388\n",
      "Epoch [27/100], Step [56/84], Loss: 0.2237\n",
      "Epoch [27/100], Step [57/84], Loss: 0.2470\n",
      "Epoch [27/100], Step [58/84], Loss: 0.2701\n",
      "Epoch [27/100], Step [59/84], Loss: 0.2499\n",
      "Epoch [27/100], Step [60/84], Loss: 0.2561\n",
      "Epoch [27/100], Step [61/84], Loss: 0.2528\n",
      "Epoch [27/100], Step [62/84], Loss: 0.2605\n",
      "Epoch [27/100], Step [63/84], Loss: 0.2407\n",
      "Epoch [27/100], Step [64/84], Loss: 0.2387\n",
      "Epoch [27/100], Step [65/84], Loss: 0.2317\n",
      "Epoch [27/100], Step [66/84], Loss: 0.2184\n",
      "Epoch [27/100], Step [67/84], Loss: 0.2759\n",
      "Epoch [27/100], Step [68/84], Loss: 0.2463\n",
      "Epoch [27/100], Step [69/84], Loss: 0.2525\n",
      "Epoch [27/100], Step [70/84], Loss: 0.2610\n",
      "Epoch [27/100], Step [71/84], Loss: 0.2376\n",
      "Epoch [27/100], Step [72/84], Loss: 0.2386\n",
      "Epoch [27/100], Step [73/84], Loss: 0.2421\n",
      "Epoch [27/100], Step [74/84], Loss: 0.2505\n",
      "Epoch [27/100], Step [75/84], Loss: 0.2401\n",
      "Epoch [27/100], Step [76/84], Loss: 0.2297\n",
      "Epoch [27/100], Step [77/84], Loss: 0.2640\n",
      "Epoch [27/100], Step [78/84], Loss: 0.2353\n",
      "Epoch [27/100], Step [79/84], Loss: 0.2868\n",
      "Epoch [27/100], Step [80/84], Loss: 0.2391\n",
      "Epoch [27/100], Step [81/84], Loss: 0.2725\n",
      "Epoch [27/100], Step [82/84], Loss: 0.2701\n",
      "Epoch [27/100], Step [83/84], Loss: 0.2294\n",
      "Epoch [27/100], Step [84/84], Loss: 0.3365\n",
      "Epoch [28/100], Step [1/84], Loss: 0.2629\n",
      "Epoch [28/100], Step [2/84], Loss: 0.2547\n",
      "Epoch [28/100], Step [3/84], Loss: 0.2486\n",
      "Epoch [28/100], Step [4/84], Loss: 0.2343\n",
      "Epoch [28/100], Step [5/84], Loss: 0.2540\n",
      "Epoch [28/100], Step [6/84], Loss: 0.2702\n",
      "Epoch [28/100], Step [7/84], Loss: 0.2179\n",
      "Epoch [28/100], Step [8/84], Loss: 0.2275\n",
      "Epoch [28/100], Step [9/84], Loss: 0.2597\n",
      "Epoch [28/100], Step [10/84], Loss: 0.3012\n",
      "Epoch [28/100], Step [11/84], Loss: 0.2782\n",
      "Epoch [28/100], Step [12/84], Loss: 0.2425\n",
      "Epoch [28/100], Step [13/84], Loss: 0.2692\n",
      "Epoch [28/100], Step [14/84], Loss: 0.2637\n",
      "Epoch [28/100], Step [15/84], Loss: 0.2484\n",
      "Epoch [28/100], Step [16/84], Loss: 0.2431\n",
      "Epoch [28/100], Step [17/84], Loss: 0.2487\n",
      "Epoch [28/100], Step [18/84], Loss: 0.2401\n",
      "Epoch [28/100], Step [19/84], Loss: 0.2468\n",
      "Epoch [28/100], Step [20/84], Loss: 0.2217\n",
      "Epoch [28/100], Step [21/84], Loss: 0.2398\n",
      "Epoch [28/100], Step [22/84], Loss: 0.2317\n",
      "Epoch [28/100], Step [23/84], Loss: 0.2315\n",
      "Epoch [28/100], Step [24/84], Loss: 0.2513\n",
      "Epoch [28/100], Step [25/84], Loss: 0.2340\n",
      "Epoch [28/100], Step [26/84], Loss: 0.2538\n",
      "Epoch [28/100], Step [27/84], Loss: 0.2351\n",
      "Epoch [28/100], Step [28/84], Loss: 0.2418\n",
      "Epoch [28/100], Step [29/84], Loss: 0.2394\n",
      "Epoch [28/100], Step [30/84], Loss: 0.2651\n",
      "Epoch [28/100], Step [31/84], Loss: 0.2690\n",
      "Epoch [28/100], Step [32/84], Loss: 0.2263\n",
      "Epoch [28/100], Step [33/84], Loss: 0.2524\n",
      "Epoch [28/100], Step [34/84], Loss: 0.2615\n",
      "Epoch [28/100], Step [35/84], Loss: 0.2440\n",
      "Epoch [28/100], Step [36/84], Loss: 0.2285\n",
      "Epoch [28/100], Step [37/84], Loss: 0.2402\n",
      "Epoch [28/100], Step [38/84], Loss: 0.2300\n",
      "Epoch [28/100], Step [39/84], Loss: 0.2671\n",
      "Epoch [28/100], Step [40/84], Loss: 0.2362\n",
      "Epoch [28/100], Step [41/84], Loss: 0.2167\n",
      "Epoch [28/100], Step [42/84], Loss: 0.2273\n",
      "Epoch [28/100], Step [43/84], Loss: 0.2344\n",
      "Epoch [28/100], Step [44/84], Loss: 0.2228\n",
      "Epoch [28/100], Step [45/84], Loss: 0.2436\n",
      "Epoch [28/100], Step [46/84], Loss: 0.2251\n",
      "Epoch [28/100], Step [47/84], Loss: 0.2549\n",
      "Epoch [28/100], Step [48/84], Loss: 0.2387\n",
      "Epoch [28/100], Step [49/84], Loss: 0.2441\n",
      "Epoch [28/100], Step [50/84], Loss: 0.3098\n",
      "Epoch [28/100], Step [51/84], Loss: 0.2485\n",
      "Epoch [28/100], Step [52/84], Loss: 0.2422\n",
      "Epoch [28/100], Step [53/84], Loss: 0.2313\n",
      "Epoch [28/100], Step [54/84], Loss: 0.2763\n",
      "Epoch [28/100], Step [55/84], Loss: 0.2310\n",
      "Epoch [28/100], Step [56/84], Loss: 0.2239\n",
      "Epoch [28/100], Step [57/84], Loss: 0.2293\n",
      "Epoch [28/100], Step [58/84], Loss: 0.2325\n",
      "Epoch [28/100], Step [59/84], Loss: 0.2322\n",
      "Epoch [28/100], Step [60/84], Loss: 0.2324\n",
      "Epoch [28/100], Step [61/84], Loss: 0.2232\n",
      "Epoch [28/100], Step [62/84], Loss: 0.2463\n",
      "Epoch [28/100], Step [63/84], Loss: 0.2488\n",
      "Epoch [28/100], Step [64/84], Loss: 0.2282\n",
      "Epoch [28/100], Step [65/84], Loss: 0.2588\n",
      "Epoch [28/100], Step [66/84], Loss: 0.2251\n",
      "Epoch [28/100], Step [67/84], Loss: 0.2551\n",
      "Epoch [28/100], Step [68/84], Loss: 0.2394\n",
      "Epoch [28/100], Step [69/84], Loss: 0.2385\n",
      "Epoch [28/100], Step [70/84], Loss: 0.2336\n",
      "Epoch [28/100], Step [71/84], Loss: 0.2593\n",
      "Epoch [28/100], Step [72/84], Loss: 0.2390\n",
      "Epoch [28/100], Step [73/84], Loss: 0.2562\n",
      "Epoch [28/100], Step [74/84], Loss: 0.2630\n",
      "Epoch [28/100], Step [75/84], Loss: 0.2344\n",
      "Epoch [28/100], Step [76/84], Loss: 0.2530\n",
      "Epoch [28/100], Step [77/84], Loss: 0.2416\n",
      "Epoch [28/100], Step [78/84], Loss: 0.2575\n",
      "Epoch [28/100], Step [79/84], Loss: 0.2574\n",
      "Epoch [28/100], Step [80/84], Loss: 0.2332\n",
      "Epoch [28/100], Step [81/84], Loss: 0.2068\n",
      "Epoch [28/100], Step [82/84], Loss: 0.2311\n",
      "Epoch [28/100], Step [83/84], Loss: 0.2217\n",
      "Epoch [28/100], Step [84/84], Loss: 0.2574\n",
      "Epoch [29/100], Step [1/84], Loss: 0.2486\n",
      "Epoch [29/100], Step [2/84], Loss: 0.2223\n",
      "Epoch [29/100], Step [3/84], Loss: 0.2471\n",
      "Epoch [29/100], Step [4/84], Loss: 0.2648\n",
      "Epoch [29/100], Step [5/84], Loss: 0.2321\n",
      "Epoch [29/100], Step [6/84], Loss: 0.2488\n",
      "Epoch [29/100], Step [7/84], Loss: 0.2335\n",
      "Epoch [29/100], Step [8/84], Loss: 0.2290\n",
      "Epoch [29/100], Step [9/84], Loss: 0.2451\n",
      "Epoch [29/100], Step [10/84], Loss: 0.2225\n",
      "Epoch [29/100], Step [11/84], Loss: 0.2512\n",
      "Epoch [29/100], Step [12/84], Loss: 0.2457\n",
      "Epoch [29/100], Step [13/84], Loss: 0.2355\n",
      "Epoch [29/100], Step [14/84], Loss: 0.2473\n",
      "Epoch [29/100], Step [15/84], Loss: 0.2443\n",
      "Epoch [29/100], Step [16/84], Loss: 0.2423\n",
      "Epoch [29/100], Step [17/84], Loss: 0.2264\n",
      "Epoch [29/100], Step [18/84], Loss: 0.2401\n",
      "Epoch [29/100], Step [19/84], Loss: 0.2302\n",
      "Epoch [29/100], Step [20/84], Loss: 0.2228\n",
      "Epoch [29/100], Step [21/84], Loss: 0.2373\n",
      "Epoch [29/100], Step [22/84], Loss: 0.2551\n",
      "Epoch [29/100], Step [23/84], Loss: 0.2462\n",
      "Epoch [29/100], Step [24/84], Loss: 0.2147\n",
      "Epoch [29/100], Step [25/84], Loss: 0.2318\n",
      "Epoch [29/100], Step [26/84], Loss: 0.2268\n",
      "Epoch [29/100], Step [27/84], Loss: 0.2498\n",
      "Epoch [29/100], Step [28/84], Loss: 0.2242\n",
      "Epoch [29/100], Step [29/84], Loss: 0.2331\n",
      "Epoch [29/100], Step [30/84], Loss: 0.2320\n",
      "Epoch [29/100], Step [31/84], Loss: 0.2313\n",
      "Epoch [29/100], Step [32/84], Loss: 0.2113\n",
      "Epoch [29/100], Step [33/84], Loss: 0.2351\n",
      "Epoch [29/100], Step [34/84], Loss: 0.2449\n",
      "Epoch [29/100], Step [35/84], Loss: 0.2380\n",
      "Epoch [29/100], Step [36/84], Loss: 0.2450\n",
      "Epoch [29/100], Step [37/84], Loss: 0.2521\n",
      "Epoch [29/100], Step [38/84], Loss: 0.2595\n",
      "Epoch [29/100], Step [39/84], Loss: 0.2467\n",
      "Epoch [29/100], Step [40/84], Loss: 0.2322\n",
      "Epoch [29/100], Step [41/84], Loss: 0.2336\n",
      "Epoch [29/100], Step [42/84], Loss: 0.2027\n",
      "Epoch [29/100], Step [43/84], Loss: 0.2107\n",
      "Epoch [29/100], Step [44/84], Loss: 0.2623\n",
      "Epoch [29/100], Step [45/84], Loss: 0.2346\n",
      "Epoch [29/100], Step [46/84], Loss: 0.2085\n",
      "Epoch [29/100], Step [47/84], Loss: 0.2364\n",
      "Epoch [29/100], Step [48/84], Loss: 0.2627\n",
      "Epoch [29/100], Step [49/84], Loss: 0.2339\n",
      "Epoch [29/100], Step [50/84], Loss: 0.2094\n",
      "Epoch [29/100], Step [51/84], Loss: 0.2280\n",
      "Epoch [29/100], Step [52/84], Loss: 0.2247\n",
      "Epoch [29/100], Step [53/84], Loss: 0.2267\n",
      "Epoch [29/100], Step [54/84], Loss: 0.2772\n",
      "Epoch [29/100], Step [55/84], Loss: 0.2312\n",
      "Epoch [29/100], Step [56/84], Loss: 0.2212\n",
      "Epoch [29/100], Step [57/84], Loss: 0.2369\n",
      "Epoch [29/100], Step [58/84], Loss: 0.2189\n",
      "Epoch [29/100], Step [59/84], Loss: 0.2300\n",
      "Epoch [29/100], Step [60/84], Loss: 0.2265\n",
      "Epoch [29/100], Step [61/84], Loss: 0.2437\n",
      "Epoch [29/100], Step [62/84], Loss: 0.2358\n",
      "Epoch [29/100], Step [63/84], Loss: 0.2306\n",
      "Epoch [29/100], Step [64/84], Loss: 0.2313\n",
      "Epoch [29/100], Step [65/84], Loss: 0.2273\n",
      "Epoch [29/100], Step [66/84], Loss: 0.2592\n",
      "Epoch [29/100], Step [67/84], Loss: 0.2355\n",
      "Epoch [29/100], Step [68/84], Loss: 0.2237\n",
      "Epoch [29/100], Step [69/84], Loss: 0.2615\n",
      "Epoch [29/100], Step [70/84], Loss: 0.2349\n",
      "Epoch [29/100], Step [71/84], Loss: 0.2249\n",
      "Epoch [29/100], Step [72/84], Loss: 0.2431\n",
      "Epoch [29/100], Step [73/84], Loss: 0.2191\n",
      "Epoch [29/100], Step [74/84], Loss: 0.2175\n",
      "Epoch [29/100], Step [75/84], Loss: 0.2497\n",
      "Epoch [29/100], Step [76/84], Loss: 0.2608\n",
      "Epoch [29/100], Step [77/84], Loss: 0.2574\n",
      "Epoch [29/100], Step [78/84], Loss: 0.2314\n",
      "Epoch [29/100], Step [79/84], Loss: 0.2227\n",
      "Epoch [29/100], Step [80/84], Loss: 0.2120\n",
      "Epoch [29/100], Step [81/84], Loss: 0.2216\n",
      "Epoch [29/100], Step [82/84], Loss: 0.2289\n",
      "Epoch [29/100], Step [83/84], Loss: 0.2576\n",
      "Epoch [29/100], Step [84/84], Loss: 0.2636\n",
      "Epoch [30/100], Step [1/84], Loss: 0.2640\n",
      "Epoch [30/100], Step [2/84], Loss: 0.2071\n",
      "Epoch [30/100], Step [3/84], Loss: 0.2329\n",
      "Epoch [30/100], Step [4/84], Loss: 0.2391\n",
      "Epoch [30/100], Step [5/84], Loss: 0.2445\n",
      "Epoch [30/100], Step [6/84], Loss: 0.2317\n",
      "Epoch [30/100], Step [7/84], Loss: 0.2456\n",
      "Epoch [30/100], Step [8/84], Loss: 0.2307\n",
      "Epoch [30/100], Step [9/84], Loss: 0.2336\n",
      "Epoch [30/100], Step [10/84], Loss: 0.2298\n",
      "Epoch [30/100], Step [11/84], Loss: 0.2300\n",
      "Epoch [30/100], Step [12/84], Loss: 0.2350\n",
      "Epoch [30/100], Step [13/84], Loss: 0.2776\n",
      "Epoch [30/100], Step [14/84], Loss: 0.2072\n",
      "Epoch [30/100], Step [15/84], Loss: 0.2316\n",
      "Epoch [30/100], Step [16/84], Loss: 0.2335\n",
      "Epoch [30/100], Step [17/84], Loss: 0.2380\n",
      "Epoch [30/100], Step [18/84], Loss: 0.2341\n",
      "Epoch [30/100], Step [19/84], Loss: 0.2658\n",
      "Epoch [30/100], Step [20/84], Loss: 0.2196\n",
      "Epoch [30/100], Step [21/84], Loss: 0.2379\n",
      "Epoch [30/100], Step [22/84], Loss: 0.2228\n",
      "Epoch [30/100], Step [23/84], Loss: 0.2118\n",
      "Epoch [30/100], Step [24/84], Loss: 0.2278\n",
      "Epoch [30/100], Step [25/84], Loss: 0.2305\n",
      "Epoch [30/100], Step [26/84], Loss: 0.2234\n",
      "Epoch [30/100], Step [27/84], Loss: 0.2466\n",
      "Epoch [30/100], Step [28/84], Loss: 0.2156\n",
      "Epoch [30/100], Step [29/84], Loss: 0.2383\n",
      "Epoch [30/100], Step [30/84], Loss: 0.2239\n",
      "Epoch [30/100], Step [31/84], Loss: 0.2450\n",
      "Epoch [30/100], Step [32/84], Loss: 0.2445\n",
      "Epoch [30/100], Step [33/84], Loss: 0.2292\n",
      "Epoch [30/100], Step [34/84], Loss: 0.2368\n",
      "Epoch [30/100], Step [35/84], Loss: 0.2243\n",
      "Epoch [30/100], Step [36/84], Loss: 0.2150\n",
      "Epoch [30/100], Step [37/84], Loss: 0.2206\n",
      "Epoch [30/100], Step [38/84], Loss: 0.2494\n",
      "Epoch [30/100], Step [39/84], Loss: 0.2604\n",
      "Epoch [30/100], Step [40/84], Loss: 0.2201\n",
      "Epoch [30/100], Step [41/84], Loss: 0.2307\n",
      "Epoch [30/100], Step [42/84], Loss: 0.2558\n",
      "Epoch [30/100], Step [43/84], Loss: 0.2477\n",
      "Epoch [30/100], Step [44/84], Loss: 0.2410\n",
      "Epoch [30/100], Step [45/84], Loss: 0.2534\n",
      "Epoch [30/100], Step [46/84], Loss: 0.2360\n",
      "Epoch [30/100], Step [47/84], Loss: 0.2213\n",
      "Epoch [30/100], Step [48/84], Loss: 0.2469\n",
      "Epoch [30/100], Step [49/84], Loss: 0.2212\n",
      "Epoch [30/100], Step [50/84], Loss: 0.2255\n",
      "Epoch [30/100], Step [51/84], Loss: 0.2198\n",
      "Epoch [30/100], Step [52/84], Loss: 0.2283\n",
      "Epoch [30/100], Step [53/84], Loss: 0.2158\n",
      "Epoch [30/100], Step [54/84], Loss: 0.2346\n",
      "Epoch [30/100], Step [55/84], Loss: 0.2268\n",
      "Epoch [30/100], Step [56/84], Loss: 0.2123\n",
      "Epoch [30/100], Step [57/84], Loss: 0.2718\n",
      "Epoch [30/100], Step [58/84], Loss: 0.2172\n",
      "Epoch [30/100], Step [59/84], Loss: 0.2226\n",
      "Epoch [30/100], Step [60/84], Loss: 0.2332\n",
      "Epoch [30/100], Step [61/84], Loss: 0.2107\n",
      "Epoch [30/100], Step [62/84], Loss: 0.2257\n",
      "Epoch [30/100], Step [63/84], Loss: 0.2642\n",
      "Epoch [30/100], Step [64/84], Loss: 0.2243\n",
      "Epoch [30/100], Step [65/84], Loss: 0.2458\n",
      "Epoch [30/100], Step [66/84], Loss: 0.2569\n",
      "Epoch [30/100], Step [67/84], Loss: 0.2320\n",
      "Epoch [30/100], Step [68/84], Loss: 0.2600\n",
      "Epoch [30/100], Step [69/84], Loss: 0.2421\n",
      "Epoch [30/100], Step [70/84], Loss: 0.2328\n",
      "Epoch [30/100], Step [71/84], Loss: 0.2247\n",
      "Epoch [30/100], Step [72/84], Loss: 0.2180\n",
      "Epoch [30/100], Step [73/84], Loss: 0.2317\n",
      "Epoch [30/100], Step [74/84], Loss: 0.2305\n",
      "Epoch [30/100], Step [75/84], Loss: 0.2123\n",
      "Epoch [30/100], Step [76/84], Loss: 0.2181\n",
      "Epoch [30/100], Step [77/84], Loss: 0.2859\n",
      "Epoch [30/100], Step [78/84], Loss: 0.2285\n",
      "Epoch [30/100], Step [79/84], Loss: 0.2327\n",
      "Epoch [30/100], Step [80/84], Loss: 0.2620\n",
      "Epoch [30/100], Step [81/84], Loss: 0.2062\n",
      "Epoch [30/100], Step [82/84], Loss: 0.2185\n",
      "Epoch [30/100], Step [83/84], Loss: 0.2099\n",
      "Epoch [30/100], Step [84/84], Loss: 0.2380\n",
      "Epoch [31/100], Step [1/84], Loss: 0.2407\n",
      "Epoch [31/100], Step [2/84], Loss: 0.2362\n",
      "Epoch [31/100], Step [3/84], Loss: 0.2367\n",
      "Epoch [31/100], Step [4/84], Loss: 0.2353\n",
      "Epoch [31/100], Step [5/84], Loss: 0.2351\n",
      "Epoch [31/100], Step [6/84], Loss: 0.2315\n",
      "Epoch [31/100], Step [7/84], Loss: 0.2325\n",
      "Epoch [31/100], Step [8/84], Loss: 0.2393\n",
      "Epoch [31/100], Step [9/84], Loss: 0.2112\n",
      "Epoch [31/100], Step [10/84], Loss: 0.2560\n",
      "Epoch [31/100], Step [11/84], Loss: 0.2345\n",
      "Epoch [31/100], Step [12/84], Loss: 0.2544\n",
      "Epoch [31/100], Step [13/84], Loss: 0.2249\n",
      "Epoch [31/100], Step [14/84], Loss: 0.2143\n",
      "Epoch [31/100], Step [15/84], Loss: 0.2347\n",
      "Epoch [31/100], Step [16/84], Loss: 0.2304\n",
      "Epoch [31/100], Step [17/84], Loss: 0.2286\n",
      "Epoch [31/100], Step [18/84], Loss: 0.2236\n",
      "Epoch [31/100], Step [19/84], Loss: 0.2084\n",
      "Epoch [31/100], Step [20/84], Loss: 0.2147\n",
      "Epoch [31/100], Step [21/84], Loss: 0.2213\n",
      "Epoch [31/100], Step [22/84], Loss: 0.2038\n",
      "Epoch [31/100], Step [23/84], Loss: 0.2337\n",
      "Epoch [31/100], Step [24/84], Loss: 0.2269\n",
      "Epoch [31/100], Step [25/84], Loss: 0.2406\n",
      "Epoch [31/100], Step [26/84], Loss: 0.2230\n",
      "Epoch [31/100], Step [27/84], Loss: 0.2374\n",
      "Epoch [31/100], Step [28/84], Loss: 0.2134\n",
      "Epoch [31/100], Step [29/84], Loss: 0.2213\n",
      "Epoch [31/100], Step [30/84], Loss: 0.2263\n",
      "Epoch [31/100], Step [31/84], Loss: 0.2257\n",
      "Epoch [31/100], Step [32/84], Loss: 0.2224\n",
      "Epoch [31/100], Step [33/84], Loss: 0.2343\n",
      "Epoch [31/100], Step [34/84], Loss: 0.2173\n",
      "Epoch [31/100], Step [35/84], Loss: 0.2125\n",
      "Epoch [31/100], Step [36/84], Loss: 0.2197\n",
      "Epoch [31/100], Step [37/84], Loss: 0.2143\n",
      "Epoch [31/100], Step [38/84], Loss: 0.2471\n",
      "Epoch [31/100], Step [39/84], Loss: 0.2276\n",
      "Epoch [31/100], Step [40/84], Loss: 0.2204\n",
      "Epoch [31/100], Step [41/84], Loss: 0.2348\n",
      "Epoch [31/100], Step [42/84], Loss: 0.2429\n",
      "Epoch [31/100], Step [43/84], Loss: 0.2317\n",
      "Epoch [31/100], Step [44/84], Loss: 0.2339\n",
      "Epoch [31/100], Step [45/84], Loss: 0.2159\n",
      "Epoch [31/100], Step [46/84], Loss: 0.2433\n",
      "Epoch [31/100], Step [47/84], Loss: 0.2228\n",
      "Epoch [31/100], Step [48/84], Loss: 0.2219\n",
      "Epoch [31/100], Step [49/84], Loss: 0.2395\n",
      "Epoch [31/100], Step [50/84], Loss: 0.2268\n",
      "Epoch [31/100], Step [51/84], Loss: 0.2398\n",
      "Epoch [31/100], Step [52/84], Loss: 0.2325\n",
      "Epoch [31/100], Step [53/84], Loss: 0.2102\n",
      "Epoch [31/100], Step [54/84], Loss: 0.2202\n",
      "Epoch [31/100], Step [55/84], Loss: 0.2630\n",
      "Epoch [31/100], Step [56/84], Loss: 0.2611\n",
      "Epoch [31/100], Step [57/84], Loss: 0.2064\n",
      "Epoch [31/100], Step [58/84], Loss: 0.2043\n",
      "Epoch [31/100], Step [59/84], Loss: 0.2118\n",
      "Epoch [31/100], Step [60/84], Loss: 0.2125\n",
      "Epoch [31/100], Step [61/84], Loss: 0.2256\n",
      "Epoch [31/100], Step [62/84], Loss: 0.2089\n",
      "Epoch [31/100], Step [63/84], Loss: 0.2227\n",
      "Epoch [31/100], Step [64/84], Loss: 0.2305\n",
      "Epoch [31/100], Step [65/84], Loss: 0.2288\n",
      "Epoch [31/100], Step [66/84], Loss: 0.2124\n",
      "Epoch [31/100], Step [67/84], Loss: 0.2076\n",
      "Epoch [31/100], Step [68/84], Loss: 0.2189\n",
      "Epoch [31/100], Step [69/84], Loss: 0.2310\n",
      "Epoch [31/100], Step [70/84], Loss: 0.2692\n",
      "Epoch [31/100], Step [71/84], Loss: 0.2479\n",
      "Epoch [31/100], Step [72/84], Loss: 0.2141\n",
      "Epoch [31/100], Step [73/84], Loss: 0.2356\n",
      "Epoch [31/100], Step [74/84], Loss: 0.2543\n",
      "Epoch [31/100], Step [75/84], Loss: 0.2054\n",
      "Epoch [31/100], Step [76/84], Loss: 0.2348\n",
      "Epoch [31/100], Step [77/84], Loss: 0.2310\n",
      "Epoch [31/100], Step [78/84], Loss: 0.2144\n",
      "Epoch [31/100], Step [79/84], Loss: 0.2301\n",
      "Epoch [31/100], Step [80/84], Loss: 0.2177\n",
      "Epoch [31/100], Step [81/84], Loss: 0.2119\n",
      "Epoch [31/100], Step [82/84], Loss: 0.2136\n",
      "Epoch [31/100], Step [83/84], Loss: 0.2053\n",
      "Epoch [31/100], Step [84/84], Loss: 0.2074\n",
      "Epoch [32/100], Step [1/84], Loss: 0.2362\n",
      "Epoch [32/100], Step [2/84], Loss: 0.2128\n",
      "Epoch [32/100], Step [3/84], Loss: 0.2175\n",
      "Epoch [32/100], Step [4/84], Loss: 0.2066\n",
      "Epoch [32/100], Step [5/84], Loss: 0.2078\n",
      "Epoch [32/100], Step [6/84], Loss: 0.2343\n",
      "Epoch [32/100], Step [7/84], Loss: 0.2459\n",
      "Epoch [32/100], Step [8/84], Loss: 0.2264\n",
      "Epoch [32/100], Step [9/84], Loss: 0.2284\n",
      "Epoch [32/100], Step [10/84], Loss: 0.2648\n",
      "Epoch [32/100], Step [11/84], Loss: 0.2300\n",
      "Epoch [32/100], Step [12/84], Loss: 0.2119\n",
      "Epoch [32/100], Step [13/84], Loss: 0.2281\n",
      "Epoch [32/100], Step [14/84], Loss: 0.2119\n",
      "Epoch [32/100], Step [15/84], Loss: 0.2021\n",
      "Epoch [32/100], Step [16/84], Loss: 0.2051\n",
      "Epoch [32/100], Step [17/84], Loss: 0.1980\n",
      "Epoch [32/100], Step [18/84], Loss: 0.2321\n",
      "Epoch [32/100], Step [19/84], Loss: 0.2158\n",
      "Epoch [32/100], Step [20/84], Loss: 0.2269\n",
      "Epoch [32/100], Step [21/84], Loss: 0.2272\n",
      "Epoch [32/100], Step [22/84], Loss: 0.2248\n",
      "Epoch [32/100], Step [23/84], Loss: 0.2325\n",
      "Epoch [32/100], Step [24/84], Loss: 0.2224\n",
      "Epoch [32/100], Step [25/84], Loss: 0.2198\n",
      "Epoch [32/100], Step [26/84], Loss: 0.2354\n",
      "Epoch [32/100], Step [27/84], Loss: 0.2336\n",
      "Epoch [32/100], Step [28/84], Loss: 0.2264\n",
      "Epoch [32/100], Step [29/84], Loss: 0.2306\n",
      "Epoch [32/100], Step [30/84], Loss: 0.2226\n",
      "Epoch [32/100], Step [31/84], Loss: 0.2049\n",
      "Epoch [32/100], Step [32/84], Loss: 0.2365\n",
      "Epoch [32/100], Step [33/84], Loss: 0.2339\n",
      "Epoch [32/100], Step [34/84], Loss: 0.2061\n",
      "Epoch [32/100], Step [35/84], Loss: 0.2218\n",
      "Epoch [32/100], Step [36/84], Loss: 0.2253\n",
      "Epoch [32/100], Step [37/84], Loss: 0.2191\n",
      "Epoch [32/100], Step [38/84], Loss: 0.2289\n",
      "Epoch [32/100], Step [39/84], Loss: 0.2088\n",
      "Epoch [32/100], Step [40/84], Loss: 0.2093\n",
      "Epoch [32/100], Step [41/84], Loss: 0.2208\n",
      "Epoch [32/100], Step [42/84], Loss: 0.2024\n",
      "Epoch [32/100], Step [43/84], Loss: 0.2316\n",
      "Epoch [32/100], Step [44/84], Loss: 0.1998\n",
      "Epoch [32/100], Step [45/84], Loss: 0.2221\n",
      "Epoch [32/100], Step [46/84], Loss: 0.1978\n",
      "Epoch [32/100], Step [47/84], Loss: 0.2203\n",
      "Epoch [32/100], Step [48/84], Loss: 0.2232\n",
      "Epoch [32/100], Step [49/84], Loss: 0.2404\n",
      "Epoch [32/100], Step [50/84], Loss: 0.2188\n",
      "Epoch [32/100], Step [51/84], Loss: 0.2124\n",
      "Epoch [32/100], Step [52/84], Loss: 0.2111\n",
      "Epoch [32/100], Step [53/84], Loss: 0.2420\n",
      "Epoch [32/100], Step [54/84], Loss: 0.2160\n",
      "Epoch [32/100], Step [55/84], Loss: 0.2134\n",
      "Epoch [32/100], Step [56/84], Loss: 0.2316\n",
      "Epoch [32/100], Step [57/84], Loss: 0.2096\n",
      "Epoch [32/100], Step [58/84], Loss: 0.2377\n",
      "Epoch [32/100], Step [59/84], Loss: 0.2114\n",
      "Epoch [32/100], Step [60/84], Loss: 0.2274\n",
      "Epoch [32/100], Step [61/84], Loss: 0.2260\n",
      "Epoch [32/100], Step [62/84], Loss: 0.1952\n",
      "Epoch [32/100], Step [63/84], Loss: 0.1946\n",
      "Epoch [32/100], Step [64/84], Loss: 0.2241\n",
      "Epoch [32/100], Step [65/84], Loss: 0.2326\n",
      "Epoch [32/100], Step [66/84], Loss: 0.2496\n",
      "Epoch [32/100], Step [67/84], Loss: 0.2356\n",
      "Epoch [32/100], Step [68/84], Loss: 0.2476\n",
      "Epoch [32/100], Step [69/84], Loss: 0.2049\n",
      "Epoch [32/100], Step [70/84], Loss: 0.2284\n",
      "Epoch [32/100], Step [71/84], Loss: 0.2257\n",
      "Epoch [32/100], Step [72/84], Loss: 0.2307\n",
      "Epoch [32/100], Step [73/84], Loss: 0.2262\n",
      "Epoch [32/100], Step [74/84], Loss: 0.2198\n",
      "Epoch [32/100], Step [75/84], Loss: 0.2188\n",
      "Epoch [32/100], Step [76/84], Loss: 0.2052\n",
      "Epoch [32/100], Step [77/84], Loss: 0.2233\n",
      "Epoch [32/100], Step [78/84], Loss: 0.2116\n",
      "Epoch [32/100], Step [79/84], Loss: 0.2178\n",
      "Epoch [32/100], Step [80/84], Loss: 0.2426\n",
      "Epoch [32/100], Step [81/84], Loss: 0.2008\n",
      "Epoch [32/100], Step [82/84], Loss: 0.2272\n",
      "Epoch [32/100], Step [83/84], Loss: 0.2236\n",
      "Epoch [32/100], Step [84/84], Loss: 0.2088\n",
      "Epoch [33/100], Step [1/84], Loss: 0.2303\n",
      "Epoch [33/100], Step [2/84], Loss: 0.2244\n",
      "Epoch [33/100], Step [3/84], Loss: 0.2144\n",
      "Epoch [33/100], Step [4/84], Loss: 0.2050\n",
      "Epoch [33/100], Step [5/84], Loss: 0.2184\n",
      "Epoch [33/100], Step [6/84], Loss: 0.2658\n",
      "Epoch [33/100], Step [7/84], Loss: 0.1982\n",
      "Epoch [33/100], Step [8/84], Loss: 0.2364\n",
      "Epoch [33/100], Step [9/84], Loss: 0.2243\n",
      "Epoch [33/100], Step [10/84], Loss: 0.1991\n",
      "Epoch [33/100], Step [11/84], Loss: 0.2356\n",
      "Epoch [33/100], Step [12/84], Loss: 0.1912\n",
      "Epoch [33/100], Step [13/84], Loss: 0.2267\n",
      "Epoch [33/100], Step [14/84], Loss: 0.2267\n",
      "Epoch [33/100], Step [15/84], Loss: 0.2141\n",
      "Epoch [33/100], Step [16/84], Loss: 0.2238\n",
      "Epoch [33/100], Step [17/84], Loss: 0.2172\n",
      "Epoch [33/100], Step [18/84], Loss: 0.1977\n",
      "Epoch [33/100], Step [19/84], Loss: 0.1985\n",
      "Epoch [33/100], Step [20/84], Loss: 0.2044\n",
      "Epoch [33/100], Step [21/84], Loss: 0.2126\n",
      "Epoch [33/100], Step [22/84], Loss: 0.2647\n",
      "Epoch [33/100], Step [23/84], Loss: 0.1968\n",
      "Epoch [33/100], Step [24/84], Loss: 0.2300\n",
      "Epoch [33/100], Step [25/84], Loss: 0.1948\n",
      "Epoch [33/100], Step [26/84], Loss: 0.2385\n",
      "Epoch [33/100], Step [27/84], Loss: 0.2138\n",
      "Epoch [33/100], Step [28/84], Loss: 0.2174\n",
      "Epoch [33/100], Step [29/84], Loss: 0.2262\n",
      "Epoch [33/100], Step [30/84], Loss: 0.2395\n",
      "Epoch [33/100], Step [31/84], Loss: 0.2330\n",
      "Epoch [33/100], Step [32/84], Loss: 0.2261\n",
      "Epoch [33/100], Step [33/84], Loss: 0.2197\n",
      "Epoch [33/100], Step [34/84], Loss: 0.2153\n",
      "Epoch [33/100], Step [35/84], Loss: 0.2146\n",
      "Epoch [33/100], Step [36/84], Loss: 0.2318\n",
      "Epoch [33/100], Step [37/84], Loss: 0.2200\n",
      "Epoch [33/100], Step [38/84], Loss: 0.2151\n",
      "Epoch [33/100], Step [39/84], Loss: 0.2214\n",
      "Epoch [33/100], Step [40/84], Loss: 0.2029\n",
      "Epoch [33/100], Step [41/84], Loss: 0.2304\n",
      "Epoch [33/100], Step [42/84], Loss: 0.2086\n",
      "Epoch [33/100], Step [43/84], Loss: 0.2152\n",
      "Epoch [33/100], Step [44/84], Loss: 0.1933\n",
      "Epoch [33/100], Step [45/84], Loss: 0.2097\n",
      "Epoch [33/100], Step [46/84], Loss: 0.2073\n",
      "Epoch [33/100], Step [47/84], Loss: 0.2236\n",
      "Epoch [33/100], Step [48/84], Loss: 0.2134\n",
      "Epoch [33/100], Step [49/84], Loss: 0.2109\n",
      "Epoch [33/100], Step [50/84], Loss: 0.2127\n",
      "Epoch [33/100], Step [51/84], Loss: 0.2082\n",
      "Epoch [33/100], Step [52/84], Loss: 0.2068\n",
      "Epoch [33/100], Step [53/84], Loss: 0.2206\n",
      "Epoch [33/100], Step [54/84], Loss: 0.2030\n",
      "Epoch [33/100], Step [55/84], Loss: 0.2371\n",
      "Epoch [33/100], Step [56/84], Loss: 0.2070\n",
      "Epoch [33/100], Step [57/84], Loss: 0.2402\n",
      "Epoch [33/100], Step [58/84], Loss: 0.2094\n",
      "Epoch [33/100], Step [59/84], Loss: 0.2171\n",
      "Epoch [33/100], Step [60/84], Loss: 0.2109\n",
      "Epoch [33/100], Step [61/84], Loss: 0.2237\n",
      "Epoch [33/100], Step [62/84], Loss: 0.2326\n",
      "Epoch [33/100], Step [63/84], Loss: 0.2464\n",
      "Epoch [33/100], Step [64/84], Loss: 0.1874\n",
      "Epoch [33/100], Step [65/84], Loss: 0.2061\n",
      "Epoch [33/100], Step [66/84], Loss: 0.2279\n",
      "Epoch [33/100], Step [67/84], Loss: 0.2293\n",
      "Epoch [33/100], Step [68/84], Loss: 0.2256\n",
      "Epoch [33/100], Step [69/84], Loss: 0.2118\n",
      "Epoch [33/100], Step [70/84], Loss: 0.2024\n",
      "Epoch [33/100], Step [71/84], Loss: 0.2147\n",
      "Epoch [33/100], Step [72/84], Loss: 0.2251\n",
      "Epoch [33/100], Step [73/84], Loss: 0.2217\n",
      "Epoch [33/100], Step [74/84], Loss: 0.2109\n",
      "Epoch [33/100], Step [75/84], Loss: 0.2050\n",
      "Epoch [33/100], Step [76/84], Loss: 0.2251\n",
      "Epoch [33/100], Step [77/84], Loss: 0.2447\n",
      "Epoch [33/100], Step [78/84], Loss: 0.2263\n",
      "Epoch [33/100], Step [79/84], Loss: 0.2146\n",
      "Epoch [33/100], Step [80/84], Loss: 0.2325\n",
      "Epoch [33/100], Step [81/84], Loss: 0.2442\n",
      "Epoch [33/100], Step [82/84], Loss: 0.2110\n",
      "Epoch [33/100], Step [83/84], Loss: 0.2243\n",
      "Epoch [33/100], Step [84/84], Loss: 0.2192\n",
      "Epoch [34/100], Step [1/84], Loss: 0.1918\n",
      "Epoch [34/100], Step [2/84], Loss: 0.2098\n",
      "Epoch [34/100], Step [3/84], Loss: 0.1971\n",
      "Epoch [34/100], Step [4/84], Loss: 0.2109\n",
      "Epoch [34/100], Step [5/84], Loss: 0.2124\n",
      "Epoch [34/100], Step [6/84], Loss: 0.1948\n",
      "Epoch [34/100], Step [7/84], Loss: 0.2205\n",
      "Epoch [34/100], Step [8/84], Loss: 0.2311\n",
      "Epoch [34/100], Step [9/84], Loss: 0.2338\n",
      "Epoch [34/100], Step [10/84], Loss: 0.1986\n",
      "Epoch [34/100], Step [11/84], Loss: 0.2076\n",
      "Epoch [34/100], Step [12/84], Loss: 0.2197\n",
      "Epoch [34/100], Step [13/84], Loss: 0.2289\n",
      "Epoch [34/100], Step [14/84], Loss: 0.2055\n",
      "Epoch [34/100], Step [15/84], Loss: 0.2032\n",
      "Epoch [34/100], Step [16/84], Loss: 0.2243\n",
      "Epoch [34/100], Step [17/84], Loss: 0.2133\n",
      "Epoch [34/100], Step [18/84], Loss: 0.2295\n",
      "Epoch [34/100], Step [19/84], Loss: 0.2586\n",
      "Epoch [34/100], Step [20/84], Loss: 0.2186\n",
      "Epoch [34/100], Step [21/84], Loss: 0.2164\n",
      "Epoch [34/100], Step [22/84], Loss: 0.2133\n",
      "Epoch [34/100], Step [23/84], Loss: 0.1942\n",
      "Epoch [34/100], Step [24/84], Loss: 0.2094\n",
      "Epoch [34/100], Step [25/84], Loss: 0.2005\n",
      "Epoch [34/100], Step [26/84], Loss: 0.2010\n",
      "Epoch [34/100], Step [27/84], Loss: 0.2365\n",
      "Epoch [34/100], Step [28/84], Loss: 0.1976\n",
      "Epoch [34/100], Step [29/84], Loss: 0.2188\n",
      "Epoch [34/100], Step [30/84], Loss: 0.2103\n",
      "Epoch [34/100], Step [31/84], Loss: 0.2169\n",
      "Epoch [34/100], Step [32/84], Loss: 0.2407\n",
      "Epoch [34/100], Step [33/84], Loss: 0.2409\n",
      "Epoch [34/100], Step [34/84], Loss: 0.1987\n",
      "Epoch [34/100], Step [35/84], Loss: 0.2207\n",
      "Epoch [34/100], Step [36/84], Loss: 0.2226\n",
      "Epoch [34/100], Step [37/84], Loss: 0.2042\n",
      "Epoch [34/100], Step [38/84], Loss: 0.2593\n",
      "Epoch [34/100], Step [39/84], Loss: 0.2062\n",
      "Epoch [34/100], Step [40/84], Loss: 0.2420\n",
      "Epoch [34/100], Step [41/84], Loss: 0.2157\n",
      "Epoch [34/100], Step [42/84], Loss: 0.2213\n",
      "Epoch [34/100], Step [43/84], Loss: 0.2228\n",
      "Epoch [34/100], Step [44/84], Loss: 0.2079\n",
      "Epoch [34/100], Step [45/84], Loss: 0.2060\n",
      "Epoch [34/100], Step [46/84], Loss: 0.2628\n",
      "Epoch [34/100], Step [47/84], Loss: 0.2186\n",
      "Epoch [34/100], Step [48/84], Loss: 0.2174\n",
      "Epoch [34/100], Step [49/84], Loss: 0.2299\n",
      "Epoch [34/100], Step [50/84], Loss: 0.2117\n",
      "Epoch [34/100], Step [51/84], Loss: 0.2218\n",
      "Epoch [34/100], Step [52/84], Loss: 0.1987\n",
      "Epoch [34/100], Step [53/84], Loss: 0.1974\n",
      "Epoch [34/100], Step [54/84], Loss: 0.2185\n",
      "Epoch [34/100], Step [55/84], Loss: 0.2129\n",
      "Epoch [34/100], Step [56/84], Loss: 0.1910\n",
      "Epoch [34/100], Step [57/84], Loss: 0.2127\n",
      "Epoch [34/100], Step [58/84], Loss: 0.2093\n",
      "Epoch [34/100], Step [59/84], Loss: 0.2045\n",
      "Epoch [34/100], Step [60/84], Loss: 0.2252\n",
      "Epoch [34/100], Step [61/84], Loss: 0.2281\n",
      "Epoch [34/100], Step [62/84], Loss: 0.2450\n",
      "Epoch [34/100], Step [63/84], Loss: 0.2086\n",
      "Epoch [34/100], Step [64/84], Loss: 0.2121\n",
      "Epoch [34/100], Step [65/84], Loss: 0.2159\n",
      "Epoch [34/100], Step [66/84], Loss: 0.1900\n",
      "Epoch [34/100], Step [67/84], Loss: 0.2434\n",
      "Epoch [34/100], Step [68/84], Loss: 0.2340\n",
      "Epoch [34/100], Step [69/84], Loss: 0.2042\n",
      "Epoch [34/100], Step [70/84], Loss: 0.1916\n",
      "Epoch [34/100], Step [71/84], Loss: 0.2081\n",
      "Epoch [34/100], Step [72/84], Loss: 0.2217\n",
      "Epoch [34/100], Step [73/84], Loss: 0.2119\n",
      "Epoch [34/100], Step [74/84], Loss: 0.2045\n",
      "Epoch [34/100], Step [75/84], Loss: 0.1949\n",
      "Epoch [34/100], Step [76/84], Loss: 0.2023\n",
      "Epoch [34/100], Step [77/84], Loss: 0.2045\n",
      "Epoch [34/100], Step [78/84], Loss: 0.2287\n",
      "Epoch [34/100], Step [79/84], Loss: 0.2017\n",
      "Epoch [34/100], Step [80/84], Loss: 0.2122\n",
      "Epoch [34/100], Step [81/84], Loss: 0.1898\n",
      "Epoch [34/100], Step [82/84], Loss: 0.1942\n",
      "Epoch [34/100], Step [83/84], Loss: 0.2212\n",
      "Epoch [34/100], Step [84/84], Loss: 0.2088\n",
      "Epoch [35/100], Step [1/84], Loss: 0.2148\n",
      "Epoch [35/100], Step [2/84], Loss: 0.1981\n",
      "Epoch [35/100], Step [3/84], Loss: 0.2086\n",
      "Epoch [35/100], Step [4/84], Loss: 0.2154\n",
      "Epoch [35/100], Step [5/84], Loss: 0.2386\n",
      "Epoch [35/100], Step [6/84], Loss: 0.2283\n",
      "Epoch [35/100], Step [7/84], Loss: 0.1945\n",
      "Epoch [35/100], Step [8/84], Loss: 0.2084\n",
      "Epoch [35/100], Step [9/84], Loss: 0.2073\n",
      "Epoch [35/100], Step [10/84], Loss: 0.2145\n",
      "Epoch [35/100], Step [11/84], Loss: 0.2208\n",
      "Epoch [35/100], Step [12/84], Loss: 0.2339\n",
      "Epoch [35/100], Step [13/84], Loss: 0.1933\n",
      "Epoch [35/100], Step [14/84], Loss: 0.2263\n",
      "Epoch [35/100], Step [15/84], Loss: 0.1948\n",
      "Epoch [35/100], Step [16/84], Loss: 0.2097\n",
      "Epoch [35/100], Step [17/84], Loss: 0.2049\n",
      "Epoch [35/100], Step [18/84], Loss: 0.1978\n",
      "Epoch [35/100], Step [19/84], Loss: 0.1972\n",
      "Epoch [35/100], Step [20/84], Loss: 0.2084\n",
      "Epoch [35/100], Step [21/84], Loss: 0.1986\n",
      "Epoch [35/100], Step [22/84], Loss: 0.2131\n",
      "Epoch [35/100], Step [23/84], Loss: 0.1974\n",
      "Epoch [35/100], Step [24/84], Loss: 0.1972\n",
      "Epoch [35/100], Step [25/84], Loss: 0.2014\n",
      "Epoch [35/100], Step [26/84], Loss: 0.2088\n",
      "Epoch [35/100], Step [27/84], Loss: 0.2179\n",
      "Epoch [35/100], Step [28/84], Loss: 0.2086\n",
      "Epoch [35/100], Step [29/84], Loss: 0.1907\n",
      "Epoch [35/100], Step [30/84], Loss: 0.2095\n",
      "Epoch [35/100], Step [31/84], Loss: 0.2204\n",
      "Epoch [35/100], Step [32/84], Loss: 0.2066\n",
      "Epoch [35/100], Step [33/84], Loss: 0.2212\n",
      "Epoch [35/100], Step [34/84], Loss: 0.1953\n",
      "Epoch [35/100], Step [35/84], Loss: 0.2389\n",
      "Epoch [35/100], Step [36/84], Loss: 0.2089\n",
      "Epoch [35/100], Step [37/84], Loss: 0.2027\n",
      "Epoch [35/100], Step [38/84], Loss: 0.2007\n",
      "Epoch [35/100], Step [39/84], Loss: 0.2116\n",
      "Epoch [35/100], Step [40/84], Loss: 0.1994\n",
      "Epoch [35/100], Step [41/84], Loss: 0.2411\n",
      "Epoch [35/100], Step [42/84], Loss: 0.1957\n",
      "Epoch [35/100], Step [43/84], Loss: 0.2177\n",
      "Epoch [35/100], Step [44/84], Loss: 0.2288\n",
      "Epoch [35/100], Step [45/84], Loss: 0.2246\n",
      "Epoch [35/100], Step [46/84], Loss: 0.2365\n",
      "Epoch [35/100], Step [47/84], Loss: 0.2109\n",
      "Epoch [35/100], Step [48/84], Loss: 0.2031\n",
      "Epoch [35/100], Step [49/84], Loss: 0.2188\n",
      "Epoch [35/100], Step [50/84], Loss: 0.2240\n",
      "Epoch [35/100], Step [51/84], Loss: 0.2110\n",
      "Epoch [35/100], Step [52/84], Loss: 0.2087\n",
      "Epoch [35/100], Step [53/84], Loss: 0.2237\n",
      "Epoch [35/100], Step [54/84], Loss: 0.2267\n",
      "Epoch [35/100], Step [55/84], Loss: 0.2078\n",
      "Epoch [35/100], Step [56/84], Loss: 0.2016\n",
      "Epoch [35/100], Step [57/84], Loss: 0.1835\n",
      "Epoch [35/100], Step [58/84], Loss: 0.2235\n",
      "Epoch [35/100], Step [59/84], Loss: 0.2134\n",
      "Epoch [35/100], Step [60/84], Loss: 0.1990\n",
      "Epoch [35/100], Step [61/84], Loss: 0.2152\n",
      "Epoch [35/100], Step [62/84], Loss: 0.2300\n",
      "Epoch [35/100], Step [63/84], Loss: 0.1899\n",
      "Epoch [35/100], Step [64/84], Loss: 0.1912\n",
      "Epoch [35/100], Step [65/84], Loss: 0.2067\n",
      "Epoch [35/100], Step [66/84], Loss: 0.1918\n",
      "Epoch [35/100], Step [67/84], Loss: 0.2115\n",
      "Epoch [35/100], Step [68/84], Loss: 0.2096\n",
      "Epoch [35/100], Step [69/84], Loss: 0.2073\n",
      "Epoch [35/100], Step [70/84], Loss: 0.2046\n",
      "Epoch [35/100], Step [71/84], Loss: 0.2083\n",
      "Epoch [35/100], Step [72/84], Loss: 0.1977\n",
      "Epoch [35/100], Step [73/84], Loss: 0.2095\n",
      "Epoch [35/100], Step [74/84], Loss: 0.2084\n",
      "Epoch [35/100], Step [75/84], Loss: 0.2025\n",
      "Epoch [35/100], Step [76/84], Loss: 0.2351\n",
      "Epoch [35/100], Step [77/84], Loss: 0.1966\n",
      "Epoch [35/100], Step [78/84], Loss: 0.2101\n",
      "Epoch [35/100], Step [79/84], Loss: 0.2247\n",
      "Epoch [35/100], Step [80/84], Loss: 0.2116\n",
      "Epoch [35/100], Step [81/84], Loss: 0.2221\n",
      "Epoch [35/100], Step [82/84], Loss: 0.2172\n",
      "Epoch [35/100], Step [83/84], Loss: 0.2362\n",
      "Epoch [35/100], Step [84/84], Loss: 0.2733\n",
      "Epoch [36/100], Step [1/84], Loss: 0.1938\n",
      "Epoch [36/100], Step [2/84], Loss: 0.2156\n",
      "Epoch [36/100], Step [3/84], Loss: 0.2087\n",
      "Epoch [36/100], Step [4/84], Loss: 0.2055\n",
      "Epoch [36/100], Step [5/84], Loss: 0.2000\n",
      "Epoch [36/100], Step [6/84], Loss: 0.1988\n",
      "Epoch [36/100], Step [7/84], Loss: 0.2040\n",
      "Epoch [36/100], Step [8/84], Loss: 0.1760\n",
      "Epoch [36/100], Step [9/84], Loss: 0.2205\n",
      "Epoch [36/100], Step [10/84], Loss: 0.2081\n",
      "Epoch [36/100], Step [11/84], Loss: 0.2364\n",
      "Epoch [36/100], Step [12/84], Loss: 0.1870\n",
      "Epoch [36/100], Step [13/84], Loss: 0.1863\n",
      "Epoch [36/100], Step [14/84], Loss: 0.2010\n",
      "Epoch [36/100], Step [15/84], Loss: 0.1913\n",
      "Epoch [36/100], Step [16/84], Loss: 0.2178\n",
      "Epoch [36/100], Step [17/84], Loss: 0.2055\n",
      "Epoch [36/100], Step [18/84], Loss: 0.2005\n",
      "Epoch [36/100], Step [19/84], Loss: 0.2556\n",
      "Epoch [36/100], Step [20/84], Loss: 0.2044\n",
      "Epoch [36/100], Step [21/84], Loss: 0.1937\n",
      "Epoch [36/100], Step [22/84], Loss: 0.2077\n",
      "Epoch [36/100], Step [23/84], Loss: 0.1908\n",
      "Epoch [36/100], Step [24/84], Loss: 0.1955\n",
      "Epoch [36/100], Step [25/84], Loss: 0.1934\n",
      "Epoch [36/100], Step [26/84], Loss: 0.1990\n",
      "Epoch [36/100], Step [27/84], Loss: 0.2067\n",
      "Epoch [36/100], Step [28/84], Loss: 0.1953\n",
      "Epoch [36/100], Step [29/84], Loss: 0.1947\n",
      "Epoch [36/100], Step [30/84], Loss: 0.1934\n",
      "Epoch [36/100], Step [31/84], Loss: 0.1839\n",
      "Epoch [36/100], Step [32/84], Loss: 0.1893\n",
      "Epoch [36/100], Step [33/84], Loss: 0.1940\n",
      "Epoch [36/100], Step [34/84], Loss: 0.2208\n",
      "Epoch [36/100], Step [35/84], Loss: 0.2066\n",
      "Epoch [36/100], Step [36/84], Loss: 0.2137\n",
      "Epoch [36/100], Step [37/84], Loss: 0.2029\n",
      "Epoch [36/100], Step [38/84], Loss: 0.2110\n",
      "Epoch [36/100], Step [39/84], Loss: 0.1876\n",
      "Epoch [36/100], Step [40/84], Loss: 0.2123\n",
      "Epoch [36/100], Step [41/84], Loss: 0.2302\n",
      "Epoch [36/100], Step [42/84], Loss: 0.2118\n",
      "Epoch [36/100], Step [43/84], Loss: 0.2019\n",
      "Epoch [36/100], Step [44/84], Loss: 0.2299\n",
      "Epoch [36/100], Step [45/84], Loss: 0.2408\n",
      "Epoch [36/100], Step [46/84], Loss: 0.2074\n",
      "Epoch [36/100], Step [47/84], Loss: 0.2313\n",
      "Epoch [36/100], Step [48/84], Loss: 0.1948\n",
      "Epoch [36/100], Step [49/84], Loss: 0.1900\n",
      "Epoch [36/100], Step [50/84], Loss: 0.1817\n",
      "Epoch [36/100], Step [51/84], Loss: 0.2287\n",
      "Epoch [36/100], Step [52/84], Loss: 0.2176\n",
      "Epoch [36/100], Step [53/84], Loss: 0.2010\n",
      "Epoch [36/100], Step [54/84], Loss: 0.2214\n",
      "Epoch [36/100], Step [55/84], Loss: 0.2082\n",
      "Epoch [36/100], Step [56/84], Loss: 0.1980\n",
      "Epoch [36/100], Step [57/84], Loss: 0.2262\n",
      "Epoch [36/100], Step [58/84], Loss: 0.2015\n",
      "Epoch [36/100], Step [59/84], Loss: 0.2254\n",
      "Epoch [36/100], Step [60/84], Loss: 0.1996\n",
      "Epoch [36/100], Step [61/84], Loss: 0.2168\n",
      "Epoch [36/100], Step [62/84], Loss: 0.2214\n",
      "Epoch [36/100], Step [63/84], Loss: 0.2061\n",
      "Epoch [36/100], Step [64/84], Loss: 0.2037\n",
      "Epoch [36/100], Step [65/84], Loss: 0.2011\n",
      "Epoch [36/100], Step [66/84], Loss: 0.1925\n",
      "Epoch [36/100], Step [67/84], Loss: 0.2082\n",
      "Epoch [36/100], Step [68/84], Loss: 0.2382\n",
      "Epoch [36/100], Step [69/84], Loss: 0.2048\n",
      "Epoch [36/100], Step [70/84], Loss: 0.2433\n",
      "Epoch [36/100], Step [71/84], Loss: 0.2045\n",
      "Epoch [36/100], Step [72/84], Loss: 0.1949\n",
      "Epoch [36/100], Step [73/84], Loss: 0.2032\n",
      "Epoch [36/100], Step [74/84], Loss: 0.2106\n",
      "Epoch [36/100], Step [75/84], Loss: 0.2290\n",
      "Epoch [36/100], Step [76/84], Loss: 0.2077\n",
      "Epoch [36/100], Step [77/84], Loss: 0.1991\n",
      "Epoch [36/100], Step [78/84], Loss: 0.2182\n",
      "Epoch [36/100], Step [79/84], Loss: 0.1970\n",
      "Epoch [36/100], Step [80/84], Loss: 0.1937\n",
      "Epoch [36/100], Step [81/84], Loss: 0.2124\n",
      "Epoch [36/100], Step [82/84], Loss: 0.1939\n",
      "Epoch [36/100], Step [83/84], Loss: 0.2142\n",
      "Epoch [36/100], Step [84/84], Loss: 0.2220\n",
      "Epoch [37/100], Step [1/84], Loss: 0.2143\n",
      "Epoch [37/100], Step [2/84], Loss: 0.1932\n",
      "Epoch [37/100], Step [3/84], Loss: 0.2283\n",
      "Epoch [37/100], Step [4/84], Loss: 0.2137\n",
      "Epoch [37/100], Step [5/84], Loss: 0.2094\n",
      "Epoch [37/100], Step [6/84], Loss: 0.1860\n",
      "Epoch [37/100], Step [7/84], Loss: 0.2199\n",
      "Epoch [37/100], Step [8/84], Loss: 0.2038\n",
      "Epoch [37/100], Step [9/84], Loss: 0.1895\n",
      "Epoch [37/100], Step [10/84], Loss: 0.1979\n",
      "Epoch [37/100], Step [11/84], Loss: 0.1924\n",
      "Epoch [37/100], Step [12/84], Loss: 0.2128\n",
      "Epoch [37/100], Step [13/84], Loss: 0.2147\n",
      "Epoch [37/100], Step [14/84], Loss: 0.2140\n",
      "Epoch [37/100], Step [15/84], Loss: 0.2001\n",
      "Epoch [37/100], Step [16/84], Loss: 0.2059\n",
      "Epoch [37/100], Step [17/84], Loss: 0.1984\n",
      "Epoch [37/100], Step [18/84], Loss: 0.2093\n",
      "Epoch [37/100], Step [19/84], Loss: 0.2048\n",
      "Epoch [37/100], Step [20/84], Loss: 0.1879\n",
      "Epoch [37/100], Step [21/84], Loss: 0.2141\n",
      "Epoch [37/100], Step [22/84], Loss: 0.1971\n",
      "Epoch [37/100], Step [23/84], Loss: 0.1942\n",
      "Epoch [37/100], Step [24/84], Loss: 0.2340\n",
      "Epoch [37/100], Step [25/84], Loss: 0.1955\n",
      "Epoch [37/100], Step [26/84], Loss: 0.2057\n",
      "Epoch [37/100], Step [27/84], Loss: 0.2202\n",
      "Epoch [37/100], Step [28/84], Loss: 0.1889\n",
      "Epoch [37/100], Step [29/84], Loss: 0.1868\n",
      "Epoch [37/100], Step [30/84], Loss: 0.2151\n",
      "Epoch [37/100], Step [31/84], Loss: 0.1906\n",
      "Epoch [37/100], Step [32/84], Loss: 0.2056\n",
      "Epoch [37/100], Step [33/84], Loss: 0.2009\n",
      "Epoch [37/100], Step [34/84], Loss: 0.2343\n",
      "Epoch [37/100], Step [35/84], Loss: 0.2060\n",
      "Epoch [37/100], Step [36/84], Loss: 0.2036\n",
      "Epoch [37/100], Step [37/84], Loss: 0.1907\n",
      "Epoch [37/100], Step [38/84], Loss: 0.1796\n",
      "Epoch [37/100], Step [39/84], Loss: 0.1902\n",
      "Epoch [37/100], Step [40/84], Loss: 0.1948\n",
      "Epoch [37/100], Step [41/84], Loss: 0.2453\n",
      "Epoch [37/100], Step [42/84], Loss: 0.1869\n",
      "Epoch [37/100], Step [43/84], Loss: 0.2479\n",
      "Epoch [37/100], Step [44/84], Loss: 0.1794\n",
      "Epoch [37/100], Step [45/84], Loss: 0.2151\n",
      "Epoch [37/100], Step [46/84], Loss: 0.2026\n",
      "Epoch [37/100], Step [47/84], Loss: 0.1914\n",
      "Epoch [37/100], Step [48/84], Loss: 0.2048\n",
      "Epoch [37/100], Step [49/84], Loss: 0.2133\n",
      "Epoch [37/100], Step [50/84], Loss: 0.1882\n",
      "Epoch [37/100], Step [51/84], Loss: 0.1942\n",
      "Epoch [37/100], Step [52/84], Loss: 0.2009\n",
      "Epoch [37/100], Step [53/84], Loss: 0.1970\n",
      "Epoch [37/100], Step [54/84], Loss: 0.2115\n",
      "Epoch [37/100], Step [55/84], Loss: 0.2153\n",
      "Epoch [37/100], Step [56/84], Loss: 0.1960\n",
      "Epoch [37/100], Step [57/84], Loss: 0.1849\n",
      "Epoch [37/100], Step [58/84], Loss: 0.2066\n",
      "Epoch [37/100], Step [59/84], Loss: 0.2064\n",
      "Epoch [37/100], Step [60/84], Loss: 0.2137\n",
      "Epoch [37/100], Step [61/84], Loss: 0.2151\n",
      "Epoch [37/100], Step [62/84], Loss: 0.2011\n",
      "Epoch [37/100], Step [63/84], Loss: 0.2042\n",
      "Epoch [37/100], Step [64/84], Loss: 0.2068\n",
      "Epoch [37/100], Step [65/84], Loss: 0.1971\n",
      "Epoch [37/100], Step [66/84], Loss: 0.2140\n",
      "Epoch [37/100], Step [67/84], Loss: 0.2049\n",
      "Epoch [37/100], Step [68/84], Loss: 0.2206\n",
      "Epoch [37/100], Step [69/84], Loss: 0.2372\n",
      "Epoch [37/100], Step [70/84], Loss: 0.2305\n",
      "Epoch [37/100], Step [71/84], Loss: 0.2203\n",
      "Epoch [37/100], Step [72/84], Loss: 0.2008\n",
      "Epoch [37/100], Step [73/84], Loss: 0.2155\n",
      "Epoch [37/100], Step [74/84], Loss: 0.1996\n",
      "Epoch [37/100], Step [75/84], Loss: 0.2065\n",
      "Epoch [37/100], Step [76/84], Loss: 0.2176\n",
      "Epoch [37/100], Step [77/84], Loss: 0.2180\n",
      "Epoch [37/100], Step [78/84], Loss: 0.2306\n",
      "Epoch [37/100], Step [79/84], Loss: 0.1922\n",
      "Epoch [37/100], Step [80/84], Loss: 0.1929\n",
      "Epoch [37/100], Step [81/84], Loss: 0.2048\n",
      "Epoch [37/100], Step [82/84], Loss: 0.2145\n",
      "Epoch [37/100], Step [83/84], Loss: 0.2108\n",
      "Epoch [37/100], Step [84/84], Loss: 0.1858\n",
      "Epoch [38/100], Step [1/84], Loss: 0.1992\n",
      "Epoch [38/100], Step [2/84], Loss: 0.1938\n",
      "Epoch [38/100], Step [3/84], Loss: 0.2148\n",
      "Epoch [38/100], Step [4/84], Loss: 0.1949\n",
      "Epoch [38/100], Step [5/84], Loss: 0.1887\n",
      "Epoch [38/100], Step [6/84], Loss: 0.1930\n",
      "Epoch [38/100], Step [7/84], Loss: 0.1906\n",
      "Epoch [38/100], Step [8/84], Loss: 0.1953\n",
      "Epoch [38/100], Step [9/84], Loss: 0.1897\n",
      "Epoch [38/100], Step [10/84], Loss: 0.2257\n",
      "Epoch [38/100], Step [11/84], Loss: 0.2264\n",
      "Epoch [38/100], Step [12/84], Loss: 0.1821\n",
      "Epoch [38/100], Step [13/84], Loss: 0.2172\n",
      "Epoch [38/100], Step [14/84], Loss: 0.1811\n",
      "Epoch [38/100], Step [15/84], Loss: 0.1865\n",
      "Epoch [38/100], Step [16/84], Loss: 0.2006\n",
      "Epoch [38/100], Step [17/84], Loss: 0.2252\n",
      "Epoch [38/100], Step [18/84], Loss: 0.1813\n",
      "Epoch [38/100], Step [19/84], Loss: 0.2036\n",
      "Epoch [38/100], Step [20/84], Loss: 0.1932\n",
      "Epoch [38/100], Step [21/84], Loss: 0.1864\n",
      "Epoch [38/100], Step [22/84], Loss: 0.2159\n",
      "Epoch [38/100], Step [23/84], Loss: 0.2213\n",
      "Epoch [38/100], Step [24/84], Loss: 0.2103\n",
      "Epoch [38/100], Step [25/84], Loss: 0.1900\n",
      "Epoch [38/100], Step [26/84], Loss: 0.2000\n",
      "Epoch [38/100], Step [27/84], Loss: 0.1914\n",
      "Epoch [38/100], Step [28/84], Loss: 0.2107\n",
      "Epoch [38/100], Step [29/84], Loss: 0.1911\n",
      "Epoch [38/100], Step [30/84], Loss: 0.2178\n",
      "Epoch [38/100], Step [31/84], Loss: 0.2027\n",
      "Epoch [38/100], Step [32/84], Loss: 0.2210\n",
      "Epoch [38/100], Step [33/84], Loss: 0.2184\n",
      "Epoch [38/100], Step [34/84], Loss: 0.1979\n",
      "Epoch [38/100], Step [35/84], Loss: 0.2108\n",
      "Epoch [38/100], Step [36/84], Loss: 0.1973\n",
      "Epoch [38/100], Step [37/84], Loss: 0.2231\n",
      "Epoch [38/100], Step [38/84], Loss: 0.1942\n",
      "Epoch [38/100], Step [39/84], Loss: 0.2033\n",
      "Epoch [38/100], Step [40/84], Loss: 0.2190\n",
      "Epoch [38/100], Step [41/84], Loss: 0.2101\n",
      "Epoch [38/100], Step [42/84], Loss: 0.1880\n",
      "Epoch [38/100], Step [43/84], Loss: 0.1901\n",
      "Epoch [38/100], Step [44/84], Loss: 0.1971\n",
      "Epoch [38/100], Step [45/84], Loss: 0.1958\n",
      "Epoch [38/100], Step [46/84], Loss: 0.2144\n",
      "Epoch [38/100], Step [47/84], Loss: 0.2004\n",
      "Epoch [38/100], Step [48/84], Loss: 0.1985\n",
      "Epoch [38/100], Step [49/84], Loss: 0.2205\n",
      "Epoch [38/100], Step [50/84], Loss: 0.1795\n",
      "Epoch [38/100], Step [51/84], Loss: 0.1960\n",
      "Epoch [38/100], Step [52/84], Loss: 0.2036\n",
      "Epoch [38/100], Step [53/84], Loss: 0.1759\n",
      "Epoch [38/100], Step [54/84], Loss: 0.1952\n",
      "Epoch [38/100], Step [55/84], Loss: 0.2001\n",
      "Epoch [38/100], Step [56/84], Loss: 0.1900\n",
      "Epoch [38/100], Step [57/84], Loss: 0.2065\n",
      "Epoch [38/100], Step [58/84], Loss: 0.2056\n",
      "Epoch [38/100], Step [59/84], Loss: 0.2033\n",
      "Epoch [38/100], Step [60/84], Loss: 0.1918\n",
      "Epoch [38/100], Step [61/84], Loss: 0.1988\n",
      "Epoch [38/100], Step [62/84], Loss: 0.1915\n",
      "Epoch [38/100], Step [63/84], Loss: 0.2064\n",
      "Epoch [38/100], Step [64/84], Loss: 0.1877\n",
      "Epoch [38/100], Step [65/84], Loss: 0.1981\n",
      "Epoch [38/100], Step [66/84], Loss: 0.2035\n",
      "Epoch [38/100], Step [67/84], Loss: 0.2196\n",
      "Epoch [38/100], Step [68/84], Loss: 0.2063\n",
      "Epoch [38/100], Step [69/84], Loss: 0.2027\n",
      "Epoch [38/100], Step [70/84], Loss: 0.1947\n",
      "Epoch [38/100], Step [71/84], Loss: 0.1917\n",
      "Epoch [38/100], Step [72/84], Loss: 0.1966\n",
      "Epoch [38/100], Step [73/84], Loss: 0.2009\n",
      "Epoch [38/100], Step [74/84], Loss: 0.1823\n",
      "Epoch [38/100], Step [75/84], Loss: 0.1917\n",
      "Epoch [38/100], Step [76/84], Loss: 0.2020\n",
      "Epoch [38/100], Step [77/84], Loss: 0.1992\n",
      "Epoch [38/100], Step [78/84], Loss: 0.1986\n",
      "Epoch [38/100], Step [79/84], Loss: 0.2361\n",
      "Epoch [38/100], Step [80/84], Loss: 0.2041\n",
      "Epoch [38/100], Step [81/84], Loss: 0.2107\n",
      "Epoch [38/100], Step [82/84], Loss: 0.1898\n",
      "Epoch [38/100], Step [83/84], Loss: 0.1891\n",
      "Epoch [38/100], Step [84/84], Loss: 0.1897\n",
      "Epoch [39/100], Step [1/84], Loss: 0.1906\n",
      "Epoch [39/100], Step [2/84], Loss: 0.1920\n",
      "Epoch [39/100], Step [3/84], Loss: 0.1902\n",
      "Epoch [39/100], Step [4/84], Loss: 0.1918\n",
      "Epoch [39/100], Step [5/84], Loss: 0.2273\n",
      "Epoch [39/100], Step [6/84], Loss: 0.1928\n",
      "Epoch [39/100], Step [7/84], Loss: 0.1861\n",
      "Epoch [39/100], Step [8/84], Loss: 0.1995\n",
      "Epoch [39/100], Step [9/84], Loss: 0.1780\n",
      "Epoch [39/100], Step [10/84], Loss: 0.2294\n",
      "Epoch [39/100], Step [11/84], Loss: 0.1901\n",
      "Epoch [39/100], Step [12/84], Loss: 0.2097\n",
      "Epoch [39/100], Step [13/84], Loss: 0.2083\n",
      "Epoch [39/100], Step [14/84], Loss: 0.2105\n",
      "Epoch [39/100], Step [15/84], Loss: 0.2026\n",
      "Epoch [39/100], Step [16/84], Loss: 0.1747\n",
      "Epoch [39/100], Step [17/84], Loss: 0.1885\n",
      "Epoch [39/100], Step [18/84], Loss: 0.2124\n",
      "Epoch [39/100], Step [19/84], Loss: 0.2283\n",
      "Epoch [39/100], Step [20/84], Loss: 0.1866\n",
      "Epoch [39/100], Step [21/84], Loss: 0.1972\n",
      "Epoch [39/100], Step [22/84], Loss: 0.1948\n",
      "Epoch [39/100], Step [23/84], Loss: 0.2307\n",
      "Epoch [39/100], Step [24/84], Loss: 0.1844\n",
      "Epoch [39/100], Step [25/84], Loss: 0.1876\n",
      "Epoch [39/100], Step [26/84], Loss: 0.2149\n",
      "Epoch [39/100], Step [27/84], Loss: 0.1890\n",
      "Epoch [39/100], Step [28/84], Loss: 0.2031\n",
      "Epoch [39/100], Step [29/84], Loss: 0.1974\n",
      "Epoch [39/100], Step [30/84], Loss: 0.2284\n",
      "Epoch [39/100], Step [31/84], Loss: 0.1937\n",
      "Epoch [39/100], Step [32/84], Loss: 0.2004\n",
      "Epoch [39/100], Step [33/84], Loss: 0.2090\n",
      "Epoch [39/100], Step [34/84], Loss: 0.1936\n",
      "Epoch [39/100], Step [35/84], Loss: 0.1966\n",
      "Epoch [39/100], Step [36/84], Loss: 0.1976\n",
      "Epoch [39/100], Step [37/84], Loss: 0.2017\n",
      "Epoch [39/100], Step [38/84], Loss: 0.1911\n",
      "Epoch [39/100], Step [39/84], Loss: 0.1935\n",
      "Epoch [39/100], Step [40/84], Loss: 0.1842\n",
      "Epoch [39/100], Step [41/84], Loss: 0.2000\n",
      "Epoch [39/100], Step [42/84], Loss: 0.2144\n",
      "Epoch [39/100], Step [43/84], Loss: 0.1982\n",
      "Epoch [39/100], Step [44/84], Loss: 0.1920\n",
      "Epoch [39/100], Step [45/84], Loss: 0.1928\n",
      "Epoch [39/100], Step [46/84], Loss: 0.2018\n",
      "Epoch [39/100], Step [47/84], Loss: 0.2164\n",
      "Epoch [39/100], Step [48/84], Loss: 0.1934\n",
      "Epoch [39/100], Step [49/84], Loss: 0.1928\n",
      "Epoch [39/100], Step [50/84], Loss: 0.1862\n",
      "Epoch [39/100], Step [51/84], Loss: 0.1958\n",
      "Epoch [39/100], Step [52/84], Loss: 0.1828\n",
      "Epoch [39/100], Step [53/84], Loss: 0.1977\n",
      "Epoch [39/100], Step [54/84], Loss: 0.1722\n",
      "Epoch [39/100], Step [55/84], Loss: 0.2092\n",
      "Epoch [39/100], Step [56/84], Loss: 0.2114\n",
      "Epoch [39/100], Step [57/84], Loss: 0.1871\n",
      "Epoch [39/100], Step [58/84], Loss: 0.1953\n",
      "Epoch [39/100], Step [59/84], Loss: 0.2307\n",
      "Epoch [39/100], Step [60/84], Loss: 0.1857\n",
      "Epoch [39/100], Step [61/84], Loss: 0.2042\n",
      "Epoch [39/100], Step [62/84], Loss: 0.1883\n",
      "Epoch [39/100], Step [63/84], Loss: 0.1942\n",
      "Epoch [39/100], Step [64/84], Loss: 0.1952\n",
      "Epoch [39/100], Step [65/84], Loss: 0.1873\n",
      "Epoch [39/100], Step [66/84], Loss: 0.2294\n",
      "Epoch [39/100], Step [67/84], Loss: 0.2073\n",
      "Epoch [39/100], Step [68/84], Loss: 0.1713\n",
      "Epoch [39/100], Step [69/84], Loss: 0.1858\n",
      "Epoch [39/100], Step [70/84], Loss: 0.1803\n",
      "Epoch [39/100], Step [71/84], Loss: 0.1985\n",
      "Epoch [39/100], Step [72/84], Loss: 0.1806\n",
      "Epoch [39/100], Step [73/84], Loss: 0.1832\n",
      "Epoch [39/100], Step [74/84], Loss: 0.1770\n",
      "Epoch [39/100], Step [75/84], Loss: 0.2029\n",
      "Epoch [39/100], Step [76/84], Loss: 0.1823\n",
      "Epoch [39/100], Step [77/84], Loss: 0.1779\n",
      "Epoch [39/100], Step [78/84], Loss: 0.2012\n",
      "Epoch [39/100], Step [79/84], Loss: 0.2071\n",
      "Epoch [39/100], Step [80/84], Loss: 0.2236\n",
      "Epoch [39/100], Step [81/84], Loss: 0.1869\n",
      "Epoch [39/100], Step [82/84], Loss: 0.2109\n",
      "Epoch [39/100], Step [83/84], Loss: 0.1845\n",
      "Epoch [39/100], Step [84/84], Loss: 0.2157\n",
      "Epoch [40/100], Step [1/84], Loss: 0.1961\n",
      "Epoch [40/100], Step [2/84], Loss: 0.2234\n",
      "Epoch [40/100], Step [3/84], Loss: 0.1905\n",
      "Epoch [40/100], Step [4/84], Loss: 0.1814\n",
      "Epoch [40/100], Step [5/84], Loss: 0.1832\n",
      "Epoch [40/100], Step [6/84], Loss: 0.1887\n",
      "Epoch [40/100], Step [7/84], Loss: 0.1757\n",
      "Epoch [40/100], Step [8/84], Loss: 0.1932\n",
      "Epoch [40/100], Step [9/84], Loss: 0.2145\n",
      "Epoch [40/100], Step [10/84], Loss: 0.1905\n",
      "Epoch [40/100], Step [11/84], Loss: 0.1996\n",
      "Epoch [40/100], Step [12/84], Loss: 0.1850\n",
      "Epoch [40/100], Step [13/84], Loss: 0.1917\n",
      "Epoch [40/100], Step [14/84], Loss: 0.2170\n",
      "Epoch [40/100], Step [15/84], Loss: 0.1877\n",
      "Epoch [40/100], Step [16/84], Loss: 0.2019\n",
      "Epoch [40/100], Step [17/84], Loss: 0.2093\n",
      "Epoch [40/100], Step [18/84], Loss: 0.1777\n",
      "Epoch [40/100], Step [19/84], Loss: 0.2234\n",
      "Epoch [40/100], Step [20/84], Loss: 0.2275\n",
      "Epoch [40/100], Step [21/84], Loss: 0.1789\n",
      "Epoch [40/100], Step [22/84], Loss: 0.1921\n",
      "Epoch [40/100], Step [23/84], Loss: 0.1959\n",
      "Epoch [40/100], Step [24/84], Loss: 0.2105\n",
      "Epoch [40/100], Step [25/84], Loss: 0.1771\n",
      "Epoch [40/100], Step [26/84], Loss: 0.1877\n",
      "Epoch [40/100], Step [27/84], Loss: 0.1828\n",
      "Epoch [40/100], Step [28/84], Loss: 0.1765\n",
      "Epoch [40/100], Step [29/84], Loss: 0.1843\n",
      "Epoch [40/100], Step [30/84], Loss: 0.1851\n",
      "Epoch [40/100], Step [31/84], Loss: 0.1923\n",
      "Epoch [40/100], Step [32/84], Loss: 0.1848\n",
      "Epoch [40/100], Step [33/84], Loss: 0.2056\n",
      "Epoch [40/100], Step [34/84], Loss: 0.1949\n",
      "Epoch [40/100], Step [35/84], Loss: 0.2010\n",
      "Epoch [40/100], Step [36/84], Loss: 0.2093\n",
      "Epoch [40/100], Step [37/84], Loss: 0.1971\n",
      "Epoch [40/100], Step [38/84], Loss: 0.1968\n",
      "Epoch [40/100], Step [39/84], Loss: 0.1824\n",
      "Epoch [40/100], Step [40/84], Loss: 0.1909\n",
      "Epoch [40/100], Step [41/84], Loss: 0.1909\n",
      "Epoch [40/100], Step [42/84], Loss: 0.1980\n",
      "Epoch [40/100], Step [43/84], Loss: 0.2084\n",
      "Epoch [40/100], Step [44/84], Loss: 0.2199\n",
      "Epoch [40/100], Step [45/84], Loss: 0.2104\n",
      "Epoch [40/100], Step [46/84], Loss: 0.1971\n",
      "Epoch [40/100], Step [47/84], Loss: 0.2129\n",
      "Epoch [40/100], Step [48/84], Loss: 0.2005\n",
      "Epoch [40/100], Step [49/84], Loss: 0.2018\n",
      "Epoch [40/100], Step [50/84], Loss: 0.1828\n",
      "Epoch [40/100], Step [51/84], Loss: 0.2012\n",
      "Epoch [40/100], Step [52/84], Loss: 0.1855\n",
      "Epoch [40/100], Step [53/84], Loss: 0.1926\n",
      "Epoch [40/100], Step [54/84], Loss: 0.2050\n",
      "Epoch [40/100], Step [55/84], Loss: 0.1853\n",
      "Epoch [40/100], Step [56/84], Loss: 0.1802\n",
      "Epoch [40/100], Step [57/84], Loss: 0.1989\n",
      "Epoch [40/100], Step [58/84], Loss: 0.1976\n",
      "Epoch [40/100], Step [59/84], Loss: 0.1909\n",
      "Epoch [40/100], Step [60/84], Loss: 0.2010\n",
      "Epoch [40/100], Step [61/84], Loss: 0.1992\n",
      "Epoch [40/100], Step [62/84], Loss: 0.1709\n",
      "Epoch [40/100], Step [63/84], Loss: 0.2044\n",
      "Epoch [40/100], Step [64/84], Loss: 0.1771\n",
      "Epoch [40/100], Step [65/84], Loss: 0.1779\n",
      "Epoch [40/100], Step [66/84], Loss: 0.2107\n",
      "Epoch [40/100], Step [67/84], Loss: 0.1988\n",
      "Epoch [40/100], Step [68/84], Loss: 0.1930\n",
      "Epoch [40/100], Step [69/84], Loss: 0.2228\n",
      "Epoch [40/100], Step [70/84], Loss: 0.1931\n",
      "Epoch [40/100], Step [71/84], Loss: 0.2034\n",
      "Epoch [40/100], Step [72/84], Loss: 0.2033\n",
      "Epoch [40/100], Step [73/84], Loss: 0.1799\n",
      "Epoch [40/100], Step [74/84], Loss: 0.1984\n",
      "Epoch [40/100], Step [75/84], Loss: 0.1990\n",
      "Epoch [40/100], Step [76/84], Loss: 0.1862\n",
      "Epoch [40/100], Step [77/84], Loss: 0.1810\n",
      "Epoch [40/100], Step [78/84], Loss: 0.2194\n",
      "Epoch [40/100], Step [79/84], Loss: 0.2175\n",
      "Epoch [40/100], Step [80/84], Loss: 0.1994\n",
      "Epoch [40/100], Step [81/84], Loss: 0.1830\n",
      "Epoch [40/100], Step [82/84], Loss: 0.2092\n",
      "Epoch [40/100], Step [83/84], Loss: 0.1999\n",
      "Epoch [40/100], Step [84/84], Loss: 0.2179\n",
      "Epoch [41/100], Step [1/84], Loss: 0.2115\n",
      "Epoch [41/100], Step [2/84], Loss: 0.1913\n",
      "Epoch [41/100], Step [3/84], Loss: 0.1923\n",
      "Epoch [41/100], Step [4/84], Loss: 0.1860\n",
      "Epoch [41/100], Step [5/84], Loss: 0.1931\n",
      "Epoch [41/100], Step [6/84], Loss: 0.1922\n",
      "Epoch [41/100], Step [7/84], Loss: 0.1874\n",
      "Epoch [41/100], Step [8/84], Loss: 0.1808\n",
      "Epoch [41/100], Step [9/84], Loss: 0.1825\n",
      "Epoch [41/100], Step [10/84], Loss: 0.1772\n",
      "Epoch [41/100], Step [11/84], Loss: 0.2027\n",
      "Epoch [41/100], Step [12/84], Loss: 0.1804\n",
      "Epoch [41/100], Step [13/84], Loss: 0.1922\n",
      "Epoch [41/100], Step [14/84], Loss: 0.1869\n",
      "Epoch [41/100], Step [15/84], Loss: 0.2224\n",
      "Epoch [41/100], Step [16/84], Loss: 0.1809\n",
      "Epoch [41/100], Step [17/84], Loss: 0.1823\n",
      "Epoch [41/100], Step [18/84], Loss: 0.1927\n",
      "Epoch [41/100], Step [19/84], Loss: 0.1764\n",
      "Epoch [41/100], Step [20/84], Loss: 0.1913\n",
      "Epoch [41/100], Step [21/84], Loss: 0.1835\n",
      "Epoch [41/100], Step [22/84], Loss: 0.2076\n",
      "Epoch [41/100], Step [23/84], Loss: 0.1990\n",
      "Epoch [41/100], Step [24/84], Loss: 0.1949\n",
      "Epoch [41/100], Step [25/84], Loss: 0.1844\n",
      "Epoch [41/100], Step [26/84], Loss: 0.2278\n",
      "Epoch [41/100], Step [27/84], Loss: 0.1863\n",
      "Epoch [41/100], Step [28/84], Loss: 0.1715\n",
      "Epoch [41/100], Step [29/84], Loss: 0.1860\n",
      "Epoch [41/100], Step [30/84], Loss: 0.2011\n",
      "Epoch [41/100], Step [31/84], Loss: 0.1973\n",
      "Epoch [41/100], Step [32/84], Loss: 0.1999\n",
      "Epoch [41/100], Step [33/84], Loss: 0.2040\n",
      "Epoch [41/100], Step [34/84], Loss: 0.1860\n",
      "Epoch [41/100], Step [35/84], Loss: 0.1803\n",
      "Epoch [41/100], Step [36/84], Loss: 0.1854\n",
      "Epoch [41/100], Step [37/84], Loss: 0.1756\n",
      "Epoch [41/100], Step [38/84], Loss: 0.2005\n",
      "Epoch [41/100], Step [39/84], Loss: 0.1927\n",
      "Epoch [41/100], Step [40/84], Loss: 0.1813\n",
      "Epoch [41/100], Step [41/84], Loss: 0.1989\n",
      "Epoch [41/100], Step [42/84], Loss: 0.1888\n",
      "Epoch [41/100], Step [43/84], Loss: 0.2127\n",
      "Epoch [41/100], Step [44/84], Loss: 0.1743\n",
      "Epoch [41/100], Step [45/84], Loss: 0.1845\n",
      "Epoch [41/100], Step [46/84], Loss: 0.2293\n",
      "Epoch [41/100], Step [47/84], Loss: 0.2035\n",
      "Epoch [41/100], Step [48/84], Loss: 0.1880\n",
      "Epoch [41/100], Step [49/84], Loss: 0.2128\n",
      "Epoch [41/100], Step [50/84], Loss: 0.1971\n",
      "Epoch [41/100], Step [51/84], Loss: 0.1786\n",
      "Epoch [41/100], Step [52/84], Loss: 0.1917\n",
      "Epoch [41/100], Step [53/84], Loss: 0.2037\n",
      "Epoch [41/100], Step [54/84], Loss: 0.1945\n",
      "Epoch [41/100], Step [55/84], Loss: 0.1906\n",
      "Epoch [41/100], Step [56/84], Loss: 0.1841\n",
      "Epoch [41/100], Step [57/84], Loss: 0.1903\n",
      "Epoch [41/100], Step [58/84], Loss: 0.1832\n",
      "Epoch [41/100], Step [59/84], Loss: 0.1972\n",
      "Epoch [41/100], Step [60/84], Loss: 0.1755\n",
      "Epoch [41/100], Step [61/84], Loss: 0.1796\n",
      "Epoch [41/100], Step [62/84], Loss: 0.1698\n",
      "Epoch [41/100], Step [63/84], Loss: 0.1872\n",
      "Epoch [41/100], Step [64/84], Loss: 0.1933\n",
      "Epoch [41/100], Step [65/84], Loss: 0.1800\n",
      "Epoch [41/100], Step [66/84], Loss: 0.1879\n",
      "Epoch [41/100], Step [67/84], Loss: 0.1787\n",
      "Epoch [41/100], Step [68/84], Loss: 0.2029\n",
      "Epoch [41/100], Step [69/84], Loss: 0.1987\n",
      "Epoch [41/100], Step [70/84], Loss: 0.1855\n",
      "Epoch [41/100], Step [71/84], Loss: 0.1866\n",
      "Epoch [41/100], Step [72/84], Loss: 0.1826\n",
      "Epoch [41/100], Step [73/84], Loss: 0.1701\n",
      "Epoch [41/100], Step [74/84], Loss: 0.1958\n",
      "Epoch [41/100], Step [75/84], Loss: 0.2042\n",
      "Epoch [41/100], Step [76/84], Loss: 0.1788\n",
      "Epoch [41/100], Step [77/84], Loss: 0.1707\n",
      "Epoch [41/100], Step [78/84], Loss: 0.1759\n",
      "Epoch [41/100], Step [79/84], Loss: 0.1759\n",
      "Epoch [41/100], Step [80/84], Loss: 0.1749\n",
      "Epoch [41/100], Step [81/84], Loss: 0.2110\n",
      "Epoch [41/100], Step [82/84], Loss: 0.1904\n",
      "Epoch [41/100], Step [83/84], Loss: 0.2244\n",
      "Epoch [41/100], Step [84/84], Loss: 0.2264\n",
      "Epoch [42/100], Step [1/84], Loss: 0.1861\n",
      "Epoch [42/100], Step [2/84], Loss: 0.1690\n",
      "Epoch [42/100], Step [3/84], Loss: 0.1862\n",
      "Epoch [42/100], Step [4/84], Loss: 0.2030\n",
      "Epoch [42/100], Step [5/84], Loss: 0.1876\n",
      "Epoch [42/100], Step [6/84], Loss: 0.1884\n",
      "Epoch [42/100], Step [7/84], Loss: 0.1865\n",
      "Epoch [42/100], Step [8/84], Loss: 0.1935\n",
      "Epoch [42/100], Step [9/84], Loss: 0.1988\n",
      "Epoch [42/100], Step [10/84], Loss: 0.1781\n",
      "Epoch [42/100], Step [11/84], Loss: 0.1774\n",
      "Epoch [42/100], Step [12/84], Loss: 0.1917\n",
      "Epoch [42/100], Step [13/84], Loss: 0.1701\n",
      "Epoch [42/100], Step [14/84], Loss: 0.1835\n",
      "Epoch [42/100], Step [15/84], Loss: 0.2288\n",
      "Epoch [42/100], Step [16/84], Loss: 0.1966\n",
      "Epoch [42/100], Step [17/84], Loss: 0.1680\n",
      "Epoch [42/100], Step [18/84], Loss: 0.2015\n",
      "Epoch [42/100], Step [19/84], Loss: 0.2067\n",
      "Epoch [42/100], Step [20/84], Loss: 0.1678\n",
      "Epoch [42/100], Step [21/84], Loss: 0.1798\n",
      "Epoch [42/100], Step [22/84], Loss: 0.1755\n",
      "Epoch [42/100], Step [23/84], Loss: 0.2016\n",
      "Epoch [42/100], Step [24/84], Loss: 0.2223\n",
      "Epoch [42/100], Step [25/84], Loss: 0.1975\n",
      "Epoch [42/100], Step [26/84], Loss: 0.2159\n",
      "Epoch [42/100], Step [27/84], Loss: 0.1782\n",
      "Epoch [42/100], Step [28/84], Loss: 0.2006\n",
      "Epoch [42/100], Step [29/84], Loss: 0.2052\n",
      "Epoch [42/100], Step [30/84], Loss: 0.1645\n",
      "Epoch [42/100], Step [31/84], Loss: 0.1752\n",
      "Epoch [42/100], Step [32/84], Loss: 0.1754\n",
      "Epoch [42/100], Step [33/84], Loss: 0.2021\n",
      "Epoch [42/100], Step [34/84], Loss: 0.1874\n",
      "Epoch [42/100], Step [35/84], Loss: 0.1753\n",
      "Epoch [42/100], Step [36/84], Loss: 0.2184\n",
      "Epoch [42/100], Step [37/84], Loss: 0.2048\n",
      "Epoch [42/100], Step [38/84], Loss: 0.1862\n",
      "Epoch [42/100], Step [39/84], Loss: 0.2025\n",
      "Epoch [42/100], Step [40/84], Loss: 0.1871\n",
      "Epoch [42/100], Step [41/84], Loss: 0.1885\n",
      "Epoch [42/100], Step [42/84], Loss: 0.1946\n",
      "Epoch [42/100], Step [43/84], Loss: 0.1853\n",
      "Epoch [42/100], Step [44/84], Loss: 0.2005\n",
      "Epoch [42/100], Step [45/84], Loss: 0.1788\n",
      "Epoch [42/100], Step [46/84], Loss: 0.1939\n",
      "Epoch [42/100], Step [47/84], Loss: 0.1899\n",
      "Epoch [42/100], Step [48/84], Loss: 0.1978\n",
      "Epoch [42/100], Step [49/84], Loss: 0.1920\n",
      "Epoch [42/100], Step [50/84], Loss: 0.1998\n",
      "Epoch [42/100], Step [51/84], Loss: 0.1969\n",
      "Epoch [42/100], Step [52/84], Loss: 0.2021\n",
      "Epoch [42/100], Step [53/84], Loss: 0.1783\n",
      "Epoch [42/100], Step [54/84], Loss: 0.1726\n",
      "Epoch [42/100], Step [55/84], Loss: 0.1954\n",
      "Epoch [42/100], Step [56/84], Loss: 0.2413\n",
      "Epoch [42/100], Step [57/84], Loss: 0.1838\n",
      "Epoch [42/100], Step [58/84], Loss: 0.2028\n",
      "Epoch [42/100], Step [59/84], Loss: 0.1855\n",
      "Epoch [42/100], Step [60/84], Loss: 0.2036\n",
      "Epoch [42/100], Step [61/84], Loss: 0.1906\n",
      "Epoch [42/100], Step [62/84], Loss: 0.2188\n",
      "Epoch [42/100], Step [63/84], Loss: 0.1704\n",
      "Epoch [42/100], Step [64/84], Loss: 0.1867\n",
      "Epoch [42/100], Step [65/84], Loss: 0.1792\n",
      "Epoch [42/100], Step [66/84], Loss: 0.1840\n",
      "Epoch [42/100], Step [67/84], Loss: 0.1828\n",
      "Epoch [42/100], Step [68/84], Loss: 0.1870\n",
      "Epoch [42/100], Step [69/84], Loss: 0.1890\n",
      "Epoch [42/100], Step [70/84], Loss: 0.1823\n",
      "Epoch [42/100], Step [71/84], Loss: 0.1741\n",
      "Epoch [42/100], Step [72/84], Loss: 0.1800\n",
      "Epoch [42/100], Step [73/84], Loss: 0.1835\n",
      "Epoch [42/100], Step [74/84], Loss: 0.1938\n",
      "Epoch [42/100], Step [75/84], Loss: 0.1837\n",
      "Epoch [42/100], Step [76/84], Loss: 0.2046\n",
      "Epoch [42/100], Step [77/84], Loss: 0.2098\n",
      "Epoch [42/100], Step [78/84], Loss: 0.1809\n",
      "Epoch [42/100], Step [79/84], Loss: 0.1908\n",
      "Epoch [42/100], Step [80/84], Loss: 0.2215\n",
      "Epoch [42/100], Step [81/84], Loss: 0.1991\n",
      "Epoch [42/100], Step [82/84], Loss: 0.1848\n",
      "Epoch [42/100], Step [83/84], Loss: 0.1707\n",
      "Epoch [42/100], Step [84/84], Loss: 0.1702\n",
      "Epoch [43/100], Step [1/84], Loss: 0.1825\n",
      "Epoch [43/100], Step [2/84], Loss: 0.1972\n",
      "Epoch [43/100], Step [3/84], Loss: 0.1759\n",
      "Epoch [43/100], Step [4/84], Loss: 0.2169\n",
      "Epoch [43/100], Step [5/84], Loss: 0.1794\n",
      "Epoch [43/100], Step [6/84], Loss: 0.1799\n",
      "Epoch [43/100], Step [7/84], Loss: 0.1892\n",
      "Epoch [43/100], Step [8/84], Loss: 0.1926\n",
      "Epoch [43/100], Step [9/84], Loss: 0.1746\n",
      "Epoch [43/100], Step [10/84], Loss: 0.1914\n",
      "Epoch [43/100], Step [11/84], Loss: 0.2035\n",
      "Epoch [43/100], Step [12/84], Loss: 0.1861\n",
      "Epoch [43/100], Step [13/84], Loss: 0.1728\n",
      "Epoch [43/100], Step [14/84], Loss: 0.2163\n",
      "Epoch [43/100], Step [15/84], Loss: 0.1985\n",
      "Epoch [43/100], Step [16/84], Loss: 0.2003\n",
      "Epoch [43/100], Step [17/84], Loss: 0.1918\n",
      "Epoch [43/100], Step [18/84], Loss: 0.1858\n",
      "Epoch [43/100], Step [19/84], Loss: 0.1911\n",
      "Epoch [43/100], Step [20/84], Loss: 0.1877\n",
      "Epoch [43/100], Step [21/84], Loss: 0.1688\n",
      "Epoch [43/100], Step [22/84], Loss: 0.1641\n",
      "Epoch [43/100], Step [23/84], Loss: 0.1657\n",
      "Epoch [43/100], Step [24/84], Loss: 0.1793\n",
      "Epoch [43/100], Step [25/84], Loss: 0.1901\n",
      "Epoch [43/100], Step [26/84], Loss: 0.1961\n",
      "Epoch [43/100], Step [27/84], Loss: 0.2230\n",
      "Epoch [43/100], Step [28/84], Loss: 0.1701\n",
      "Epoch [43/100], Step [29/84], Loss: 0.1862\n",
      "Epoch [43/100], Step [30/84], Loss: 0.1867\n",
      "Epoch [43/100], Step [31/84], Loss: 0.1859\n",
      "Epoch [43/100], Step [32/84], Loss: 0.1806\n",
      "Epoch [43/100], Step [33/84], Loss: 0.1833\n",
      "Epoch [43/100], Step [34/84], Loss: 0.1984\n",
      "Epoch [43/100], Step [35/84], Loss: 0.1812\n",
      "Epoch [43/100], Step [36/84], Loss: 0.2068\n",
      "Epoch [43/100], Step [37/84], Loss: 0.1999\n",
      "Epoch [43/100], Step [38/84], Loss: 0.1941\n",
      "Epoch [43/100], Step [39/84], Loss: 0.1992\n",
      "Epoch [43/100], Step [40/84], Loss: 0.2031\n",
      "Epoch [43/100], Step [41/84], Loss: 0.2138\n",
      "Epoch [43/100], Step [42/84], Loss: 0.1936\n",
      "Epoch [43/100], Step [43/84], Loss: 0.1867\n",
      "Epoch [43/100], Step [44/84], Loss: 0.1781\n",
      "Epoch [43/100], Step [45/84], Loss: 0.1836\n",
      "Epoch [43/100], Step [46/84], Loss: 0.1751\n",
      "Epoch [43/100], Step [47/84], Loss: 0.1742\n",
      "Epoch [43/100], Step [48/84], Loss: 0.1740\n",
      "Epoch [43/100], Step [49/84], Loss: 0.2083\n",
      "Epoch [43/100], Step [50/84], Loss: 0.1717\n",
      "Epoch [43/100], Step [51/84], Loss: 0.1970\n",
      "Epoch [43/100], Step [52/84], Loss: 0.1664\n",
      "Epoch [43/100], Step [53/84], Loss: 0.1720\n",
      "Epoch [43/100], Step [54/84], Loss: 0.1952\n",
      "Epoch [43/100], Step [55/84], Loss: 0.1940\n",
      "Epoch [43/100], Step [56/84], Loss: 0.1769\n",
      "Epoch [43/100], Step [57/84], Loss: 0.1806\n",
      "Epoch [43/100], Step [58/84], Loss: 0.2146\n",
      "Epoch [43/100], Step [59/84], Loss: 0.1949\n",
      "Epoch [43/100], Step [60/84], Loss: 0.1873\n",
      "Epoch [43/100], Step [61/84], Loss: 0.1812\n",
      "Epoch [43/100], Step [62/84], Loss: 0.1764\n",
      "Epoch [43/100], Step [63/84], Loss: 0.1934\n",
      "Epoch [43/100], Step [64/84], Loss: 0.1961\n",
      "Epoch [43/100], Step [65/84], Loss: 0.1795\n",
      "Epoch [43/100], Step [66/84], Loss: 0.2077\n",
      "Epoch [43/100], Step [67/84], Loss: 0.1800\n",
      "Epoch [43/100], Step [68/84], Loss: 0.1903\n",
      "Epoch [43/100], Step [69/84], Loss: 0.1687\n",
      "Epoch [43/100], Step [70/84], Loss: 0.1762\n",
      "Epoch [43/100], Step [71/84], Loss: 0.1923\n",
      "Epoch [43/100], Step [72/84], Loss: 0.1923\n",
      "Epoch [43/100], Step [73/84], Loss: 0.1874\n",
      "Epoch [43/100], Step [74/84], Loss: 0.1732\n",
      "Epoch [43/100], Step [75/84], Loss: 0.1790\n",
      "Epoch [43/100], Step [76/84], Loss: 0.1806\n",
      "Epoch [43/100], Step [77/84], Loss: 0.1813\n",
      "Epoch [43/100], Step [78/84], Loss: 0.1817\n",
      "Epoch [43/100], Step [79/84], Loss: 0.1957\n",
      "Epoch [43/100], Step [80/84], Loss: 0.2127\n",
      "Epoch [43/100], Step [81/84], Loss: 0.1768\n",
      "Epoch [43/100], Step [82/84], Loss: 0.1772\n",
      "Epoch [43/100], Step [83/84], Loss: 0.1961\n",
      "Epoch [43/100], Step [84/84], Loss: 0.2210\n",
      "Epoch [44/100], Step [1/84], Loss: 0.1834\n",
      "Epoch [44/100], Step [2/84], Loss: 0.1692\n",
      "Epoch [44/100], Step [3/84], Loss: 0.1811\n",
      "Epoch [44/100], Step [4/84], Loss: 0.2038\n",
      "Epoch [44/100], Step [5/84], Loss: 0.1836\n",
      "Epoch [44/100], Step [6/84], Loss: 0.1932\n",
      "Epoch [44/100], Step [7/84], Loss: 0.1877\n",
      "Epoch [44/100], Step [8/84], Loss: 0.1748\n",
      "Epoch [44/100], Step [9/84], Loss: 0.1900\n",
      "Epoch [44/100], Step [10/84], Loss: 0.1762\n",
      "Epoch [44/100], Step [11/84], Loss: 0.2179\n",
      "Epoch [44/100], Step [12/84], Loss: 0.1776\n",
      "Epoch [44/100], Step [13/84], Loss: 0.2030\n",
      "Epoch [44/100], Step [14/84], Loss: 0.1823\n",
      "Epoch [44/100], Step [15/84], Loss: 0.1943\n",
      "Epoch [44/100], Step [16/84], Loss: 0.1861\n",
      "Epoch [44/100], Step [17/84], Loss: 0.1725\n",
      "Epoch [44/100], Step [18/84], Loss: 0.1813\n",
      "Epoch [44/100], Step [19/84], Loss: 0.2081\n",
      "Epoch [44/100], Step [20/84], Loss: 0.2007\n",
      "Epoch [44/100], Step [21/84], Loss: 0.1769\n",
      "Epoch [44/100], Step [22/84], Loss: 0.1950\n",
      "Epoch [44/100], Step [23/84], Loss: 0.2108\n",
      "Epoch [44/100], Step [24/84], Loss: 0.1784\n",
      "Epoch [44/100], Step [25/84], Loss: 0.1849\n",
      "Epoch [44/100], Step [26/84], Loss: 0.1781\n",
      "Epoch [44/100], Step [27/84], Loss: 0.1826\n",
      "Epoch [44/100], Step [28/84], Loss: 0.2109\n",
      "Epoch [44/100], Step [29/84], Loss: 0.1903\n",
      "Epoch [44/100], Step [30/84], Loss: 0.1699\n",
      "Epoch [44/100], Step [31/84], Loss: 0.1947\n",
      "Epoch [44/100], Step [32/84], Loss: 0.1830\n",
      "Epoch [44/100], Step [33/84], Loss: 0.1843\n",
      "Epoch [44/100], Step [34/84], Loss: 0.1861\n",
      "Epoch [44/100], Step [35/84], Loss: 0.1837\n",
      "Epoch [44/100], Step [36/84], Loss: 0.1987\n",
      "Epoch [44/100], Step [37/84], Loss: 0.1693\n",
      "Epoch [44/100], Step [38/84], Loss: 0.1776\n",
      "Epoch [44/100], Step [39/84], Loss: 0.1693\n",
      "Epoch [44/100], Step [40/84], Loss: 0.1882\n",
      "Epoch [44/100], Step [41/84], Loss: 0.1884\n",
      "Epoch [44/100], Step [42/84], Loss: 0.1758\n",
      "Epoch [44/100], Step [43/84], Loss: 0.2089\n",
      "Epoch [44/100], Step [44/84], Loss: 0.1682\n",
      "Epoch [44/100], Step [45/84], Loss: 0.1658\n",
      "Epoch [44/100], Step [46/84], Loss: 0.1748\n",
      "Epoch [44/100], Step [47/84], Loss: 0.1709\n",
      "Epoch [44/100], Step [48/84], Loss: 0.2088\n",
      "Epoch [44/100], Step [49/84], Loss: 0.1779\n",
      "Epoch [44/100], Step [50/84], Loss: 0.1742\n",
      "Epoch [44/100], Step [51/84], Loss: 0.2034\n",
      "Epoch [44/100], Step [52/84], Loss: 0.1840\n",
      "Epoch [44/100], Step [53/84], Loss: 0.1882\n",
      "Epoch [44/100], Step [54/84], Loss: 0.2108\n",
      "Epoch [44/100], Step [55/84], Loss: 0.1752\n",
      "Epoch [44/100], Step [56/84], Loss: 0.1759\n",
      "Epoch [44/100], Step [57/84], Loss: 0.1769\n",
      "Epoch [44/100], Step [58/84], Loss: 0.1831\n",
      "Epoch [44/100], Step [59/84], Loss: 0.2099\n",
      "Epoch [44/100], Step [60/84], Loss: 0.1696\n",
      "Epoch [44/100], Step [61/84], Loss: 0.1845\n",
      "Epoch [44/100], Step [62/84], Loss: 0.1785\n",
      "Epoch [44/100], Step [63/84], Loss: 0.1710\n",
      "Epoch [44/100], Step [64/84], Loss: 0.1712\n",
      "Epoch [44/100], Step [65/84], Loss: 0.1776\n",
      "Epoch [44/100], Step [66/84], Loss: 0.1774\n",
      "Epoch [44/100], Step [67/84], Loss: 0.1899\n",
      "Epoch [44/100], Step [68/84], Loss: 0.1857\n",
      "Epoch [44/100], Step [69/84], Loss: 0.1878\n",
      "Epoch [44/100], Step [70/84], Loss: 0.2018\n",
      "Epoch [44/100], Step [71/84], Loss: 0.1989\n",
      "Epoch [44/100], Step [72/84], Loss: 0.1798\n",
      "Epoch [44/100], Step [73/84], Loss: 0.1716\n",
      "Epoch [44/100], Step [74/84], Loss: 0.1994\n",
      "Epoch [44/100], Step [75/84], Loss: 0.2001\n",
      "Epoch [44/100], Step [76/84], Loss: 0.1998\n",
      "Epoch [44/100], Step [77/84], Loss: 0.1847\n",
      "Epoch [44/100], Step [78/84], Loss: 0.1845\n",
      "Epoch [44/100], Step [79/84], Loss: 0.1703\n",
      "Epoch [44/100], Step [80/84], Loss: 0.1793\n",
      "Epoch [44/100], Step [81/84], Loss: 0.1754\n",
      "Epoch [44/100], Step [82/84], Loss: 0.2024\n",
      "Epoch [44/100], Step [83/84], Loss: 0.2083\n",
      "Epoch [44/100], Step [84/84], Loss: 0.1986\n",
      "Epoch [45/100], Step [1/84], Loss: 0.2198\n",
      "Epoch [45/100], Step [2/84], Loss: 0.1835\n",
      "Epoch [45/100], Step [3/84], Loss: 0.1743\n",
      "Epoch [45/100], Step [4/84], Loss: 0.1836\n",
      "Epoch [45/100], Step [5/84], Loss: 0.1902\n",
      "Epoch [45/100], Step [6/84], Loss: 0.1691\n",
      "Epoch [45/100], Step [7/84], Loss: 0.1835\n",
      "Epoch [45/100], Step [8/84], Loss: 0.2015\n",
      "Epoch [45/100], Step [9/84], Loss: 0.1808\n",
      "Epoch [45/100], Step [10/84], Loss: 0.1918\n",
      "Epoch [45/100], Step [11/84], Loss: 0.1934\n",
      "Epoch [45/100], Step [12/84], Loss: 0.1876\n",
      "Epoch [45/100], Step [13/84], Loss: 0.1861\n",
      "Epoch [45/100], Step [14/84], Loss: 0.1793\n",
      "Epoch [45/100], Step [15/84], Loss: 0.1764\n",
      "Epoch [45/100], Step [16/84], Loss: 0.1767\n",
      "Epoch [45/100], Step [17/84], Loss: 0.2079\n",
      "Epoch [45/100], Step [18/84], Loss: 0.1930\n",
      "Epoch [45/100], Step [19/84], Loss: 0.1773\n",
      "Epoch [45/100], Step [20/84], Loss: 0.1839\n",
      "Epoch [45/100], Step [21/84], Loss: 0.1897\n",
      "Epoch [45/100], Step [22/84], Loss: 0.1935\n",
      "Epoch [45/100], Step [23/84], Loss: 0.1996\n",
      "Epoch [45/100], Step [24/84], Loss: 0.1725\n",
      "Epoch [45/100], Step [25/84], Loss: 0.1778\n",
      "Epoch [45/100], Step [26/84], Loss: 0.1916\n",
      "Epoch [45/100], Step [27/84], Loss: 0.1642\n",
      "Epoch [45/100], Step [28/84], Loss: 0.1719\n",
      "Epoch [45/100], Step [29/84], Loss: 0.1849\n",
      "Epoch [45/100], Step [30/84], Loss: 0.1860\n",
      "Epoch [45/100], Step [31/84], Loss: 0.1738\n",
      "Epoch [45/100], Step [32/84], Loss: 0.1918\n",
      "Epoch [45/100], Step [33/84], Loss: 0.1869\n",
      "Epoch [45/100], Step [34/84], Loss: 0.1887\n",
      "Epoch [45/100], Step [35/84], Loss: 0.1943\n",
      "Epoch [45/100], Step [36/84], Loss: 0.1904\n",
      "Epoch [45/100], Step [37/84], Loss: 0.2283\n",
      "Epoch [45/100], Step [38/84], Loss: 0.1886\n",
      "Epoch [45/100], Step [39/84], Loss: 0.1937\n",
      "Epoch [45/100], Step [40/84], Loss: 0.1898\n",
      "Epoch [45/100], Step [41/84], Loss: 0.1793\n",
      "Epoch [45/100], Step [42/84], Loss: 0.1973\n",
      "Epoch [45/100], Step [43/84], Loss: 0.1871\n",
      "Epoch [45/100], Step [44/84], Loss: 0.2012\n",
      "Epoch [45/100], Step [45/84], Loss: 0.1642\n",
      "Epoch [45/100], Step [46/84], Loss: 0.1996\n",
      "Epoch [45/100], Step [47/84], Loss: 0.1947\n",
      "Epoch [45/100], Step [48/84], Loss: 0.1770\n",
      "Epoch [45/100], Step [49/84], Loss: 0.1847\n",
      "Epoch [45/100], Step [50/84], Loss: 0.1685\n",
      "Epoch [45/100], Step [51/84], Loss: 0.1843\n",
      "Epoch [45/100], Step [52/84], Loss: 0.1951\n",
      "Epoch [45/100], Step [53/84], Loss: 0.1722\n",
      "Epoch [45/100], Step [54/84], Loss: 0.1832\n",
      "Epoch [45/100], Step [55/84], Loss: 0.1871\n",
      "Epoch [45/100], Step [56/84], Loss: 0.1706\n",
      "Epoch [45/100], Step [57/84], Loss: 0.1727\n",
      "Epoch [45/100], Step [58/84], Loss: 0.1840\n",
      "Epoch [45/100], Step [59/84], Loss: 0.1697\n",
      "Epoch [45/100], Step [60/84], Loss: 0.1813\n",
      "Epoch [45/100], Step [61/84], Loss: 0.1894\n",
      "Epoch [45/100], Step [62/84], Loss: 0.1696\n",
      "Epoch [45/100], Step [63/84], Loss: 0.1813\n",
      "Epoch [45/100], Step [64/84], Loss: 0.1883\n",
      "Epoch [45/100], Step [65/84], Loss: 0.1668\n",
      "Epoch [45/100], Step [66/84], Loss: 0.1790\n",
      "Epoch [45/100], Step [67/84], Loss: 0.1771\n",
      "Epoch [45/100], Step [68/84], Loss: 0.1997\n",
      "Epoch [45/100], Step [69/84], Loss: 0.1776\n",
      "Epoch [45/100], Step [70/84], Loss: 0.1705\n",
      "Epoch [45/100], Step [71/84], Loss: 0.1939\n",
      "Epoch [45/100], Step [72/84], Loss: 0.2026\n",
      "Epoch [45/100], Step [73/84], Loss: 0.1750\n",
      "Epoch [45/100], Step [74/84], Loss: 0.1786\n",
      "Epoch [45/100], Step [75/84], Loss: 0.1910\n",
      "Epoch [45/100], Step [76/84], Loss: 0.1746\n",
      "Epoch [45/100], Step [77/84], Loss: 0.1812\n",
      "Epoch [45/100], Step [78/84], Loss: 0.1697\n",
      "Epoch [45/100], Step [79/84], Loss: 0.1782\n",
      "Epoch [45/100], Step [80/84], Loss: 0.1829\n",
      "Epoch [45/100], Step [81/84], Loss: 0.1778\n",
      "Epoch [45/100], Step [82/84], Loss: 0.1655\n",
      "Epoch [45/100], Step [83/84], Loss: 0.1901\n",
      "Epoch [45/100], Step [84/84], Loss: 0.2167\n",
      "Epoch [46/100], Step [1/84], Loss: 0.1935\n",
      "Epoch [46/100], Step [2/84], Loss: 0.1843\n",
      "Epoch [46/100], Step [3/84], Loss: 0.1880\n",
      "Epoch [46/100], Step [4/84], Loss: 0.1904\n",
      "Epoch [46/100], Step [5/84], Loss: 0.1819\n",
      "Epoch [46/100], Step [6/84], Loss: 0.1723\n",
      "Epoch [46/100], Step [7/84], Loss: 0.1831\n",
      "Epoch [46/100], Step [8/84], Loss: 0.2031\n",
      "Epoch [46/100], Step [9/84], Loss: 0.1769\n",
      "Epoch [46/100], Step [10/84], Loss: 0.1946\n",
      "Epoch [46/100], Step [11/84], Loss: 0.1654\n",
      "Epoch [46/100], Step [12/84], Loss: 0.1798\n",
      "Epoch [46/100], Step [13/84], Loss: 0.1728\n",
      "Epoch [46/100], Step [14/84], Loss: 0.1650\n",
      "Epoch [46/100], Step [15/84], Loss: 0.1894\n",
      "Epoch [46/100], Step [16/84], Loss: 0.2012\n",
      "Epoch [46/100], Step [17/84], Loss: 0.1760\n",
      "Epoch [46/100], Step [18/84], Loss: 0.1841\n",
      "Epoch [46/100], Step [19/84], Loss: 0.1957\n",
      "Epoch [46/100], Step [20/84], Loss: 0.1882\n",
      "Epoch [46/100], Step [21/84], Loss: 0.1743\n",
      "Epoch [46/100], Step [22/84], Loss: 0.2040\n",
      "Epoch [46/100], Step [23/84], Loss: 0.1808\n",
      "Epoch [46/100], Step [24/84], Loss: 0.1807\n",
      "Epoch [46/100], Step [25/84], Loss: 0.1761\n",
      "Epoch [46/100], Step [26/84], Loss: 0.1836\n",
      "Epoch [46/100], Step [27/84], Loss: 0.1784\n",
      "Epoch [46/100], Step [28/84], Loss: 0.1683\n",
      "Epoch [46/100], Step [29/84], Loss: 0.1801\n",
      "Epoch [46/100], Step [30/84], Loss: 0.1852\n",
      "Epoch [46/100], Step [31/84], Loss: 0.1778\n",
      "Epoch [46/100], Step [32/84], Loss: 0.2129\n",
      "Epoch [46/100], Step [33/84], Loss: 0.2037\n",
      "Epoch [46/100], Step [34/84], Loss: 0.1770\n",
      "Epoch [46/100], Step [35/84], Loss: 0.1985\n",
      "Epoch [46/100], Step [36/84], Loss: 0.1811\n",
      "Epoch [46/100], Step [37/84], Loss: 0.2005\n",
      "Epoch [46/100], Step [38/84], Loss: 0.1836\n",
      "Epoch [46/100], Step [39/84], Loss: 0.1720\n",
      "Epoch [46/100], Step [40/84], Loss: 0.1791\n",
      "Epoch [46/100], Step [41/84], Loss: 0.1833\n",
      "Epoch [46/100], Step [42/84], Loss: 0.1857\n",
      "Epoch [46/100], Step [43/84], Loss: 0.1792\n",
      "Epoch [46/100], Step [44/84], Loss: 0.1774\n",
      "Epoch [46/100], Step [45/84], Loss: 0.1788\n",
      "Epoch [46/100], Step [46/84], Loss: 0.1759\n",
      "Epoch [46/100], Step [47/84], Loss: 0.1721\n",
      "Epoch [46/100], Step [48/84], Loss: 0.1709\n",
      "Epoch [46/100], Step [49/84], Loss: 0.1960\n",
      "Epoch [46/100], Step [50/84], Loss: 0.1782\n",
      "Epoch [46/100], Step [51/84], Loss: 0.1708\n",
      "Epoch [46/100], Step [52/84], Loss: 0.1943\n",
      "Epoch [46/100], Step [53/84], Loss: 0.1797\n",
      "Epoch [46/100], Step [54/84], Loss: 0.1700\n",
      "Epoch [46/100], Step [55/84], Loss: 0.1905\n",
      "Epoch [46/100], Step [56/84], Loss: 0.1763\n",
      "Epoch [46/100], Step [57/84], Loss: 0.1781\n",
      "Epoch [46/100], Step [58/84], Loss: 0.1973\n",
      "Epoch [46/100], Step [59/84], Loss: 0.2027\n",
      "Epoch [46/100], Step [60/84], Loss: 0.1881\n",
      "Epoch [46/100], Step [61/84], Loss: 0.2005\n",
      "Epoch [46/100], Step [62/84], Loss: 0.1776\n",
      "Epoch [46/100], Step [63/84], Loss: 0.1882\n",
      "Epoch [46/100], Step [64/84], Loss: 0.2054\n",
      "Epoch [46/100], Step [65/84], Loss: 0.1704\n",
      "Epoch [46/100], Step [66/84], Loss: 0.2045\n",
      "Epoch [46/100], Step [67/84], Loss: 0.1721\n",
      "Epoch [46/100], Step [68/84], Loss: 0.1751\n",
      "Epoch [46/100], Step [69/84], Loss: 0.1808\n",
      "Epoch [46/100], Step [70/84], Loss: 0.1842\n",
      "Epoch [46/100], Step [71/84], Loss: 0.1689\n",
      "Epoch [46/100], Step [72/84], Loss: 0.1631\n",
      "Epoch [46/100], Step [73/84], Loss: 0.1976\n",
      "Epoch [46/100], Step [74/84], Loss: 0.1718\n",
      "Epoch [46/100], Step [75/84], Loss: 0.1752\n",
      "Epoch [46/100], Step [76/84], Loss: 0.1755\n",
      "Epoch [46/100], Step [77/84], Loss: 0.2115\n",
      "Epoch [46/100], Step [78/84], Loss: 0.1819\n",
      "Epoch [46/100], Step [79/84], Loss: 0.1785\n",
      "Epoch [46/100], Step [80/84], Loss: 0.1809\n",
      "Epoch [46/100], Step [81/84], Loss: 0.1758\n",
      "Epoch [46/100], Step [82/84], Loss: 0.1796\n",
      "Epoch [46/100], Step [83/84], Loss: 0.1823\n",
      "Epoch [46/100], Step [84/84], Loss: 0.1828\n",
      "Epoch [47/100], Step [1/84], Loss: 0.1532\n",
      "Epoch [47/100], Step [2/84], Loss: 0.1980\n",
      "Epoch [47/100], Step [3/84], Loss: 0.1789\n",
      "Epoch [47/100], Step [4/84], Loss: 0.1996\n",
      "Epoch [47/100], Step [5/84], Loss: 0.1917\n",
      "Epoch [47/100], Step [6/84], Loss: 0.1747\n",
      "Epoch [47/100], Step [7/84], Loss: 0.2074\n",
      "Epoch [47/100], Step [8/84], Loss: 0.2024\n",
      "Epoch [47/100], Step [9/84], Loss: 0.1781\n",
      "Epoch [47/100], Step [10/84], Loss: 0.1806\n",
      "Epoch [47/100], Step [11/84], Loss: 0.1799\n",
      "Epoch [47/100], Step [12/84], Loss: 0.1757\n",
      "Epoch [47/100], Step [13/84], Loss: 0.1650\n",
      "Epoch [47/100], Step [14/84], Loss: 0.1887\n",
      "Epoch [47/100], Step [15/84], Loss: 0.1847\n",
      "Epoch [47/100], Step [16/84], Loss: 0.1700\n",
      "Epoch [47/100], Step [17/84], Loss: 0.1861\n",
      "Epoch [47/100], Step [18/84], Loss: 0.1975\n",
      "Epoch [47/100], Step [19/84], Loss: 0.1938\n",
      "Epoch [47/100], Step [20/84], Loss: 0.1880\n",
      "Epoch [47/100], Step [21/84], Loss: 0.1796\n",
      "Epoch [47/100], Step [22/84], Loss: 0.1867\n",
      "Epoch [47/100], Step [23/84], Loss: 0.1915\n",
      "Epoch [47/100], Step [24/84], Loss: 0.1780\n",
      "Epoch [47/100], Step [25/84], Loss: 0.1775\n",
      "Epoch [47/100], Step [26/84], Loss: 0.2069\n",
      "Epoch [47/100], Step [27/84], Loss: 0.1821\n",
      "Epoch [47/100], Step [28/84], Loss: 0.1689\n",
      "Epoch [47/100], Step [29/84], Loss: 0.1762\n",
      "Epoch [47/100], Step [30/84], Loss: 0.1729\n",
      "Epoch [47/100], Step [31/84], Loss: 0.1845\n",
      "Epoch [47/100], Step [32/84], Loss: 0.1570\n",
      "Epoch [47/100], Step [33/84], Loss: 0.1832\n",
      "Epoch [47/100], Step [34/84], Loss: 0.1821\n",
      "Epoch [47/100], Step [35/84], Loss: 0.1835\n",
      "Epoch [47/100], Step [36/84], Loss: 0.1816\n",
      "Epoch [47/100], Step [37/84], Loss: 0.1678\n",
      "Epoch [47/100], Step [38/84], Loss: 0.1865\n",
      "Epoch [47/100], Step [39/84], Loss: 0.1659\n",
      "Epoch [47/100], Step [40/84], Loss: 0.1698\n",
      "Epoch [47/100], Step [41/84], Loss: 0.1616\n",
      "Epoch [47/100], Step [42/84], Loss: 0.1802\n",
      "Epoch [47/100], Step [43/84], Loss: 0.2148\n",
      "Epoch [47/100], Step [44/84], Loss: 0.1976\n",
      "Epoch [47/100], Step [45/84], Loss: 0.2063\n",
      "Epoch [47/100], Step [46/84], Loss: 0.1995\n",
      "Epoch [47/100], Step [47/84], Loss: 0.1980\n",
      "Epoch [47/100], Step [48/84], Loss: 0.1638\n",
      "Epoch [47/100], Step [49/84], Loss: 0.1823\n",
      "Epoch [47/100], Step [50/84], Loss: 0.1954\n",
      "Epoch [47/100], Step [51/84], Loss: 0.1601\n",
      "Epoch [47/100], Step [52/84], Loss: 0.1835\n",
      "Epoch [47/100], Step [53/84], Loss: 0.1845\n",
      "Epoch [47/100], Step [54/84], Loss: 0.1709\n",
      "Epoch [47/100], Step [55/84], Loss: 0.1861\n",
      "Epoch [47/100], Step [56/84], Loss: 0.1853\n",
      "Epoch [47/100], Step [57/84], Loss: 0.1923\n",
      "Epoch [47/100], Step [58/84], Loss: 0.1694\n",
      "Epoch [47/100], Step [59/84], Loss: 0.1827\n",
      "Epoch [47/100], Step [60/84], Loss: 0.1785\n",
      "Epoch [47/100], Step [61/84], Loss: 0.1831\n",
      "Epoch [47/100], Step [62/84], Loss: 0.1771\n",
      "Epoch [47/100], Step [63/84], Loss: 0.1763\n",
      "Epoch [47/100], Step [64/84], Loss: 0.1812\n",
      "Epoch [47/100], Step [65/84], Loss: 0.1761\n",
      "Epoch [47/100], Step [66/84], Loss: 0.1944\n",
      "Epoch [47/100], Step [67/84], Loss: 0.1665\n",
      "Epoch [47/100], Step [68/84], Loss: 0.2013\n",
      "Epoch [47/100], Step [69/84], Loss: 0.1711\n",
      "Epoch [47/100], Step [70/84], Loss: 0.1788\n",
      "Epoch [47/100], Step [71/84], Loss: 0.1926\n",
      "Epoch [47/100], Step [72/84], Loss: 0.1746\n",
      "Epoch [47/100], Step [73/84], Loss: 0.1698\n",
      "Epoch [47/100], Step [74/84], Loss: 0.1846\n",
      "Epoch [47/100], Step [75/84], Loss: 0.1778\n",
      "Epoch [47/100], Step [76/84], Loss: 0.1618\n",
      "Epoch [47/100], Step [77/84], Loss: 0.1883\n",
      "Epoch [47/100], Step [78/84], Loss: 0.1866\n",
      "Epoch [47/100], Step [79/84], Loss: 0.1796\n",
      "Epoch [47/100], Step [80/84], Loss: 0.1791\n",
      "Epoch [47/100], Step [81/84], Loss: 0.1842\n",
      "Epoch [47/100], Step [82/84], Loss: 0.1801\n",
      "Epoch [47/100], Step [83/84], Loss: 0.1773\n",
      "Epoch [47/100], Step [84/84], Loss: 0.2084\n",
      "Epoch [48/100], Step [1/84], Loss: 0.1694\n",
      "Epoch [48/100], Step [2/84], Loss: 0.1604\n",
      "Epoch [48/100], Step [3/84], Loss: 0.1653\n",
      "Epoch [48/100], Step [4/84], Loss: 0.1798\n",
      "Epoch [48/100], Step [5/84], Loss: 0.1902\n",
      "Epoch [48/100], Step [6/84], Loss: 0.1720\n",
      "Epoch [48/100], Step [7/84], Loss: 0.1732\n",
      "Epoch [48/100], Step [8/84], Loss: 0.1826\n",
      "Epoch [48/100], Step [9/84], Loss: 0.1865\n",
      "Epoch [48/100], Step [10/84], Loss: 0.1741\n",
      "Epoch [48/100], Step [11/84], Loss: 0.1866\n",
      "Epoch [48/100], Step [12/84], Loss: 0.1666\n",
      "Epoch [48/100], Step [13/84], Loss: 0.1728\n",
      "Epoch [48/100], Step [14/84], Loss: 0.1763\n",
      "Epoch [48/100], Step [15/84], Loss: 0.1771\n",
      "Epoch [48/100], Step [16/84], Loss: 0.1915\n",
      "Epoch [48/100], Step [17/84], Loss: 0.2021\n",
      "Epoch [48/100], Step [18/84], Loss: 0.1758\n",
      "Epoch [48/100], Step [19/84], Loss: 0.1707\n",
      "Epoch [48/100], Step [20/84], Loss: 0.1832\n",
      "Epoch [48/100], Step [21/84], Loss: 0.1812\n",
      "Epoch [48/100], Step [22/84], Loss: 0.1805\n",
      "Epoch [48/100], Step [23/84], Loss: 0.1645\n",
      "Epoch [48/100], Step [24/84], Loss: 0.1842\n",
      "Epoch [48/100], Step [25/84], Loss: 0.1789\n",
      "Epoch [48/100], Step [26/84], Loss: 0.2073\n",
      "Epoch [48/100], Step [27/84], Loss: 0.1826\n",
      "Epoch [48/100], Step [28/84], Loss: 0.1665\n",
      "Epoch [48/100], Step [29/84], Loss: 0.1768\n",
      "Epoch [48/100], Step [30/84], Loss: 0.1880\n",
      "Epoch [48/100], Step [31/84], Loss: 0.1858\n",
      "Epoch [48/100], Step [32/84], Loss: 0.1786\n",
      "Epoch [48/100], Step [33/84], Loss: 0.1754\n",
      "Epoch [48/100], Step [34/84], Loss: 0.1882\n",
      "Epoch [48/100], Step [35/84], Loss: 0.1805\n",
      "Epoch [48/100], Step [36/84], Loss: 0.1919\n",
      "Epoch [48/100], Step [37/84], Loss: 0.1723\n",
      "Epoch [48/100], Step [38/84], Loss: 0.1932\n",
      "Epoch [48/100], Step [39/84], Loss: 0.1778\n",
      "Epoch [48/100], Step [40/84], Loss: 0.1703\n",
      "Epoch [48/100], Step [41/84], Loss: 0.1742\n",
      "Epoch [48/100], Step [42/84], Loss: 0.1907\n",
      "Epoch [48/100], Step [43/84], Loss: 0.1875\n",
      "Epoch [48/100], Step [44/84], Loss: 0.1945\n",
      "Epoch [48/100], Step [45/84], Loss: 0.1814\n",
      "Epoch [48/100], Step [46/84], Loss: 0.1833\n",
      "Epoch [48/100], Step [47/84], Loss: 0.1617\n",
      "Epoch [48/100], Step [48/84], Loss: 0.1823\n",
      "Epoch [48/100], Step [49/84], Loss: 0.1682\n",
      "Epoch [48/100], Step [50/84], Loss: 0.1741\n",
      "Epoch [48/100], Step [51/84], Loss: 0.1887\n",
      "Epoch [48/100], Step [52/84], Loss: 0.1690\n",
      "Epoch [48/100], Step [53/84], Loss: 0.1698\n",
      "Epoch [48/100], Step [54/84], Loss: 0.1770\n",
      "Epoch [48/100], Step [55/84], Loss: 0.1761\n",
      "Epoch [48/100], Step [56/84], Loss: 0.1748\n",
      "Epoch [48/100], Step [57/84], Loss: 0.1908\n",
      "Epoch [48/100], Step [58/84], Loss: 0.1695\n",
      "Epoch [48/100], Step [59/84], Loss: 0.1916\n",
      "Epoch [48/100], Step [60/84], Loss: 0.1835\n",
      "Epoch [48/100], Step [61/84], Loss: 0.1901\n",
      "Epoch [48/100], Step [62/84], Loss: 0.1620\n",
      "Epoch [48/100], Step [63/84], Loss: 0.1831\n",
      "Epoch [48/100], Step [64/84], Loss: 0.1702\n",
      "Epoch [48/100], Step [65/84], Loss: 0.1770\n",
      "Epoch [48/100], Step [66/84], Loss: 0.1795\n",
      "Epoch [48/100], Step [67/84], Loss: 0.1960\n",
      "Epoch [48/100], Step [68/84], Loss: 0.1885\n",
      "Epoch [48/100], Step [69/84], Loss: 0.1934\n",
      "Epoch [48/100], Step [70/84], Loss: 0.1530\n",
      "Epoch [48/100], Step [71/84], Loss: 0.1668\n",
      "Epoch [48/100], Step [72/84], Loss: 0.1953\n",
      "Epoch [48/100], Step [73/84], Loss: 0.1801\n",
      "Epoch [48/100], Step [74/84], Loss: 0.1907\n",
      "Epoch [48/100], Step [75/84], Loss: 0.1802\n",
      "Epoch [48/100], Step [76/84], Loss: 0.1702\n",
      "Epoch [48/100], Step [77/84], Loss: 0.1820\n",
      "Epoch [48/100], Step [78/84], Loss: 0.1744\n",
      "Epoch [48/100], Step [79/84], Loss: 0.1890\n",
      "Epoch [48/100], Step [80/84], Loss: 0.1753\n",
      "Epoch [48/100], Step [81/84], Loss: 0.1794\n",
      "Epoch [48/100], Step [82/84], Loss: 0.1750\n",
      "Epoch [48/100], Step [83/84], Loss: 0.1963\n",
      "Epoch [48/100], Step [84/84], Loss: 0.1884\n",
      "Epoch [49/100], Step [1/84], Loss: 0.1980\n",
      "Epoch [49/100], Step [2/84], Loss: 0.1828\n",
      "Epoch [49/100], Step [3/84], Loss: 0.1729\n",
      "Epoch [49/100], Step [4/84], Loss: 0.1850\n",
      "Epoch [49/100], Step [5/84], Loss: 0.1624\n",
      "Epoch [49/100], Step [6/84], Loss: 0.1723\n",
      "Epoch [49/100], Step [7/84], Loss: 0.1813\n",
      "Epoch [49/100], Step [8/84], Loss: 0.1632\n",
      "Epoch [49/100], Step [9/84], Loss: 0.1715\n",
      "Epoch [49/100], Step [10/84], Loss: 0.1909\n",
      "Epoch [49/100], Step [11/84], Loss: 0.1765\n",
      "Epoch [49/100], Step [12/84], Loss: 0.1763\n",
      "Epoch [49/100], Step [13/84], Loss: 0.1835\n",
      "Epoch [49/100], Step [14/84], Loss: 0.1649\n",
      "Epoch [49/100], Step [15/84], Loss: 0.1798\n",
      "Epoch [49/100], Step [16/84], Loss: 0.1766\n",
      "Epoch [49/100], Step [17/84], Loss: 0.1607\n",
      "Epoch [49/100], Step [18/84], Loss: 0.1670\n",
      "Epoch [49/100], Step [19/84], Loss: 0.1845\n",
      "Epoch [49/100], Step [20/84], Loss: 0.1587\n",
      "Epoch [49/100], Step [21/84], Loss: 0.1728\n",
      "Epoch [49/100], Step [22/84], Loss: 0.2008\n",
      "Epoch [49/100], Step [23/84], Loss: 0.1847\n",
      "Epoch [49/100], Step [24/84], Loss: 0.1712\n",
      "Epoch [49/100], Step [25/84], Loss: 0.1863\n",
      "Epoch [49/100], Step [26/84], Loss: 0.1534\n",
      "Epoch [49/100], Step [27/84], Loss: 0.1750\n",
      "Epoch [49/100], Step [28/84], Loss: 0.2080\n",
      "Epoch [49/100], Step [29/84], Loss: 0.1657\n",
      "Epoch [49/100], Step [30/84], Loss: 0.1988\n",
      "Epoch [49/100], Step [31/84], Loss: 0.1852\n",
      "Epoch [49/100], Step [32/84], Loss: 0.1587\n",
      "Epoch [49/100], Step [33/84], Loss: 0.1854\n",
      "Epoch [49/100], Step [34/84], Loss: 0.1768\n",
      "Epoch [49/100], Step [35/84], Loss: 0.1755\n",
      "Epoch [49/100], Step [36/84], Loss: 0.1824\n",
      "Epoch [49/100], Step [37/84], Loss: 0.1773\n",
      "Epoch [49/100], Step [38/84], Loss: 0.1720\n",
      "Epoch [49/100], Step [39/84], Loss: 0.1838\n",
      "Epoch [49/100], Step [40/84], Loss: 0.1677\n",
      "Epoch [49/100], Step [41/84], Loss: 0.1608\n",
      "Epoch [49/100], Step [42/84], Loss: 0.1731\n",
      "Epoch [49/100], Step [43/84], Loss: 0.1758\n",
      "Epoch [49/100], Step [44/84], Loss: 0.1930\n",
      "Epoch [49/100], Step [45/84], Loss: 0.1909\n",
      "Epoch [49/100], Step [46/84], Loss: 0.1796\n",
      "Epoch [49/100], Step [47/84], Loss: 0.1733\n",
      "Epoch [49/100], Step [48/84], Loss: 0.1878\n",
      "Epoch [49/100], Step [49/84], Loss: 0.2051\n",
      "Epoch [49/100], Step [50/84], Loss: 0.1606\n",
      "Epoch [49/100], Step [51/84], Loss: 0.1671\n",
      "Epoch [49/100], Step [52/84], Loss: 0.1678\n",
      "Epoch [49/100], Step [53/84], Loss: 0.1694\n",
      "Epoch [49/100], Step [54/84], Loss: 0.1958\n",
      "Epoch [49/100], Step [55/84], Loss: 0.1652\n",
      "Epoch [49/100], Step [56/84], Loss: 0.1788\n",
      "Epoch [49/100], Step [57/84], Loss: 0.1779\n",
      "Epoch [49/100], Step [58/84], Loss: 0.1775\n",
      "Epoch [49/100], Step [59/84], Loss: 0.1650\n",
      "Epoch [49/100], Step [60/84], Loss: 0.1776\n",
      "Epoch [49/100], Step [61/84], Loss: 0.1751\n",
      "Epoch [49/100], Step [62/84], Loss: 0.1574\n",
      "Epoch [49/100], Step [63/84], Loss: 0.1769\n",
      "Epoch [49/100], Step [64/84], Loss: 0.1648\n",
      "Epoch [49/100], Step [65/84], Loss: 0.1655\n",
      "Epoch [49/100], Step [66/84], Loss: 0.1968\n",
      "Epoch [49/100], Step [67/84], Loss: 0.1812\n",
      "Epoch [49/100], Step [68/84], Loss: 0.1773\n",
      "Epoch [49/100], Step [69/84], Loss: 0.1685\n",
      "Epoch [49/100], Step [70/84], Loss: 0.1685\n",
      "Epoch [49/100], Step [71/84], Loss: 0.1853\n",
      "Epoch [49/100], Step [72/84], Loss: 0.1801\n",
      "Epoch [49/100], Step [73/84], Loss: 0.1926\n",
      "Epoch [49/100], Step [74/84], Loss: 0.1799\n",
      "Epoch [49/100], Step [75/84], Loss: 0.1637\n",
      "Epoch [49/100], Step [76/84], Loss: 0.1813\n",
      "Epoch [49/100], Step [77/84], Loss: 0.2240\n",
      "Epoch [49/100], Step [78/84], Loss: 0.1632\n",
      "Epoch [49/100], Step [79/84], Loss: 0.1759\n",
      "Epoch [49/100], Step [80/84], Loss: 0.1807\n",
      "Epoch [49/100], Step [81/84], Loss: 0.1763\n",
      "Epoch [49/100], Step [82/84], Loss: 0.1674\n",
      "Epoch [49/100], Step [83/84], Loss: 0.1777\n",
      "Epoch [49/100], Step [84/84], Loss: 0.1752\n",
      "Epoch [50/100], Step [1/84], Loss: 0.1890\n",
      "Epoch [50/100], Step [2/84], Loss: 0.1783\n",
      "Epoch [50/100], Step [3/84], Loss: 0.1778\n",
      "Epoch [50/100], Step [4/84], Loss: 0.1627\n",
      "Epoch [50/100], Step [5/84], Loss: 0.1615\n",
      "Epoch [50/100], Step [6/84], Loss: 0.1735\n",
      "Epoch [50/100], Step [7/84], Loss: 0.1629\n",
      "Epoch [50/100], Step [8/84], Loss: 0.1872\n",
      "Epoch [50/100], Step [9/84], Loss: 0.1828\n",
      "Epoch [50/100], Step [10/84], Loss: 0.1816\n",
      "Epoch [50/100], Step [11/84], Loss: 0.1789\n",
      "Epoch [50/100], Step [12/84], Loss: 0.1890\n",
      "Epoch [50/100], Step [13/84], Loss: 0.1846\n",
      "Epoch [50/100], Step [14/84], Loss: 0.1753\n",
      "Epoch [50/100], Step [15/84], Loss: 0.2039\n",
      "Epoch [50/100], Step [16/84], Loss: 0.1870\n",
      "Epoch [50/100], Step [17/84], Loss: 0.1654\n",
      "Epoch [50/100], Step [18/84], Loss: 0.1786\n",
      "Epoch [50/100], Step [19/84], Loss: 0.1983\n",
      "Epoch [50/100], Step [20/84], Loss: 0.1676\n",
      "Epoch [50/100], Step [21/84], Loss: 0.1985\n",
      "Epoch [50/100], Step [22/84], Loss: 0.1661\n",
      "Epoch [50/100], Step [23/84], Loss: 0.1706\n",
      "Epoch [50/100], Step [24/84], Loss: 0.1776\n",
      "Epoch [50/100], Step [25/84], Loss: 0.1772\n",
      "Epoch [50/100], Step [26/84], Loss: 0.1774\n",
      "Epoch [50/100], Step [27/84], Loss: 0.1637\n",
      "Epoch [50/100], Step [28/84], Loss: 0.1742\n",
      "Epoch [50/100], Step [29/84], Loss: 0.1981\n",
      "Epoch [50/100], Step [30/84], Loss: 0.2042\n",
      "Epoch [50/100], Step [31/84], Loss: 0.1680\n",
      "Epoch [50/100], Step [32/84], Loss: 0.1510\n",
      "Epoch [50/100], Step [33/84], Loss: 0.1787\n",
      "Epoch [50/100], Step [34/84], Loss: 0.1887\n",
      "Epoch [50/100], Step [35/84], Loss: 0.1755\n",
      "Epoch [50/100], Step [36/84], Loss: 0.1804\n",
      "Epoch [50/100], Step [37/84], Loss: 0.1712\n",
      "Epoch [50/100], Step [38/84], Loss: 0.1716\n",
      "Epoch [50/100], Step [39/84], Loss: 0.1689\n",
      "Epoch [50/100], Step [40/84], Loss: 0.1542\n",
      "Epoch [50/100], Step [41/84], Loss: 0.1609\n",
      "Epoch [50/100], Step [42/84], Loss: 0.1658\n",
      "Epoch [50/100], Step [43/84], Loss: 0.1844\n",
      "Epoch [50/100], Step [44/84], Loss: 0.1765\n",
      "Epoch [50/100], Step [45/84], Loss: 0.1715\n",
      "Epoch [50/100], Step [46/84], Loss: 0.1835\n",
      "Epoch [50/100], Step [47/84], Loss: 0.1771\n",
      "Epoch [50/100], Step [48/84], Loss: 0.1749\n",
      "Epoch [50/100], Step [49/84], Loss: 0.1696\n",
      "Epoch [50/100], Step [50/84], Loss: 0.1835\n",
      "Epoch [50/100], Step [51/84], Loss: 0.1860\n",
      "Epoch [50/100], Step [52/84], Loss: 0.1859\n",
      "Epoch [50/100], Step [53/84], Loss: 0.1948\n",
      "Epoch [50/100], Step [54/84], Loss: 0.1663\n",
      "Epoch [50/100], Step [55/84], Loss: 0.1800\n",
      "Epoch [50/100], Step [56/84], Loss: 0.1760\n",
      "Epoch [50/100], Step [57/84], Loss: 0.1766\n",
      "Epoch [50/100], Step [58/84], Loss: 0.1726\n",
      "Epoch [50/100], Step [59/84], Loss: 0.1736\n",
      "Epoch [50/100], Step [60/84], Loss: 0.1860\n",
      "Epoch [50/100], Step [61/84], Loss: 0.1837\n",
      "Epoch [50/100], Step [62/84], Loss: 0.2089\n",
      "Epoch [50/100], Step [63/84], Loss: 0.1757\n",
      "Epoch [50/100], Step [64/84], Loss: 0.1654\n",
      "Epoch [50/100], Step [65/84], Loss: 0.1989\n",
      "Epoch [50/100], Step [66/84], Loss: 0.1867\n",
      "Epoch [50/100], Step [67/84], Loss: 0.1785\n",
      "Epoch [50/100], Step [68/84], Loss: 0.1873\n",
      "Epoch [50/100], Step [69/84], Loss: 0.2001\n",
      "Epoch [50/100], Step [70/84], Loss: 0.1746\n",
      "Epoch [50/100], Step [71/84], Loss: 0.1799\n",
      "Epoch [50/100], Step [72/84], Loss: 0.1748\n",
      "Epoch [50/100], Step [73/84], Loss: 0.1658\n",
      "Epoch [50/100], Step [74/84], Loss: 0.1651\n",
      "Epoch [50/100], Step [75/84], Loss: 0.1673\n",
      "Epoch [50/100], Step [76/84], Loss: 0.1691\n",
      "Epoch [50/100], Step [77/84], Loss: 0.1690\n",
      "Epoch [50/100], Step [78/84], Loss: 0.1631\n",
      "Epoch [50/100], Step [79/84], Loss: 0.1761\n",
      "Epoch [50/100], Step [80/84], Loss: 0.1828\n",
      "Epoch [50/100], Step [81/84], Loss: 0.1661\n",
      "Epoch [50/100], Step [82/84], Loss: 0.1660\n",
      "Epoch [50/100], Step [83/84], Loss: 0.1778\n",
      "Epoch [50/100], Step [84/84], Loss: 0.1924\n",
      "Epoch [51/100], Step [1/84], Loss: 0.1732\n",
      "Epoch [51/100], Step [2/84], Loss: 0.1697\n",
      "Epoch [51/100], Step [3/84], Loss: 0.1726\n",
      "Epoch [51/100], Step [4/84], Loss: 0.2001\n",
      "Epoch [51/100], Step [5/84], Loss: 0.1689\n",
      "Epoch [51/100], Step [6/84], Loss: 0.1760\n",
      "Epoch [51/100], Step [7/84], Loss: 0.1735\n",
      "Epoch [51/100], Step [8/84], Loss: 0.1585\n",
      "Epoch [51/100], Step [9/84], Loss: 0.1842\n",
      "Epoch [51/100], Step [10/84], Loss: 0.1780\n",
      "Epoch [51/100], Step [11/84], Loss: 0.1768\n",
      "Epoch [51/100], Step [12/84], Loss: 0.1726\n",
      "Epoch [51/100], Step [13/84], Loss: 0.1777\n",
      "Epoch [51/100], Step [14/84], Loss: 0.2257\n",
      "Epoch [51/100], Step [15/84], Loss: 0.1695\n",
      "Epoch [51/100], Step [16/84], Loss: 0.1942\n",
      "Epoch [51/100], Step [17/84], Loss: 0.1671\n",
      "Epoch [51/100], Step [18/84], Loss: 0.1644\n",
      "Epoch [51/100], Step [19/84], Loss: 0.1634\n",
      "Epoch [51/100], Step [20/84], Loss: 0.1586\n",
      "Epoch [51/100], Step [21/84], Loss: 0.1680\n",
      "Epoch [51/100], Step [22/84], Loss: 0.1725\n",
      "Epoch [51/100], Step [23/84], Loss: 0.1651\n",
      "Epoch [51/100], Step [24/84], Loss: 0.1662\n",
      "Epoch [51/100], Step [25/84], Loss: 0.1650\n",
      "Epoch [51/100], Step [26/84], Loss: 0.1828\n",
      "Epoch [51/100], Step [27/84], Loss: 0.1751\n",
      "Epoch [51/100], Step [28/84], Loss: 0.1833\n",
      "Epoch [51/100], Step [29/84], Loss: 0.1794\n",
      "Epoch [51/100], Step [30/84], Loss: 0.1672\n",
      "Epoch [51/100], Step [31/84], Loss: 0.2014\n",
      "Epoch [51/100], Step [32/84], Loss: 0.1810\n",
      "Epoch [51/100], Step [33/84], Loss: 0.1805\n",
      "Epoch [51/100], Step [34/84], Loss: 0.1815\n",
      "Epoch [51/100], Step [35/84], Loss: 0.1853\n",
      "Epoch [51/100], Step [36/84], Loss: 0.1776\n",
      "Epoch [51/100], Step [37/84], Loss: 0.2008\n",
      "Epoch [51/100], Step [38/84], Loss: 0.1730\n",
      "Epoch [51/100], Step [39/84], Loss: 0.1872\n",
      "Epoch [51/100], Step [40/84], Loss: 0.1692\n",
      "Epoch [51/100], Step [41/84], Loss: 0.1888\n",
      "Epoch [51/100], Step [42/84], Loss: 0.1838\n",
      "Epoch [51/100], Step [43/84], Loss: 0.1655\n",
      "Epoch [51/100], Step [44/84], Loss: 0.1771\n",
      "Epoch [51/100], Step [45/84], Loss: 0.1777\n",
      "Epoch [51/100], Step [46/84], Loss: 0.1695\n",
      "Epoch [51/100], Step [47/84], Loss: 0.1701\n",
      "Epoch [51/100], Step [48/84], Loss: 0.1709\n",
      "Epoch [51/100], Step [49/84], Loss: 0.1713\n",
      "Epoch [51/100], Step [50/84], Loss: 0.1796\n",
      "Epoch [51/100], Step [51/84], Loss: 0.1879\n",
      "Epoch [51/100], Step [52/84], Loss: 0.2120\n",
      "Epoch [51/100], Step [53/84], Loss: 0.1557\n",
      "Epoch [51/100], Step [54/84], Loss: 0.1844\n",
      "Epoch [51/100], Step [55/84], Loss: 0.1692\n",
      "Epoch [51/100], Step [56/84], Loss: 0.1693\n",
      "Epoch [51/100], Step [57/84], Loss: 0.1836\n",
      "Epoch [51/100], Step [58/84], Loss: 0.1773\n",
      "Epoch [51/100], Step [59/84], Loss: 0.2127\n",
      "Epoch [51/100], Step [60/84], Loss: 0.1742\n",
      "Epoch [51/100], Step [61/84], Loss: 0.1773\n",
      "Epoch [51/100], Step [62/84], Loss: 0.1700\n",
      "Epoch [51/100], Step [63/84], Loss: 0.1691\n",
      "Epoch [51/100], Step [64/84], Loss: 0.1895\n",
      "Epoch [51/100], Step [65/84], Loss: 0.1676\n",
      "Epoch [51/100], Step [66/84], Loss: 0.1914\n",
      "Epoch [51/100], Step [67/84], Loss: 0.1680\n",
      "Epoch [51/100], Step [68/84], Loss: 0.1885\n",
      "Epoch [51/100], Step [69/84], Loss: 0.1602\n",
      "Epoch [51/100], Step [70/84], Loss: 0.1680\n",
      "Epoch [51/100], Step [71/84], Loss: 0.1827\n",
      "Epoch [51/100], Step [72/84], Loss: 0.1737\n",
      "Epoch [51/100], Step [73/84], Loss: 0.1853\n",
      "Epoch [51/100], Step [74/84], Loss: 0.1653\n",
      "Epoch [51/100], Step [75/84], Loss: 0.1857\n",
      "Epoch [51/100], Step [76/84], Loss: 0.1810\n",
      "Epoch [51/100], Step [77/84], Loss: 0.1837\n",
      "Epoch [51/100], Step [78/84], Loss: 0.1895\n",
      "Epoch [51/100], Step [79/84], Loss: 0.1673\n",
      "Epoch [51/100], Step [80/84], Loss: 0.1724\n",
      "Epoch [51/100], Step [81/84], Loss: 0.1753\n",
      "Epoch [51/100], Step [82/84], Loss: 0.1809\n",
      "Epoch [51/100], Step [83/84], Loss: 0.1737\n",
      "Epoch [51/100], Step [84/84], Loss: 0.1923\n",
      "Epoch [52/100], Step [1/84], Loss: 0.1749\n",
      "Epoch [52/100], Step [2/84], Loss: 0.1847\n",
      "Epoch [52/100], Step [3/84], Loss: 0.1669\n",
      "Epoch [52/100], Step [4/84], Loss: 0.1813\n",
      "Epoch [52/100], Step [5/84], Loss: 0.1712\n",
      "Epoch [52/100], Step [6/84], Loss: 0.1847\n",
      "Epoch [52/100], Step [7/84], Loss: 0.1618\n",
      "Epoch [52/100], Step [8/84], Loss: 0.1651\n",
      "Epoch [52/100], Step [9/84], Loss: 0.1781\n",
      "Epoch [52/100], Step [10/84], Loss: 0.1663\n",
      "Epoch [52/100], Step [11/84], Loss: 0.1927\n",
      "Epoch [52/100], Step [12/84], Loss: 0.1644\n",
      "Epoch [52/100], Step [13/84], Loss: 0.1864\n",
      "Epoch [52/100], Step [14/84], Loss: 0.1745\n",
      "Epoch [52/100], Step [15/84], Loss: 0.1710\n",
      "Epoch [52/100], Step [16/84], Loss: 0.1544\n",
      "Epoch [52/100], Step [17/84], Loss: 0.1677\n",
      "Epoch [52/100], Step [18/84], Loss: 0.1669\n",
      "Epoch [52/100], Step [19/84], Loss: 0.1684\n",
      "Epoch [52/100], Step [20/84], Loss: 0.1953\n",
      "Epoch [52/100], Step [21/84], Loss: 0.1648\n",
      "Epoch [52/100], Step [22/84], Loss: 0.1777\n",
      "Epoch [52/100], Step [23/84], Loss: 0.1860\n",
      "Epoch [52/100], Step [24/84], Loss: 0.1744\n",
      "Epoch [52/100], Step [25/84], Loss: 0.1677\n",
      "Epoch [52/100], Step [26/84], Loss: 0.1990\n",
      "Epoch [52/100], Step [27/84], Loss: 0.1822\n",
      "Epoch [52/100], Step [28/84], Loss: 0.1720\n",
      "Epoch [52/100], Step [29/84], Loss: 0.1616\n",
      "Epoch [52/100], Step [30/84], Loss: 0.1699\n",
      "Epoch [52/100], Step [31/84], Loss: 0.1576\n",
      "Epoch [52/100], Step [32/84], Loss: 0.1963\n",
      "Epoch [52/100], Step [33/84], Loss: 0.1768\n",
      "Epoch [52/100], Step [34/84], Loss: 0.1755\n",
      "Epoch [52/100], Step [35/84], Loss: 0.1678\n",
      "Epoch [52/100], Step [36/84], Loss: 0.1791\n",
      "Epoch [52/100], Step [37/84], Loss: 0.1774\n",
      "Epoch [52/100], Step [38/84], Loss: 0.1679\n",
      "Epoch [52/100], Step [39/84], Loss: 0.1797\n",
      "Epoch [52/100], Step [40/84], Loss: 0.1650\n",
      "Epoch [52/100], Step [41/84], Loss: 0.1602\n",
      "Epoch [52/100], Step [42/84], Loss: 0.1636\n",
      "Epoch [52/100], Step [43/84], Loss: 0.1792\n",
      "Epoch [52/100], Step [44/84], Loss: 0.1710\n",
      "Epoch [52/100], Step [45/84], Loss: 0.1684\n",
      "Epoch [52/100], Step [46/84], Loss: 0.2017\n",
      "Epoch [52/100], Step [47/84], Loss: 0.1755\n",
      "Epoch [52/100], Step [48/84], Loss: 0.1891\n",
      "Epoch [52/100], Step [49/84], Loss: 0.1645\n",
      "Epoch [52/100], Step [50/84], Loss: 0.1641\n",
      "Epoch [52/100], Step [51/84], Loss: 0.1742\n",
      "Epoch [52/100], Step [52/84], Loss: 0.1815\n",
      "Epoch [52/100], Step [53/84], Loss: 0.1995\n",
      "Epoch [52/100], Step [54/84], Loss: 0.1682\n",
      "Epoch [52/100], Step [55/84], Loss: 0.1951\n",
      "Epoch [52/100], Step [56/84], Loss: 0.1870\n",
      "Epoch [52/100], Step [57/84], Loss: 0.1859\n",
      "Epoch [52/100], Step [58/84], Loss: 0.1819\n",
      "Epoch [52/100], Step [59/84], Loss: 0.1782\n",
      "Epoch [52/100], Step [60/84], Loss: 0.1928\n",
      "Epoch [52/100], Step [61/84], Loss: 0.1591\n",
      "Epoch [52/100], Step [62/84], Loss: 0.1694\n",
      "Epoch [52/100], Step [63/84], Loss: 0.1581\n",
      "Epoch [52/100], Step [64/84], Loss: 0.1874\n",
      "Epoch [52/100], Step [65/84], Loss: 0.1702\n",
      "Epoch [52/100], Step [66/84], Loss: 0.1644\n",
      "Epoch [52/100], Step [67/84], Loss: 0.1578\n",
      "Epoch [52/100], Step [68/84], Loss: 0.1717\n",
      "Epoch [52/100], Step [69/84], Loss: 0.1713\n",
      "Epoch [52/100], Step [70/84], Loss: 0.1731\n",
      "Epoch [52/100], Step [71/84], Loss: 0.1759\n",
      "Epoch [52/100], Step [72/84], Loss: 0.1786\n",
      "Epoch [52/100], Step [73/84], Loss: 0.1657\n",
      "Epoch [52/100], Step [74/84], Loss: 0.1566\n",
      "Epoch [52/100], Step [75/84], Loss: 0.1719\n",
      "Epoch [52/100], Step [76/84], Loss: 0.1718\n",
      "Epoch [52/100], Step [77/84], Loss: 0.1721\n",
      "Epoch [52/100], Step [78/84], Loss: 0.1868\n",
      "Epoch [52/100], Step [79/84], Loss: 0.1826\n",
      "Epoch [52/100], Step [80/84], Loss: 0.1829\n",
      "Epoch [52/100], Step [81/84], Loss: 0.1786\n",
      "Epoch [52/100], Step [82/84], Loss: 0.1800\n",
      "Epoch [52/100], Step [83/84], Loss: 0.1532\n",
      "Epoch [52/100], Step [84/84], Loss: 0.1542\n",
      "Epoch [53/100], Step [1/84], Loss: 0.1831\n",
      "Epoch [53/100], Step [2/84], Loss: 0.1743\n",
      "Epoch [53/100], Step [3/84], Loss: 0.1701\n",
      "Epoch [53/100], Step [4/84], Loss: 0.1703\n",
      "Epoch [53/100], Step [5/84], Loss: 0.1729\n",
      "Epoch [53/100], Step [6/84], Loss: 0.1741\n",
      "Epoch [53/100], Step [7/84], Loss: 0.1718\n",
      "Epoch [53/100], Step [8/84], Loss: 0.1786\n",
      "Epoch [53/100], Step [9/84], Loss: 0.1630\n",
      "Epoch [53/100], Step [10/84], Loss: 0.1637\n",
      "Epoch [53/100], Step [11/84], Loss: 0.1623\n",
      "Epoch [53/100], Step [12/84], Loss: 0.1609\n",
      "Epoch [53/100], Step [13/84], Loss: 0.1632\n",
      "Epoch [53/100], Step [14/84], Loss: 0.1736\n",
      "Epoch [53/100], Step [15/84], Loss: 0.1830\n",
      "Epoch [53/100], Step [16/84], Loss: 0.1755\n",
      "Epoch [53/100], Step [17/84], Loss: 0.1698\n",
      "Epoch [53/100], Step [18/84], Loss: 0.1621\n",
      "Epoch [53/100], Step [19/84], Loss: 0.1747\n",
      "Epoch [53/100], Step [20/84], Loss: 0.1650\n",
      "Epoch [53/100], Step [21/84], Loss: 0.1904\n",
      "Epoch [53/100], Step [22/84], Loss: 0.2083\n",
      "Epoch [53/100], Step [23/84], Loss: 0.1691\n",
      "Epoch [53/100], Step [24/84], Loss: 0.1595\n",
      "Epoch [53/100], Step [25/84], Loss: 0.1746\n",
      "Epoch [53/100], Step [26/84], Loss: 0.2032\n",
      "Epoch [53/100], Step [27/84], Loss: 0.1691\n",
      "Epoch [53/100], Step [28/84], Loss: 0.1703\n",
      "Epoch [53/100], Step [29/84], Loss: 0.1781\n",
      "Epoch [53/100], Step [30/84], Loss: 0.1691\n",
      "Epoch [53/100], Step [31/84], Loss: 0.1772\n",
      "Epoch [53/100], Step [32/84], Loss: 0.1767\n",
      "Epoch [53/100], Step [33/84], Loss: 0.1646\n",
      "Epoch [53/100], Step [34/84], Loss: 0.1808\n",
      "Epoch [53/100], Step [35/84], Loss: 0.1544\n",
      "Epoch [53/100], Step [36/84], Loss: 0.1586\n",
      "Epoch [53/100], Step [37/84], Loss: 0.1669\n",
      "Epoch [53/100], Step [38/84], Loss: 0.1840\n",
      "Epoch [53/100], Step [39/84], Loss: 0.1853\n",
      "Epoch [53/100], Step [40/84], Loss: 0.1657\n",
      "Epoch [53/100], Step [41/84], Loss: 0.1877\n",
      "Epoch [53/100], Step [42/84], Loss: 0.1798\n",
      "Epoch [53/100], Step [43/84], Loss: 0.1742\n",
      "Epoch [53/100], Step [44/84], Loss: 0.1736\n",
      "Epoch [53/100], Step [45/84], Loss: 0.1718\n",
      "Epoch [53/100], Step [46/84], Loss: 0.1772\n",
      "Epoch [53/100], Step [47/84], Loss: 0.1759\n",
      "Epoch [53/100], Step [48/84], Loss: 0.1874\n",
      "Epoch [53/100], Step [49/84], Loss: 0.1824\n",
      "Epoch [53/100], Step [50/84], Loss: 0.1742\n",
      "Epoch [53/100], Step [51/84], Loss: 0.1742\n",
      "Epoch [53/100], Step [52/84], Loss: 0.1745\n",
      "Epoch [53/100], Step [53/84], Loss: 0.1723\n",
      "Epoch [53/100], Step [54/84], Loss: 0.1709\n",
      "Epoch [53/100], Step [55/84], Loss: 0.1765\n",
      "Epoch [53/100], Step [56/84], Loss: 0.1717\n",
      "Epoch [53/100], Step [57/84], Loss: 0.1740\n",
      "Epoch [53/100], Step [58/84], Loss: 0.1674\n",
      "Epoch [53/100], Step [59/84], Loss: 0.1770\n",
      "Epoch [53/100], Step [60/84], Loss: 0.1857\n",
      "Epoch [53/100], Step [61/84], Loss: 0.1731\n",
      "Epoch [53/100], Step [62/84], Loss: 0.2073\n",
      "Epoch [53/100], Step [63/84], Loss: 0.1632\n",
      "Epoch [53/100], Step [64/84], Loss: 0.1696\n",
      "Epoch [53/100], Step [65/84], Loss: 0.1825\n",
      "Epoch [53/100], Step [66/84], Loss: 0.1697\n",
      "Epoch [53/100], Step [67/84], Loss: 0.1990\n",
      "Epoch [53/100], Step [68/84], Loss: 0.1596\n",
      "Epoch [53/100], Step [69/84], Loss: 0.1837\n",
      "Epoch [53/100], Step [70/84], Loss: 0.1671\n",
      "Epoch [53/100], Step [71/84], Loss: 0.1744\n",
      "Epoch [53/100], Step [72/84], Loss: 0.1750\n",
      "Epoch [53/100], Step [73/84], Loss: 0.1631\n",
      "Epoch [53/100], Step [74/84], Loss: 0.1760\n",
      "Epoch [53/100], Step [75/84], Loss: 0.1580\n",
      "Epoch [53/100], Step [76/84], Loss: 0.1644\n",
      "Epoch [53/100], Step [77/84], Loss: 0.1750\n",
      "Epoch [53/100], Step [78/84], Loss: 0.1622\n",
      "Epoch [53/100], Step [79/84], Loss: 0.1828\n",
      "Epoch [53/100], Step [80/84], Loss: 0.1609\n",
      "Epoch [53/100], Step [81/84], Loss: 0.1803\n",
      "Epoch [53/100], Step [82/84], Loss: 0.1931\n",
      "Epoch [53/100], Step [83/84], Loss: 0.1644\n",
      "Epoch [53/100], Step [84/84], Loss: 0.1647\n",
      "Epoch [54/100], Step [1/84], Loss: 0.1804\n",
      "Epoch [54/100], Step [2/84], Loss: 0.1639\n",
      "Epoch [54/100], Step [3/84], Loss: 0.1651\n",
      "Epoch [54/100], Step [4/84], Loss: 0.1700\n",
      "Epoch [54/100], Step [5/84], Loss: 0.1844\n",
      "Epoch [54/100], Step [6/84], Loss: 0.1568\n",
      "Epoch [54/100], Step [7/84], Loss: 0.1630\n",
      "Epoch [54/100], Step [8/84], Loss: 0.1597\n",
      "Epoch [54/100], Step [9/84], Loss: 0.1640\n",
      "Epoch [54/100], Step [10/84], Loss: 0.1728\n",
      "Epoch [54/100], Step [11/84], Loss: 0.1783\n",
      "Epoch [54/100], Step [12/84], Loss: 0.1701\n",
      "Epoch [54/100], Step [13/84], Loss: 0.1718\n",
      "Epoch [54/100], Step [14/84], Loss: 0.1565\n",
      "Epoch [54/100], Step [15/84], Loss: 0.1708\n",
      "Epoch [54/100], Step [16/84], Loss: 0.1569\n",
      "Epoch [54/100], Step [17/84], Loss: 0.1602\n",
      "Epoch [54/100], Step [18/84], Loss: 0.1767\n",
      "Epoch [54/100], Step [19/84], Loss: 0.1718\n",
      "Epoch [54/100], Step [20/84], Loss: 0.1738\n",
      "Epoch [54/100], Step [21/84], Loss: 0.1669\n",
      "Epoch [54/100], Step [22/84], Loss: 0.1697\n",
      "Epoch [54/100], Step [23/84], Loss: 0.1733\n",
      "Epoch [54/100], Step [24/84], Loss: 0.1597\n",
      "Epoch [54/100], Step [25/84], Loss: 0.1763\n",
      "Epoch [54/100], Step [26/84], Loss: 0.1647\n",
      "Epoch [54/100], Step [27/84], Loss: 0.1595\n",
      "Epoch [54/100], Step [28/84], Loss: 0.1809\n",
      "Epoch [54/100], Step [29/84], Loss: 0.1712\n",
      "Epoch [54/100], Step [30/84], Loss: 0.1645\n",
      "Epoch [54/100], Step [31/84], Loss: 0.1673\n",
      "Epoch [54/100], Step [32/84], Loss: 0.1684\n",
      "Epoch [54/100], Step [33/84], Loss: 0.1619\n",
      "Epoch [54/100], Step [34/84], Loss: 0.1693\n",
      "Epoch [54/100], Step [35/84], Loss: 0.1877\n",
      "Epoch [54/100], Step [36/84], Loss: 0.1800\n",
      "Epoch [54/100], Step [37/84], Loss: 0.1915\n",
      "Epoch [54/100], Step [38/84], Loss: 0.1804\n",
      "Epoch [54/100], Step [39/84], Loss: 0.1575\n",
      "Epoch [54/100], Step [40/84], Loss: 0.1710\n",
      "Epoch [54/100], Step [41/84], Loss: 0.1920\n",
      "Epoch [54/100], Step [42/84], Loss: 0.1745\n",
      "Epoch [54/100], Step [43/84], Loss: 0.1676\n",
      "Epoch [54/100], Step [44/84], Loss: 0.1799\n",
      "Epoch [54/100], Step [45/84], Loss: 0.1698\n",
      "Epoch [54/100], Step [46/84], Loss: 0.1622\n",
      "Epoch [54/100], Step [47/84], Loss: 0.1624\n",
      "Epoch [54/100], Step [48/84], Loss: 0.1841\n",
      "Epoch [54/100], Step [49/84], Loss: 0.1681\n",
      "Epoch [54/100], Step [50/84], Loss: 0.1720\n",
      "Epoch [54/100], Step [51/84], Loss: 0.1557\n",
      "Epoch [54/100], Step [52/84], Loss: 0.1708\n",
      "Epoch [54/100], Step [53/84], Loss: 0.1511\n",
      "Epoch [54/100], Step [54/84], Loss: 0.1548\n",
      "Epoch [54/100], Step [55/84], Loss: 0.1875\n",
      "Epoch [54/100], Step [56/84], Loss: 0.1915\n",
      "Epoch [54/100], Step [57/84], Loss: 0.1791\n",
      "Epoch [54/100], Step [58/84], Loss: 0.1635\n",
      "Epoch [54/100], Step [59/84], Loss: 0.1802\n",
      "Epoch [54/100], Step [60/84], Loss: 0.2199\n",
      "Epoch [54/100], Step [61/84], Loss: 0.1791\n",
      "Epoch [54/100], Step [62/84], Loss: 0.1740\n",
      "Epoch [54/100], Step [63/84], Loss: 0.1779\n",
      "Epoch [54/100], Step [64/84], Loss: 0.1785\n",
      "Epoch [54/100], Step [65/84], Loss: 0.1787\n",
      "Epoch [54/100], Step [66/84], Loss: 0.1776\n",
      "Epoch [54/100], Step [67/84], Loss: 0.1786\n",
      "Epoch [54/100], Step [68/84], Loss: 0.1826\n",
      "Epoch [54/100], Step [69/84], Loss: 0.1675\n",
      "Epoch [54/100], Step [70/84], Loss: 0.1538\n",
      "Epoch [54/100], Step [71/84], Loss: 0.1762\n",
      "Epoch [54/100], Step [72/84], Loss: 0.1774\n",
      "Epoch [54/100], Step [73/84], Loss: 0.1662\n",
      "Epoch [54/100], Step [74/84], Loss: 0.1612\n",
      "Epoch [54/100], Step [75/84], Loss: 0.1724\n",
      "Epoch [54/100], Step [76/84], Loss: 0.1672\n",
      "Epoch [54/100], Step [77/84], Loss: 0.1975\n",
      "Epoch [54/100], Step [78/84], Loss: 0.1761\n",
      "Epoch [54/100], Step [79/84], Loss: 0.1767\n",
      "Epoch [54/100], Step [80/84], Loss: 0.1929\n",
      "Epoch [54/100], Step [81/84], Loss: 0.1810\n",
      "Epoch [54/100], Step [82/84], Loss: 0.1658\n",
      "Epoch [54/100], Step [83/84], Loss: 0.1596\n",
      "Epoch [54/100], Step [84/84], Loss: 0.1574\n",
      "Epoch [55/100], Step [1/84], Loss: 0.1683\n",
      "Epoch [55/100], Step [2/84], Loss: 0.1629\n",
      "Epoch [55/100], Step [3/84], Loss: 0.1736\n",
      "Epoch [55/100], Step [4/84], Loss: 0.1975\n",
      "Epoch [55/100], Step [5/84], Loss: 0.1701\n",
      "Epoch [55/100], Step [6/84], Loss: 0.1750\n",
      "Epoch [55/100], Step [7/84], Loss: 0.1673\n",
      "Epoch [55/100], Step [8/84], Loss: 0.1565\n",
      "Epoch [55/100], Step [9/84], Loss: 0.1592\n",
      "Epoch [55/100], Step [10/84], Loss: 0.2041\n",
      "Epoch [55/100], Step [11/84], Loss: 0.1704\n",
      "Epoch [55/100], Step [12/84], Loss: 0.1694\n",
      "Epoch [55/100], Step [13/84], Loss: 0.1634\n",
      "Epoch [55/100], Step [14/84], Loss: 0.1837\n",
      "Epoch [55/100], Step [15/84], Loss: 0.1725\n",
      "Epoch [55/100], Step [16/84], Loss: 0.1692\n",
      "Epoch [55/100], Step [17/84], Loss: 0.1825\n",
      "Epoch [55/100], Step [18/84], Loss: 0.1878\n",
      "Epoch [55/100], Step [19/84], Loss: 0.1578\n",
      "Epoch [55/100], Step [20/84], Loss: 0.1769\n",
      "Epoch [55/100], Step [21/84], Loss: 0.1631\n",
      "Epoch [55/100], Step [22/84], Loss: 0.1682\n",
      "Epoch [55/100], Step [23/84], Loss: 0.1596\n",
      "Epoch [55/100], Step [24/84], Loss: 0.1695\n",
      "Epoch [55/100], Step [25/84], Loss: 0.1611\n",
      "Epoch [55/100], Step [26/84], Loss: 0.1573\n",
      "Epoch [55/100], Step [27/84], Loss: 0.1598\n",
      "Epoch [55/100], Step [28/84], Loss: 0.1653\n",
      "Epoch [55/100], Step [29/84], Loss: 0.1563\n",
      "Epoch [55/100], Step [30/84], Loss: 0.1697\n",
      "Epoch [55/100], Step [31/84], Loss: 0.1711\n",
      "Epoch [55/100], Step [32/84], Loss: 0.1827\n",
      "Epoch [55/100], Step [33/84], Loss: 0.1715\n",
      "Epoch [55/100], Step [34/84], Loss: 0.1938\n",
      "Epoch [55/100], Step [35/84], Loss: 0.1649\n",
      "Epoch [55/100], Step [36/84], Loss: 0.1628\n",
      "Epoch [55/100], Step [37/84], Loss: 0.1844\n",
      "Epoch [55/100], Step [38/84], Loss: 0.1694\n",
      "Epoch [55/100], Step [39/84], Loss: 0.1876\n",
      "Epoch [55/100], Step [40/84], Loss: 0.1803\n",
      "Epoch [55/100], Step [41/84], Loss: 0.1842\n",
      "Epoch [55/100], Step [42/84], Loss: 0.1769\n",
      "Epoch [55/100], Step [43/84], Loss: 0.2000\n",
      "Epoch [55/100], Step [44/84], Loss: 0.1815\n",
      "Epoch [55/100], Step [45/84], Loss: 0.1578\n",
      "Epoch [55/100], Step [46/84], Loss: 0.1702\n",
      "Epoch [55/100], Step [47/84], Loss: 0.2143\n",
      "Epoch [55/100], Step [48/84], Loss: 0.1652\n",
      "Epoch [55/100], Step [49/84], Loss: 0.1697\n",
      "Epoch [55/100], Step [50/84], Loss: 0.1690\n",
      "Epoch [55/100], Step [51/84], Loss: 0.1817\n",
      "Epoch [55/100], Step [52/84], Loss: 0.1665\n",
      "Epoch [55/100], Step [53/84], Loss: 0.1873\n",
      "Epoch [55/100], Step [54/84], Loss: 0.1791\n",
      "Epoch [55/100], Step [55/84], Loss: 0.1712\n",
      "Epoch [55/100], Step [56/84], Loss: 0.1615\n",
      "Epoch [55/100], Step [57/84], Loss: 0.1729\n",
      "Epoch [55/100], Step [58/84], Loss: 0.1567\n",
      "Epoch [55/100], Step [59/84], Loss: 0.1831\n",
      "Epoch [55/100], Step [60/84], Loss: 0.1638\n",
      "Epoch [55/100], Step [61/84], Loss: 0.1676\n",
      "Epoch [55/100], Step [62/84], Loss: 0.1798\n",
      "Epoch [55/100], Step [63/84], Loss: 0.1890\n",
      "Epoch [55/100], Step [64/84], Loss: 0.1661\n",
      "Epoch [55/100], Step [65/84], Loss: 0.1624\n",
      "Epoch [55/100], Step [66/84], Loss: 0.1597\n",
      "Epoch [55/100], Step [67/84], Loss: 0.1637\n",
      "Epoch [55/100], Step [68/84], Loss: 0.1732\n",
      "Epoch [55/100], Step [69/84], Loss: 0.1645\n",
      "Epoch [55/100], Step [70/84], Loss: 0.1859\n",
      "Epoch [55/100], Step [71/84], Loss: 0.1784\n",
      "Epoch [55/100], Step [72/84], Loss: 0.1657\n",
      "Epoch [55/100], Step [73/84], Loss: 0.1640\n",
      "Epoch [55/100], Step [74/84], Loss: 0.1767\n",
      "Epoch [55/100], Step [75/84], Loss: 0.1767\n",
      "Epoch [55/100], Step [76/84], Loss: 0.1882\n",
      "Epoch [55/100], Step [77/84], Loss: 0.1630\n",
      "Epoch [55/100], Step [78/84], Loss: 0.1712\n",
      "Epoch [55/100], Step [79/84], Loss: 0.1541\n",
      "Epoch [55/100], Step [80/84], Loss: 0.1788\n",
      "Epoch [55/100], Step [81/84], Loss: 0.1664\n",
      "Epoch [55/100], Step [82/84], Loss: 0.1738\n",
      "Epoch [55/100], Step [83/84], Loss: 0.1726\n",
      "Epoch [55/100], Step [84/84], Loss: 0.1818\n",
      "Epoch [56/100], Step [1/84], Loss: 0.1739\n",
      "Epoch [56/100], Step [2/84], Loss: 0.1587\n",
      "Epoch [56/100], Step [3/84], Loss: 0.1609\n",
      "Epoch [56/100], Step [4/84], Loss: 0.1757\n",
      "Epoch [56/100], Step [5/84], Loss: 0.1662\n",
      "Epoch [56/100], Step [6/84], Loss: 0.1509\n",
      "Epoch [56/100], Step [7/84], Loss: 0.1744\n",
      "Epoch [56/100], Step [8/84], Loss: 0.1699\n",
      "Epoch [56/100], Step [9/84], Loss: 0.1539\n",
      "Epoch [56/100], Step [10/84], Loss: 0.1778\n",
      "Epoch [56/100], Step [11/84], Loss: 0.1850\n",
      "Epoch [56/100], Step [12/84], Loss: 0.1423\n",
      "Epoch [56/100], Step [13/84], Loss: 0.1617\n",
      "Epoch [56/100], Step [14/84], Loss: 0.1763\n",
      "Epoch [56/100], Step [15/84], Loss: 0.1712\n",
      "Epoch [56/100], Step [16/84], Loss: 0.1598\n",
      "Epoch [56/100], Step [17/84], Loss: 0.1727\n",
      "Epoch [56/100], Step [18/84], Loss: 0.1824\n",
      "Epoch [56/100], Step [19/84], Loss: 0.1654\n",
      "Epoch [56/100], Step [20/84], Loss: 0.1909\n",
      "Epoch [56/100], Step [21/84], Loss: 0.1711\n",
      "Epoch [56/100], Step [22/84], Loss: 0.1733\n",
      "Epoch [56/100], Step [23/84], Loss: 0.1629\n",
      "Epoch [56/100], Step [24/84], Loss: 0.1834\n",
      "Epoch [56/100], Step [25/84], Loss: 0.1693\n",
      "Epoch [56/100], Step [26/84], Loss: 0.1876\n",
      "Epoch [56/100], Step [27/84], Loss: 0.1832\n",
      "Epoch [56/100], Step [28/84], Loss: 0.1584\n",
      "Epoch [56/100], Step [29/84], Loss: 0.1617\n",
      "Epoch [56/100], Step [30/84], Loss: 0.2102\n",
      "Epoch [56/100], Step [31/84], Loss: 0.1866\n",
      "Epoch [56/100], Step [32/84], Loss: 0.1991\n",
      "Epoch [56/100], Step [33/84], Loss: 0.1645\n",
      "Epoch [56/100], Step [34/84], Loss: 0.1798\n",
      "Epoch [56/100], Step [35/84], Loss: 0.1799\n",
      "Epoch [56/100], Step [36/84], Loss: 0.1730\n",
      "Epoch [56/100], Step [37/84], Loss: 0.1741\n",
      "Epoch [56/100], Step [38/84], Loss: 0.1885\n",
      "Epoch [56/100], Step [39/84], Loss: 0.1585\n",
      "Epoch [56/100], Step [40/84], Loss: 0.1543\n",
      "Epoch [56/100], Step [41/84], Loss: 0.1814\n",
      "Epoch [56/100], Step [42/84], Loss: 0.1599\n",
      "Epoch [56/100], Step [43/84], Loss: 0.1541\n",
      "Epoch [56/100], Step [44/84], Loss: 0.1764\n",
      "Epoch [56/100], Step [45/84], Loss: 0.1845\n",
      "Epoch [56/100], Step [46/84], Loss: 0.1717\n",
      "Epoch [56/100], Step [47/84], Loss: 0.1845\n",
      "Epoch [56/100], Step [48/84], Loss: 0.1638\n",
      "Epoch [56/100], Step [49/84], Loss: 0.1776\n",
      "Epoch [56/100], Step [50/84], Loss: 0.1680\n",
      "Epoch [56/100], Step [51/84], Loss: 0.1701\n",
      "Epoch [56/100], Step [52/84], Loss: 0.1792\n",
      "Epoch [56/100], Step [53/84], Loss: 0.1768\n",
      "Epoch [56/100], Step [54/84], Loss: 0.1809\n",
      "Epoch [56/100], Step [55/84], Loss: 0.1612\n",
      "Epoch [56/100], Step [56/84], Loss: 0.2041\n",
      "Epoch [56/100], Step [57/84], Loss: 0.1696\n",
      "Epoch [56/100], Step [58/84], Loss: 0.1609\n",
      "Epoch [56/100], Step [59/84], Loss: 0.1883\n",
      "Epoch [56/100], Step [60/84], Loss: 0.1898\n",
      "Epoch [56/100], Step [61/84], Loss: 0.1710\n",
      "Epoch [56/100], Step [62/84], Loss: 0.1619\n",
      "Epoch [56/100], Step [63/84], Loss: 0.1762\n",
      "Epoch [56/100], Step [64/84], Loss: 0.1538\n",
      "Epoch [56/100], Step [65/84], Loss: 0.1811\n",
      "Epoch [56/100], Step [66/84], Loss: 0.1871\n",
      "Epoch [56/100], Step [67/84], Loss: 0.1563\n",
      "Epoch [56/100], Step [68/84], Loss: 0.1672\n",
      "Epoch [56/100], Step [69/84], Loss: 0.1540\n",
      "Epoch [56/100], Step [70/84], Loss: 0.1905\n",
      "Epoch [56/100], Step [71/84], Loss: 0.1786\n",
      "Epoch [56/100], Step [72/84], Loss: 0.1940\n",
      "Epoch [56/100], Step [73/84], Loss: 0.1627\n",
      "Epoch [56/100], Step [74/84], Loss: 0.1649\n",
      "Epoch [56/100], Step [75/84], Loss: 0.1731\n",
      "Epoch [56/100], Step [76/84], Loss: 0.1645\n",
      "Epoch [56/100], Step [77/84], Loss: 0.1651\n",
      "Epoch [56/100], Step [78/84], Loss: 0.1687\n",
      "Epoch [56/100], Step [79/84], Loss: 0.1627\n",
      "Epoch [56/100], Step [80/84], Loss: 0.1741\n",
      "Epoch [56/100], Step [81/84], Loss: 0.1878\n",
      "Epoch [56/100], Step [82/84], Loss: 0.1699\n",
      "Epoch [56/100], Step [83/84], Loss: 0.1763\n",
      "Epoch [56/100], Step [84/84], Loss: 0.1754\n",
      "Epoch [57/100], Step [1/84], Loss: 0.1572\n",
      "Epoch [57/100], Step [2/84], Loss: 0.1576\n",
      "Epoch [57/100], Step [3/84], Loss: 0.1582\n",
      "Epoch [57/100], Step [4/84], Loss: 0.1736\n",
      "Epoch [57/100], Step [5/84], Loss: 0.1726\n",
      "Epoch [57/100], Step [6/84], Loss: 0.1547\n",
      "Epoch [57/100], Step [7/84], Loss: 0.1802\n",
      "Epoch [57/100], Step [8/84], Loss: 0.1649\n",
      "Epoch [57/100], Step [9/84], Loss: 0.1551\n",
      "Epoch [57/100], Step [10/84], Loss: 0.1765\n",
      "Epoch [57/100], Step [11/84], Loss: 0.1680\n",
      "Epoch [57/100], Step [12/84], Loss: 0.1705\n",
      "Epoch [57/100], Step [13/84], Loss: 0.1918\n",
      "Epoch [57/100], Step [14/84], Loss: 0.1571\n",
      "Epoch [57/100], Step [15/84], Loss: 0.1621\n",
      "Epoch [57/100], Step [16/84], Loss: 0.1729\n",
      "Epoch [57/100], Step [17/84], Loss: 0.1611\n",
      "Epoch [57/100], Step [18/84], Loss: 0.1635\n",
      "Epoch [57/100], Step [19/84], Loss: 0.1559\n",
      "Epoch [57/100], Step [20/84], Loss: 0.1591\n",
      "Epoch [57/100], Step [21/84], Loss: 0.1703\n",
      "Epoch [57/100], Step [22/84], Loss: 0.1569\n",
      "Epoch [57/100], Step [23/84], Loss: 0.1846\n",
      "Epoch [57/100], Step [24/84], Loss: 0.1658\n",
      "Epoch [57/100], Step [25/84], Loss: 0.1711\n",
      "Epoch [57/100], Step [26/84], Loss: 0.1638\n",
      "Epoch [57/100], Step [27/84], Loss: 0.1673\n",
      "Epoch [57/100], Step [28/84], Loss: 0.1587\n",
      "Epoch [57/100], Step [29/84], Loss: 0.1609\n",
      "Epoch [57/100], Step [30/84], Loss: 0.1890\n",
      "Epoch [57/100], Step [31/84], Loss: 0.1562\n",
      "Epoch [57/100], Step [32/84], Loss: 0.1710\n",
      "Epoch [57/100], Step [33/84], Loss: 0.1600\n",
      "Epoch [57/100], Step [34/84], Loss: 0.1589\n",
      "Epoch [57/100], Step [35/84], Loss: 0.1725\n",
      "Epoch [57/100], Step [36/84], Loss: 0.1831\n",
      "Epoch [57/100], Step [37/84], Loss: 0.1624\n",
      "Epoch [57/100], Step [38/84], Loss: 0.1706\n",
      "Epoch [57/100], Step [39/84], Loss: 0.1827\n",
      "Epoch [57/100], Step [40/84], Loss: 0.1823\n",
      "Epoch [57/100], Step [41/84], Loss: 0.1725\n",
      "Epoch [57/100], Step [42/84], Loss: 0.1718\n",
      "Epoch [57/100], Step [43/84], Loss: 0.1666\n",
      "Epoch [57/100], Step [44/84], Loss: 0.1655\n",
      "Epoch [57/100], Step [45/84], Loss: 0.1726\n",
      "Epoch [57/100], Step [46/84], Loss: 0.1753\n",
      "Epoch [57/100], Step [47/84], Loss: 0.1711\n",
      "Epoch [57/100], Step [48/84], Loss: 0.1526\n",
      "Epoch [57/100], Step [49/84], Loss: 0.1724\n",
      "Epoch [57/100], Step [50/84], Loss: 0.1609\n",
      "Epoch [57/100], Step [51/84], Loss: 0.1638\n",
      "Epoch [57/100], Step [52/84], Loss: 0.1717\n",
      "Epoch [57/100], Step [53/84], Loss: 0.1753\n",
      "Epoch [57/100], Step [54/84], Loss: 0.1965\n",
      "Epoch [57/100], Step [55/84], Loss: 0.1781\n",
      "Epoch [57/100], Step [56/84], Loss: 0.1624\n",
      "Epoch [57/100], Step [57/84], Loss: 0.1583\n",
      "Epoch [57/100], Step [58/84], Loss: 0.1765\n",
      "Epoch [57/100], Step [59/84], Loss: 0.1826\n",
      "Epoch [57/100], Step [60/84], Loss: 0.1557\n",
      "Epoch [57/100], Step [61/84], Loss: 0.1667\n",
      "Epoch [57/100], Step [62/84], Loss: 0.1843\n",
      "Epoch [57/100], Step [63/84], Loss: 0.1778\n",
      "Epoch [57/100], Step [64/84], Loss: 0.1637\n",
      "Epoch [57/100], Step [65/84], Loss: 0.1646\n",
      "Epoch [57/100], Step [66/84], Loss: 0.1670\n",
      "Epoch [57/100], Step [67/84], Loss: 0.1687\n",
      "Epoch [57/100], Step [68/84], Loss: 0.1841\n",
      "Epoch [57/100], Step [69/84], Loss: 0.1731\n",
      "Epoch [57/100], Step [70/84], Loss: 0.1728\n",
      "Epoch [57/100], Step [71/84], Loss: 0.1622\n",
      "Epoch [57/100], Step [72/84], Loss: 0.1761\n",
      "Epoch [57/100], Step [73/84], Loss: 0.1862\n",
      "Epoch [57/100], Step [74/84], Loss: 0.1634\n",
      "Epoch [57/100], Step [75/84], Loss: 0.1868\n",
      "Epoch [57/100], Step [76/84], Loss: 0.1651\n",
      "Epoch [57/100], Step [77/84], Loss: 0.1701\n",
      "Epoch [57/100], Step [78/84], Loss: 0.1734\n",
      "Epoch [57/100], Step [79/84], Loss: 0.1815\n",
      "Epoch [57/100], Step [80/84], Loss: 0.1606\n",
      "Epoch [57/100], Step [81/84], Loss: 0.1807\n",
      "Epoch [57/100], Step [82/84], Loss: 0.1748\n",
      "Epoch [57/100], Step [83/84], Loss: 0.1783\n",
      "Epoch [57/100], Step [84/84], Loss: 0.1869\n",
      "Epoch [58/100], Step [1/84], Loss: 0.1765\n",
      "Epoch [58/100], Step [2/84], Loss: 0.1732\n",
      "Epoch [58/100], Step [3/84], Loss: 0.1807\n",
      "Epoch [58/100], Step [4/84], Loss: 0.1649\n",
      "Epoch [58/100], Step [5/84], Loss: 0.1585\n",
      "Epoch [58/100], Step [6/84], Loss: 0.1677\n",
      "Epoch [58/100], Step [7/84], Loss: 0.1705\n",
      "Epoch [58/100], Step [8/84], Loss: 0.1700\n",
      "Epoch [58/100], Step [9/84], Loss: 0.1623\n",
      "Epoch [58/100], Step [10/84], Loss: 0.1608\n",
      "Epoch [58/100], Step [11/84], Loss: 0.1583\n",
      "Epoch [58/100], Step [12/84], Loss: 0.1684\n",
      "Epoch [58/100], Step [13/84], Loss: 0.1711\n",
      "Epoch [58/100], Step [14/84], Loss: 0.1603\n",
      "Epoch [58/100], Step [15/84], Loss: 0.1794\n",
      "Epoch [58/100], Step [16/84], Loss: 0.1840\n",
      "Epoch [58/100], Step [17/84], Loss: 0.1586\n",
      "Epoch [58/100], Step [18/84], Loss: 0.1570\n",
      "Epoch [58/100], Step [19/84], Loss: 0.1638\n",
      "Epoch [58/100], Step [20/84], Loss: 0.1606\n",
      "Epoch [58/100], Step [21/84], Loss: 0.1616\n",
      "Epoch [58/100], Step [22/84], Loss: 0.1947\n",
      "Epoch [58/100], Step [23/84], Loss: 0.1601\n",
      "Epoch [58/100], Step [24/84], Loss: 0.1832\n",
      "Epoch [58/100], Step [25/84], Loss: 0.1917\n",
      "Epoch [58/100], Step [26/84], Loss: 0.1655\n",
      "Epoch [58/100], Step [27/84], Loss: 0.1878\n",
      "Epoch [58/100], Step [28/84], Loss: 0.1883\n",
      "Epoch [58/100], Step [29/84], Loss: 0.1678\n",
      "Epoch [58/100], Step [30/84], Loss: 0.1612\n",
      "Epoch [58/100], Step [31/84], Loss: 0.1747\n",
      "Epoch [58/100], Step [32/84], Loss: 0.1714\n",
      "Epoch [58/100], Step [33/84], Loss: 0.1658\n",
      "Epoch [58/100], Step [34/84], Loss: 0.1645\n",
      "Epoch [58/100], Step [35/84], Loss: 0.1509\n",
      "Epoch [58/100], Step [36/84], Loss: 0.1806\n",
      "Epoch [58/100], Step [37/84], Loss: 0.1948\n",
      "Epoch [58/100], Step [38/84], Loss: 0.1791\n",
      "Epoch [58/100], Step [39/84], Loss: 0.1749\n",
      "Epoch [58/100], Step [40/84], Loss: 0.1532\n",
      "Epoch [58/100], Step [41/84], Loss: 0.1848\n",
      "Epoch [58/100], Step [42/84], Loss: 0.1614\n",
      "Epoch [58/100], Step [43/84], Loss: 0.1678\n",
      "Epoch [58/100], Step [44/84], Loss: 0.1668\n",
      "Epoch [58/100], Step [45/84], Loss: 0.1753\n",
      "Epoch [58/100], Step [46/84], Loss: 0.2059\n",
      "Epoch [58/100], Step [47/84], Loss: 0.1680\n",
      "Epoch [58/100], Step [48/84], Loss: 0.1652\n",
      "Epoch [58/100], Step [49/84], Loss: 0.1588\n",
      "Epoch [58/100], Step [50/84], Loss: 0.1820\n",
      "Epoch [58/100], Step [51/84], Loss: 0.1525\n",
      "Epoch [58/100], Step [52/84], Loss: 0.1621\n",
      "Epoch [58/100], Step [53/84], Loss: 0.1851\n",
      "Epoch [58/100], Step [54/84], Loss: 0.1524\n",
      "Epoch [58/100], Step [55/84], Loss: 0.1712\n",
      "Epoch [58/100], Step [56/84], Loss: 0.1581\n",
      "Epoch [58/100], Step [57/84], Loss: 0.1704\n",
      "Epoch [58/100], Step [58/84], Loss: 0.1657\n",
      "Epoch [58/100], Step [59/84], Loss: 0.1596\n",
      "Epoch [58/100], Step [60/84], Loss: 0.1561\n",
      "Epoch [58/100], Step [61/84], Loss: 0.1708\n",
      "Epoch [58/100], Step [62/84], Loss: 0.1774\n",
      "Epoch [58/100], Step [63/84], Loss: 0.1715\n",
      "Epoch [58/100], Step [64/84], Loss: 0.1655\n",
      "Epoch [58/100], Step [65/84], Loss: 0.1649\n",
      "Epoch [58/100], Step [66/84], Loss: 0.1602\n",
      "Epoch [58/100], Step [67/84], Loss: 0.1584\n",
      "Epoch [58/100], Step [68/84], Loss: 0.1731\n",
      "Epoch [58/100], Step [69/84], Loss: 0.1686\n",
      "Epoch [58/100], Step [70/84], Loss: 0.1671\n",
      "Epoch [58/100], Step [71/84], Loss: 0.1863\n",
      "Epoch [58/100], Step [72/84], Loss: 0.1642\n",
      "Epoch [58/100], Step [73/84], Loss: 0.1572\n",
      "Epoch [58/100], Step [74/84], Loss: 0.1726\n",
      "Epoch [58/100], Step [75/84], Loss: 0.1661\n",
      "Epoch [58/100], Step [76/84], Loss: 0.1690\n",
      "Epoch [58/100], Step [77/84], Loss: 0.1714\n",
      "Epoch [58/100], Step [78/84], Loss: 0.1632\n",
      "Epoch [58/100], Step [79/84], Loss: 0.1632\n",
      "Epoch [58/100], Step [80/84], Loss: 0.1858\n",
      "Epoch [58/100], Step [81/84], Loss: 0.1756\n",
      "Epoch [58/100], Step [82/84], Loss: 0.1979\n",
      "Epoch [58/100], Step [83/84], Loss: 0.1647\n",
      "Epoch [58/100], Step [84/84], Loss: 0.1886\n",
      "Epoch [59/100], Step [1/84], Loss: 0.1668\n",
      "Epoch [59/100], Step [2/84], Loss: 0.1577\n",
      "Epoch [59/100], Step [3/84], Loss: 0.1637\n",
      "Epoch [59/100], Step [4/84], Loss: 0.1748\n",
      "Epoch [59/100], Step [5/84], Loss: 0.1627\n",
      "Epoch [59/100], Step [6/84], Loss: 0.1891\n",
      "Epoch [59/100], Step [7/84], Loss: 0.1541\n",
      "Epoch [59/100], Step [8/84], Loss: 0.1571\n",
      "Epoch [59/100], Step [9/84], Loss: 0.1541\n",
      "Epoch [59/100], Step [10/84], Loss: 0.1741\n",
      "Epoch [59/100], Step [11/84], Loss: 0.1609\n",
      "Epoch [59/100], Step [12/84], Loss: 0.1582\n",
      "Epoch [59/100], Step [13/84], Loss: 0.1676\n",
      "Epoch [59/100], Step [14/84], Loss: 0.1580\n",
      "Epoch [59/100], Step [15/84], Loss: 0.1945\n",
      "Epoch [59/100], Step [16/84], Loss: 0.1574\n",
      "Epoch [59/100], Step [17/84], Loss: 0.1743\n",
      "Epoch [59/100], Step [18/84], Loss: 0.1630\n",
      "Epoch [59/100], Step [19/84], Loss: 0.1748\n",
      "Epoch [59/100], Step [20/84], Loss: 0.1906\n",
      "Epoch [59/100], Step [21/84], Loss: 0.1676\n",
      "Epoch [59/100], Step [22/84], Loss: 0.1541\n",
      "Epoch [59/100], Step [23/84], Loss: 0.1544\n",
      "Epoch [59/100], Step [24/84], Loss: 0.1771\n",
      "Epoch [59/100], Step [25/84], Loss: 0.1720\n",
      "Epoch [59/100], Step [26/84], Loss: 0.1731\n",
      "Epoch [59/100], Step [27/84], Loss: 0.1681\n",
      "Epoch [59/100], Step [28/84], Loss: 0.1567\n",
      "Epoch [59/100], Step [29/84], Loss: 0.1581\n",
      "Epoch [59/100], Step [30/84], Loss: 0.1998\n",
      "Epoch [59/100], Step [31/84], Loss: 0.1676\n",
      "Epoch [59/100], Step [32/84], Loss: 0.1676\n",
      "Epoch [59/100], Step [33/84], Loss: 0.1557\n",
      "Epoch [59/100], Step [34/84], Loss: 0.1745\n",
      "Epoch [59/100], Step [35/84], Loss: 0.1866\n",
      "Epoch [59/100], Step [36/84], Loss: 0.1663\n",
      "Epoch [59/100], Step [37/84], Loss: 0.1692\n",
      "Epoch [59/100], Step [38/84], Loss: 0.1452\n",
      "Epoch [59/100], Step [39/84], Loss: 0.1772\n",
      "Epoch [59/100], Step [40/84], Loss: 0.1690\n",
      "Epoch [59/100], Step [41/84], Loss: 0.1798\n",
      "Epoch [59/100], Step [42/84], Loss: 0.1543\n",
      "Epoch [59/100], Step [43/84], Loss: 0.1659\n",
      "Epoch [59/100], Step [44/84], Loss: 0.1762\n",
      "Epoch [59/100], Step [45/84], Loss: 0.1577\n",
      "Epoch [59/100], Step [46/84], Loss: 0.1563\n",
      "Epoch [59/100], Step [47/84], Loss: 0.1675\n",
      "Epoch [59/100], Step [48/84], Loss: 0.1551\n",
      "Epoch [59/100], Step [49/84], Loss: 0.1750\n",
      "Epoch [59/100], Step [50/84], Loss: 0.1616\n",
      "Epoch [59/100], Step [51/84], Loss: 0.1593\n",
      "Epoch [59/100], Step [52/84], Loss: 0.1648\n",
      "Epoch [59/100], Step [53/84], Loss: 0.1668\n",
      "Epoch [59/100], Step [54/84], Loss: 0.1670\n",
      "Epoch [59/100], Step [55/84], Loss: 0.1891\n",
      "Epoch [59/100], Step [56/84], Loss: 0.1591\n",
      "Epoch [59/100], Step [57/84], Loss: 0.1528\n",
      "Epoch [59/100], Step [58/84], Loss: 0.1668\n",
      "Epoch [59/100], Step [59/84], Loss: 0.1779\n",
      "Epoch [59/100], Step [60/84], Loss: 0.1533\n",
      "Epoch [59/100], Step [61/84], Loss: 0.1637\n",
      "Epoch [59/100], Step [62/84], Loss: 0.1641\n",
      "Epoch [59/100], Step [63/84], Loss: 0.1812\n",
      "Epoch [59/100], Step [64/84], Loss: 0.1745\n",
      "Epoch [59/100], Step [65/84], Loss: 0.1622\n",
      "Epoch [59/100], Step [66/84], Loss: 0.1614\n",
      "Epoch [59/100], Step [67/84], Loss: 0.1616\n",
      "Epoch [59/100], Step [68/84], Loss: 0.1799\n",
      "Epoch [59/100], Step [69/84], Loss: 0.1623\n",
      "Epoch [59/100], Step [70/84], Loss: 0.1847\n",
      "Epoch [59/100], Step [71/84], Loss: 0.1796\n",
      "Epoch [59/100], Step [72/84], Loss: 0.1673\n",
      "Epoch [59/100], Step [73/84], Loss: 0.1540\n",
      "Epoch [59/100], Step [74/84], Loss: 0.1516\n",
      "Epoch [59/100], Step [75/84], Loss: 0.1903\n",
      "Epoch [59/100], Step [76/84], Loss: 0.1573\n",
      "Epoch [59/100], Step [77/84], Loss: 0.1653\n",
      "Epoch [59/100], Step [78/84], Loss: 0.1757\n",
      "Epoch [59/100], Step [79/84], Loss: 0.1568\n",
      "Epoch [59/100], Step [80/84], Loss: 0.1674\n",
      "Epoch [59/100], Step [81/84], Loss: 0.2005\n",
      "Epoch [59/100], Step [82/84], Loss: 0.1612\n",
      "Epoch [59/100], Step [83/84], Loss: 0.1973\n",
      "Epoch [59/100], Step [84/84], Loss: 0.1901\n",
      "Epoch [60/100], Step [1/84], Loss: 0.1541\n",
      "Epoch [60/100], Step [2/84], Loss: 0.1561\n",
      "Epoch [60/100], Step [3/84], Loss: 0.1777\n",
      "Epoch [60/100], Step [4/84], Loss: 0.1616\n",
      "Epoch [60/100], Step [5/84], Loss: 0.1639\n",
      "Epoch [60/100], Step [6/84], Loss: 0.1676\n",
      "Epoch [60/100], Step [7/84], Loss: 0.1773\n",
      "Epoch [60/100], Step [8/84], Loss: 0.1790\n",
      "Epoch [60/100], Step [9/84], Loss: 0.1654\n",
      "Epoch [60/100], Step [10/84], Loss: 0.1594\n",
      "Epoch [60/100], Step [11/84], Loss: 0.1680\n",
      "Epoch [60/100], Step [12/84], Loss: 0.1729\n",
      "Epoch [60/100], Step [13/84], Loss: 0.1554\n",
      "Epoch [60/100], Step [14/84], Loss: 0.1711\n",
      "Epoch [60/100], Step [15/84], Loss: 0.1604\n",
      "Epoch [60/100], Step [16/84], Loss: 0.1592\n",
      "Epoch [60/100], Step [17/84], Loss: 0.1696\n",
      "Epoch [60/100], Step [18/84], Loss: 0.1528\n",
      "Epoch [60/100], Step [19/84], Loss: 0.1836\n",
      "Epoch [60/100], Step [20/84], Loss: 0.1563\n",
      "Epoch [60/100], Step [21/84], Loss: 0.1422\n",
      "Epoch [60/100], Step [22/84], Loss: 0.1553\n",
      "Epoch [60/100], Step [23/84], Loss: 0.1816\n",
      "Epoch [60/100], Step [24/84], Loss: 0.1545\n",
      "Epoch [60/100], Step [25/84], Loss: 0.1661\n",
      "Epoch [60/100], Step [26/84], Loss: 0.1804\n",
      "Epoch [60/100], Step [27/84], Loss: 0.1682\n",
      "Epoch [60/100], Step [28/84], Loss: 0.1794\n",
      "Epoch [60/100], Step [29/84], Loss: 0.1620\n",
      "Epoch [60/100], Step [30/84], Loss: 0.1669\n",
      "Epoch [60/100], Step [31/84], Loss: 0.1772\n",
      "Epoch [60/100], Step [32/84], Loss: 0.1731\n",
      "Epoch [60/100], Step [33/84], Loss: 0.1665\n",
      "Epoch [60/100], Step [34/84], Loss: 0.1565\n",
      "Epoch [60/100], Step [35/84], Loss: 0.1707\n",
      "Epoch [60/100], Step [36/84], Loss: 0.1702\n",
      "Epoch [60/100], Step [37/84], Loss: 0.1553\n",
      "Epoch [60/100], Step [38/84], Loss: 0.1735\n",
      "Epoch [60/100], Step [39/84], Loss: 0.1588\n",
      "Epoch [60/100], Step [40/84], Loss: 0.1635\n",
      "Epoch [60/100], Step [41/84], Loss: 0.1637\n",
      "Epoch [60/100], Step [42/84], Loss: 0.1776\n",
      "Epoch [60/100], Step [43/84], Loss: 0.1763\n",
      "Epoch [60/100], Step [44/84], Loss: 0.1608\n",
      "Epoch [60/100], Step [45/84], Loss: 0.1576\n",
      "Epoch [60/100], Step [46/84], Loss: 0.1767\n",
      "Epoch [60/100], Step [47/84], Loss: 0.1538\n",
      "Epoch [60/100], Step [48/84], Loss: 0.1637\n",
      "Epoch [60/100], Step [49/84], Loss: 0.1780\n",
      "Epoch [60/100], Step [50/84], Loss: 0.1681\n",
      "Epoch [60/100], Step [51/84], Loss: 0.1629\n",
      "Epoch [60/100], Step [52/84], Loss: 0.1954\n",
      "Epoch [60/100], Step [53/84], Loss: 0.1871\n",
      "Epoch [60/100], Step [54/84], Loss: 0.1936\n",
      "Epoch [60/100], Step [55/84], Loss: 0.1760\n",
      "Epoch [60/100], Step [56/84], Loss: 0.1745\n",
      "Epoch [60/100], Step [57/84], Loss: 0.1432\n",
      "Epoch [60/100], Step [58/84], Loss: 0.1585\n",
      "Epoch [60/100], Step [59/84], Loss: 0.1548\n",
      "Epoch [60/100], Step [60/84], Loss: 0.1691\n",
      "Epoch [60/100], Step [61/84], Loss: 0.1624\n",
      "Epoch [60/100], Step [62/84], Loss: 0.1556\n",
      "Epoch [60/100], Step [63/84], Loss: 0.1613\n",
      "Epoch [60/100], Step [64/84], Loss: 0.1942\n",
      "Epoch [60/100], Step [65/84], Loss: 0.1810\n",
      "Epoch [60/100], Step [66/84], Loss: 0.1749\n",
      "Epoch [60/100], Step [67/84], Loss: 0.1655\n",
      "Epoch [60/100], Step [68/84], Loss: 0.1697\n",
      "Epoch [60/100], Step [69/84], Loss: 0.1665\n",
      "Epoch [60/100], Step [70/84], Loss: 0.1712\n",
      "Epoch [60/100], Step [71/84], Loss: 0.1564\n",
      "Epoch [60/100], Step [72/84], Loss: 0.1808\n",
      "Epoch [60/100], Step [73/84], Loss: 0.1507\n",
      "Epoch [60/100], Step [74/84], Loss: 0.1665\n",
      "Epoch [60/100], Step [75/84], Loss: 0.1606\n",
      "Epoch [60/100], Step [76/84], Loss: 0.1558\n",
      "Epoch [60/100], Step [77/84], Loss: 0.1685\n",
      "Epoch [60/100], Step [78/84], Loss: 0.1593\n",
      "Epoch [60/100], Step [79/84], Loss: 0.1521\n",
      "Epoch [60/100], Step [80/84], Loss: 0.1645\n",
      "Epoch [60/100], Step [81/84], Loss: 0.1732\n",
      "Epoch [60/100], Step [82/84], Loss: 0.1806\n",
      "Epoch [60/100], Step [83/84], Loss: 0.1753\n",
      "Epoch [60/100], Step [84/84], Loss: 0.2357\n",
      "Epoch [61/100], Step [1/84], Loss: 0.1522\n",
      "Epoch [61/100], Step [2/84], Loss: 0.1519\n",
      "Epoch [61/100], Step [3/84], Loss: 0.1681\n",
      "Epoch [61/100], Step [4/84], Loss: 0.1704\n",
      "Epoch [61/100], Step [5/84], Loss: 0.1649\n",
      "Epoch [61/100], Step [6/84], Loss: 0.1896\n",
      "Epoch [61/100], Step [7/84], Loss: 0.1660\n",
      "Epoch [61/100], Step [8/84], Loss: 0.1566\n",
      "Epoch [61/100], Step [9/84], Loss: 0.1523\n",
      "Epoch [61/100], Step [10/84], Loss: 0.1602\n",
      "Epoch [61/100], Step [11/84], Loss: 0.1604\n",
      "Epoch [61/100], Step [12/84], Loss: 0.1592\n",
      "Epoch [61/100], Step [13/84], Loss: 0.1639\n",
      "Epoch [61/100], Step [14/84], Loss: 0.1655\n",
      "Epoch [61/100], Step [15/84], Loss: 0.1708\n",
      "Epoch [61/100], Step [16/84], Loss: 0.1653\n",
      "Epoch [61/100], Step [17/84], Loss: 0.1501\n",
      "Epoch [61/100], Step [18/84], Loss: 0.1450\n",
      "Epoch [61/100], Step [19/84], Loss: 0.1805\n",
      "Epoch [61/100], Step [20/84], Loss: 0.1654\n",
      "Epoch [61/100], Step [21/84], Loss: 0.1705\n",
      "Epoch [61/100], Step [22/84], Loss: 0.1590\n",
      "Epoch [61/100], Step [23/84], Loss: 0.1486\n",
      "Epoch [61/100], Step [24/84], Loss: 0.1665\n",
      "Epoch [61/100], Step [25/84], Loss: 0.1527\n",
      "Epoch [61/100], Step [26/84], Loss: 0.1803\n",
      "Epoch [61/100], Step [27/84], Loss: 0.1542\n",
      "Epoch [61/100], Step [28/84], Loss: 0.1467\n",
      "Epoch [61/100], Step [29/84], Loss: 0.1615\n",
      "Epoch [61/100], Step [30/84], Loss: 0.1619\n",
      "Epoch [61/100], Step [31/84], Loss: 0.1956\n",
      "Epoch [61/100], Step [32/84], Loss: 0.1585\n",
      "Epoch [61/100], Step [33/84], Loss: 0.1695\n",
      "Epoch [61/100], Step [34/84], Loss: 0.1776\n",
      "Epoch [61/100], Step [35/84], Loss: 0.1801\n",
      "Epoch [61/100], Step [36/84], Loss: 0.1852\n",
      "Epoch [61/100], Step [37/84], Loss: 0.2011\n",
      "Epoch [61/100], Step [38/84], Loss: 0.1621\n",
      "Epoch [61/100], Step [39/84], Loss: 0.1464\n",
      "Epoch [61/100], Step [40/84], Loss: 0.1612\n",
      "Epoch [61/100], Step [41/84], Loss: 0.1825\n",
      "Epoch [61/100], Step [42/84], Loss: 0.1761\n",
      "Epoch [61/100], Step [43/84], Loss: 0.1564\n",
      "Epoch [61/100], Step [44/84], Loss: 0.1569\n",
      "Epoch [61/100], Step [45/84], Loss: 0.1624\n",
      "Epoch [61/100], Step [46/84], Loss: 0.1830\n",
      "Epoch [61/100], Step [47/84], Loss: 0.1558\n",
      "Epoch [61/100], Step [48/84], Loss: 0.1534\n",
      "Epoch [61/100], Step [49/84], Loss: 0.1646\n",
      "Epoch [61/100], Step [50/84], Loss: 0.1737\n",
      "Epoch [61/100], Step [51/84], Loss: 0.1604\n",
      "Epoch [61/100], Step [52/84], Loss: 0.1464\n",
      "Epoch [61/100], Step [53/84], Loss: 0.1573\n",
      "Epoch [61/100], Step [54/84], Loss: 0.1631\n",
      "Epoch [61/100], Step [55/84], Loss: 0.1757\n",
      "Epoch [61/100], Step [56/84], Loss: 0.1693\n",
      "Epoch [61/100], Step [57/84], Loss: 0.1616\n",
      "Epoch [61/100], Step [58/84], Loss: 0.1548\n",
      "Epoch [61/100], Step [59/84], Loss: 0.1670\n",
      "Epoch [61/100], Step [60/84], Loss: 0.1665\n",
      "Epoch [61/100], Step [61/84], Loss: 0.1646\n",
      "Epoch [61/100], Step [62/84], Loss: 0.1564\n",
      "Epoch [61/100], Step [63/84], Loss: 0.1701\n",
      "Epoch [61/100], Step [64/84], Loss: 0.1624\n",
      "Epoch [61/100], Step [65/84], Loss: 0.1950\n",
      "Epoch [61/100], Step [66/84], Loss: 0.1582\n",
      "Epoch [61/100], Step [67/84], Loss: 0.1700\n",
      "Epoch [61/100], Step [68/84], Loss: 0.1624\n",
      "Epoch [61/100], Step [69/84], Loss: 0.1605\n",
      "Epoch [61/100], Step [70/84], Loss: 0.1538\n",
      "Epoch [61/100], Step [71/84], Loss: 0.1792\n",
      "Epoch [61/100], Step [72/84], Loss: 0.1592\n",
      "Epoch [61/100], Step [73/84], Loss: 0.1637\n",
      "Epoch [61/100], Step [74/84], Loss: 0.1784\n",
      "Epoch [61/100], Step [75/84], Loss: 0.1674\n",
      "Epoch [61/100], Step [76/84], Loss: 0.1575\n",
      "Epoch [61/100], Step [77/84], Loss: 0.1742\n",
      "Epoch [61/100], Step [78/84], Loss: 0.1620\n",
      "Epoch [61/100], Step [79/84], Loss: 0.1510\n",
      "Epoch [61/100], Step [80/84], Loss: 0.1601\n",
      "Epoch [61/100], Step [81/84], Loss: 0.1729\n",
      "Epoch [61/100], Step [82/84], Loss: 0.1779\n",
      "Epoch [61/100], Step [83/84], Loss: 0.1557\n",
      "Epoch [61/100], Step [84/84], Loss: 0.2494\n",
      "Epoch [62/100], Step [1/84], Loss: 0.2008\n",
      "Epoch [62/100], Step [2/84], Loss: 0.1843\n",
      "Epoch [62/100], Step [3/84], Loss: 0.1612\n",
      "Epoch [62/100], Step [4/84], Loss: 0.1564\n",
      "Epoch [62/100], Step [5/84], Loss: 0.1617\n",
      "Epoch [62/100], Step [6/84], Loss: 0.1735\n",
      "Epoch [62/100], Step [7/84], Loss: 0.1791\n",
      "Epoch [62/100], Step [8/84], Loss: 0.1736\n",
      "Epoch [62/100], Step [9/84], Loss: 0.1480\n",
      "Epoch [62/100], Step [10/84], Loss: 0.1694\n",
      "Epoch [62/100], Step [11/84], Loss: 0.1684\n",
      "Epoch [62/100], Step [12/84], Loss: 0.1572\n",
      "Epoch [62/100], Step [13/84], Loss: 0.1545\n",
      "Epoch [62/100], Step [14/84], Loss: 0.1581\n",
      "Epoch [62/100], Step [15/84], Loss: 0.1673\n",
      "Epoch [62/100], Step [16/84], Loss: 0.1682\n",
      "Epoch [62/100], Step [17/84], Loss: 0.1798\n",
      "Epoch [62/100], Step [18/84], Loss: 0.1836\n",
      "Epoch [62/100], Step [19/84], Loss: 0.1776\n",
      "Epoch [62/100], Step [20/84], Loss: 0.1630\n",
      "Epoch [62/100], Step [21/84], Loss: 0.1529\n",
      "Epoch [62/100], Step [22/84], Loss: 0.1555\n",
      "Epoch [62/100], Step [23/84], Loss: 0.1510\n",
      "Epoch [62/100], Step [24/84], Loss: 0.1618\n",
      "Epoch [62/100], Step [25/84], Loss: 0.1499\n",
      "Epoch [62/100], Step [26/84], Loss: 0.1520\n",
      "Epoch [62/100], Step [27/84], Loss: 0.1724\n",
      "Epoch [62/100], Step [28/84], Loss: 0.1585\n",
      "Epoch [62/100], Step [29/84], Loss: 0.1632\n",
      "Epoch [62/100], Step [30/84], Loss: 0.1798\n",
      "Epoch [62/100], Step [31/84], Loss: 0.2229\n",
      "Epoch [62/100], Step [32/84], Loss: 0.1610\n",
      "Epoch [62/100], Step [33/84], Loss: 0.1590\n",
      "Epoch [62/100], Step [34/84], Loss: 0.1537\n",
      "Epoch [62/100], Step [35/84], Loss: 0.1730\n",
      "Epoch [62/100], Step [36/84], Loss: 0.1723\n",
      "Epoch [62/100], Step [37/84], Loss: 0.1545\n",
      "Epoch [62/100], Step [38/84], Loss: 0.1769\n",
      "Epoch [62/100], Step [39/84], Loss: 0.1821\n",
      "Epoch [62/100], Step [40/84], Loss: 0.1809\n",
      "Epoch [62/100], Step [41/84], Loss: 0.1647\n",
      "Epoch [62/100], Step [42/84], Loss: 0.1648\n",
      "Epoch [62/100], Step [43/84], Loss: 0.1582\n",
      "Epoch [62/100], Step [44/84], Loss: 0.1842\n",
      "Epoch [62/100], Step [45/84], Loss: 0.1654\n",
      "Epoch [62/100], Step [46/84], Loss: 0.1602\n",
      "Epoch [62/100], Step [47/84], Loss: 0.1603\n",
      "Epoch [62/100], Step [48/84], Loss: 0.1506\n",
      "Epoch [62/100], Step [49/84], Loss: 0.1684\n",
      "Epoch [62/100], Step [50/84], Loss: 0.1780\n",
      "Epoch [62/100], Step [51/84], Loss: 0.1776\n",
      "Epoch [62/100], Step [52/84], Loss: 0.1706\n",
      "Epoch [62/100], Step [53/84], Loss: 0.1582\n",
      "Epoch [62/100], Step [54/84], Loss: 0.1719\n",
      "Epoch [62/100], Step [55/84], Loss: 0.1523\n",
      "Epoch [62/100], Step [56/84], Loss: 0.1552\n",
      "Epoch [62/100], Step [57/84], Loss: 0.1642\n",
      "Epoch [62/100], Step [58/84], Loss: 0.1643\n",
      "Epoch [62/100], Step [59/84], Loss: 0.1626\n",
      "Epoch [62/100], Step [60/84], Loss: 0.1489\n",
      "Epoch [62/100], Step [61/84], Loss: 0.1600\n",
      "Epoch [62/100], Step [62/84], Loss: 0.1734\n",
      "Epoch [62/100], Step [63/84], Loss: 0.1584\n",
      "Epoch [62/100], Step [64/84], Loss: 0.1588\n",
      "Epoch [62/100], Step [65/84], Loss: 0.1632\n",
      "Epoch [62/100], Step [66/84], Loss: 0.1555\n",
      "Epoch [62/100], Step [67/84], Loss: 0.1583\n",
      "Epoch [62/100], Step [68/84], Loss: 0.1875\n",
      "Epoch [62/100], Step [69/84], Loss: 0.1535\n",
      "Epoch [62/100], Step [70/84], Loss: 0.1813\n",
      "Epoch [62/100], Step [71/84], Loss: 0.1524\n",
      "Epoch [62/100], Step [72/84], Loss: 0.1933\n",
      "Epoch [62/100], Step [73/84], Loss: 0.1756\n",
      "Epoch [62/100], Step [74/84], Loss: 0.1749\n",
      "Epoch [62/100], Step [75/84], Loss: 0.1602\n",
      "Epoch [62/100], Step [76/84], Loss: 0.1502\n",
      "Epoch [62/100], Step [77/84], Loss: 0.1587\n",
      "Epoch [62/100], Step [78/84], Loss: 0.1626\n",
      "Epoch [62/100], Step [79/84], Loss: 0.1502\n",
      "Epoch [62/100], Step [80/84], Loss: 0.1671\n",
      "Epoch [62/100], Step [81/84], Loss: 0.1565\n",
      "Epoch [62/100], Step [82/84], Loss: 0.1558\n",
      "Epoch [62/100], Step [83/84], Loss: 0.1418\n",
      "Epoch [62/100], Step [84/84], Loss: 0.1715\n",
      "Epoch [63/100], Step [1/84], Loss: 0.1529\n",
      "Epoch [63/100], Step [2/84], Loss: 0.1719\n",
      "Epoch [63/100], Step [3/84], Loss: 0.1564\n",
      "Epoch [63/100], Step [4/84], Loss: 0.1598\n",
      "Epoch [63/100], Step [5/84], Loss: 0.1584\n",
      "Epoch [63/100], Step [6/84], Loss: 0.1624\n",
      "Epoch [63/100], Step [7/84], Loss: 0.1791\n",
      "Epoch [63/100], Step [8/84], Loss: 0.1646\n",
      "Epoch [63/100], Step [9/84], Loss: 0.1785\n",
      "Epoch [63/100], Step [10/84], Loss: 0.1485\n",
      "Epoch [63/100], Step [11/84], Loss: 0.1571\n",
      "Epoch [63/100], Step [12/84], Loss: 0.1778\n",
      "Epoch [63/100], Step [13/84], Loss: 0.1445\n",
      "Epoch [63/100], Step [14/84], Loss: 0.1598\n",
      "Epoch [63/100], Step [15/84], Loss: 0.1798\n",
      "Epoch [63/100], Step [16/84], Loss: 0.1582\n",
      "Epoch [63/100], Step [17/84], Loss: 0.1872\n",
      "Epoch [63/100], Step [18/84], Loss: 0.1633\n",
      "Epoch [63/100], Step [19/84], Loss: 0.1740\n",
      "Epoch [63/100], Step [20/84], Loss: 0.1797\n",
      "Epoch [63/100], Step [21/84], Loss: 0.1682\n",
      "Epoch [63/100], Step [22/84], Loss: 0.1536\n",
      "Epoch [63/100], Step [23/84], Loss: 0.1555\n",
      "Epoch [63/100], Step [24/84], Loss: 0.1614\n",
      "Epoch [63/100], Step [25/84], Loss: 0.1571\n",
      "Epoch [63/100], Step [26/84], Loss: 0.1843\n",
      "Epoch [63/100], Step [27/84], Loss: 0.1588\n",
      "Epoch [63/100], Step [28/84], Loss: 0.1666\n",
      "Epoch [63/100], Step [29/84], Loss: 0.1594\n",
      "Epoch [63/100], Step [30/84], Loss: 0.1644\n",
      "Epoch [63/100], Step [31/84], Loss: 0.1531\n",
      "Epoch [63/100], Step [32/84], Loss: 0.1962\n",
      "Epoch [63/100], Step [33/84], Loss: 0.1549\n",
      "Epoch [63/100], Step [34/84], Loss: 0.1705\n",
      "Epoch [63/100], Step [35/84], Loss: 0.1558\n",
      "Epoch [63/100], Step [36/84], Loss: 0.1603\n",
      "Epoch [63/100], Step [37/84], Loss: 0.1680\n",
      "Epoch [63/100], Step [38/84], Loss: 0.1715\n",
      "Epoch [63/100], Step [39/84], Loss: 0.1775\n",
      "Epoch [63/100], Step [40/84], Loss: 0.1606\n",
      "Epoch [63/100], Step [41/84], Loss: 0.1646\n",
      "Epoch [63/100], Step [42/84], Loss: 0.1735\n",
      "Epoch [63/100], Step [43/84], Loss: 0.1605\n",
      "Epoch [63/100], Step [44/84], Loss: 0.1503\n",
      "Epoch [63/100], Step [45/84], Loss: 0.1519\n",
      "Epoch [63/100], Step [46/84], Loss: 0.1844\n",
      "Epoch [63/100], Step [47/84], Loss: 0.1544\n",
      "Epoch [63/100], Step [48/84], Loss: 0.1626\n",
      "Epoch [63/100], Step [49/84], Loss: 0.1657\n",
      "Epoch [63/100], Step [50/84], Loss: 0.1509\n",
      "Epoch [63/100], Step [51/84], Loss: 0.1760\n",
      "Epoch [63/100], Step [52/84], Loss: 0.1647\n",
      "Epoch [63/100], Step [53/84], Loss: 0.1752\n",
      "Epoch [63/100], Step [54/84], Loss: 0.1569\n",
      "Epoch [63/100], Step [55/84], Loss: 0.1550\n",
      "Epoch [63/100], Step [56/84], Loss: 0.1423\n",
      "Epoch [63/100], Step [57/84], Loss: 0.1559\n",
      "Epoch [63/100], Step [58/84], Loss: 0.1679\n",
      "Epoch [63/100], Step [59/84], Loss: 0.1597\n",
      "Epoch [63/100], Step [60/84], Loss: 0.1669\n",
      "Epoch [63/100], Step [61/84], Loss: 0.1514\n",
      "Epoch [63/100], Step [62/84], Loss: 0.1644\n",
      "Epoch [63/100], Step [63/84], Loss: 0.1618\n",
      "Epoch [63/100], Step [64/84], Loss: 0.1783\n",
      "Epoch [63/100], Step [65/84], Loss: 0.1808\n",
      "Epoch [63/100], Step [66/84], Loss: 0.1571\n",
      "Epoch [63/100], Step [67/84], Loss: 0.1473\n",
      "Epoch [63/100], Step [68/84], Loss: 0.1711\n",
      "Epoch [63/100], Step [69/84], Loss: 0.1692\n",
      "Epoch [63/100], Step [70/84], Loss: 0.1605\n",
      "Epoch [63/100], Step [71/84], Loss: 0.1591\n",
      "Epoch [63/100], Step [72/84], Loss: 0.1488\n",
      "Epoch [63/100], Step [73/84], Loss: 0.1694\n",
      "Epoch [63/100], Step [74/84], Loss: 0.1624\n",
      "Epoch [63/100], Step [75/84], Loss: 0.1588\n",
      "Epoch [63/100], Step [76/84], Loss: 0.1769\n",
      "Epoch [63/100], Step [77/84], Loss: 0.1724\n",
      "Epoch [63/100], Step [78/84], Loss: 0.1663\n",
      "Epoch [63/100], Step [79/84], Loss: 0.1485\n",
      "Epoch [63/100], Step [80/84], Loss: 0.1589\n",
      "Epoch [63/100], Step [81/84], Loss: 0.1618\n",
      "Epoch [63/100], Step [82/84], Loss: 0.1707\n",
      "Epoch [63/100], Step [83/84], Loss: 0.1471\n",
      "Epoch [63/100], Step [84/84], Loss: 0.1726\n",
      "Epoch [64/100], Step [1/84], Loss: 0.1460\n",
      "Epoch [64/100], Step [2/84], Loss: 0.1639\n",
      "Epoch [64/100], Step [3/84], Loss: 0.1530\n",
      "Epoch [64/100], Step [4/84], Loss: 0.1650\n",
      "Epoch [64/100], Step [5/84], Loss: 0.1712\n",
      "Epoch [64/100], Step [6/84], Loss: 0.1696\n",
      "Epoch [64/100], Step [7/84], Loss: 0.1653\n",
      "Epoch [64/100], Step [8/84], Loss: 0.1804\n",
      "Epoch [64/100], Step [9/84], Loss: 0.1696\n",
      "Epoch [64/100], Step [10/84], Loss: 0.1615\n",
      "Epoch [64/100], Step [11/84], Loss: 0.1617\n",
      "Epoch [64/100], Step [12/84], Loss: 0.1798\n",
      "Epoch [64/100], Step [13/84], Loss: 0.1612\n",
      "Epoch [64/100], Step [14/84], Loss: 0.1730\n",
      "Epoch [64/100], Step [15/84], Loss: 0.1776\n",
      "Epoch [64/100], Step [16/84], Loss: 0.1478\n",
      "Epoch [64/100], Step [17/84], Loss: 0.1667\n",
      "Epoch [64/100], Step [18/84], Loss: 0.1422\n",
      "Epoch [64/100], Step [19/84], Loss: 0.1541\n",
      "Epoch [64/100], Step [20/84], Loss: 0.1685\n",
      "Epoch [64/100], Step [21/84], Loss: 0.1662\n",
      "Epoch [64/100], Step [22/84], Loss: 0.1520\n",
      "Epoch [64/100], Step [23/84], Loss: 0.1478\n",
      "Epoch [64/100], Step [24/84], Loss: 0.1480\n",
      "Epoch [64/100], Step [25/84], Loss: 0.1697\n",
      "Epoch [64/100], Step [26/84], Loss: 0.1819\n",
      "Epoch [64/100], Step [27/84], Loss: 0.1598\n",
      "Epoch [64/100], Step [28/84], Loss: 0.1612\n",
      "Epoch [64/100], Step [29/84], Loss: 0.1524\n",
      "Epoch [64/100], Step [30/84], Loss: 0.1687\n",
      "Epoch [64/100], Step [31/84], Loss: 0.1779\n",
      "Epoch [64/100], Step [32/84], Loss: 0.1686\n",
      "Epoch [64/100], Step [33/84], Loss: 0.1651\n",
      "Epoch [64/100], Step [34/84], Loss: 0.1625\n",
      "Epoch [64/100], Step [35/84], Loss: 0.1727\n",
      "Epoch [64/100], Step [36/84], Loss: 0.1726\n",
      "Epoch [64/100], Step [37/84], Loss: 0.1699\n",
      "Epoch [64/100], Step [38/84], Loss: 0.1951\n",
      "Epoch [64/100], Step [39/84], Loss: 0.1653\n",
      "Epoch [64/100], Step [40/84], Loss: 0.1583\n",
      "Epoch [64/100], Step [41/84], Loss: 0.1563\n",
      "Epoch [64/100], Step [42/84], Loss: 0.1571\n",
      "Epoch [64/100], Step [43/84], Loss: 0.1640\n",
      "Epoch [64/100], Step [44/84], Loss: 0.1475\n",
      "Epoch [64/100], Step [45/84], Loss: 0.1592\n",
      "Epoch [64/100], Step [46/84], Loss: 0.1896\n",
      "Epoch [64/100], Step [47/84], Loss: 0.1555\n",
      "Epoch [64/100], Step [48/84], Loss: 0.1597\n",
      "Epoch [64/100], Step [49/84], Loss: 0.1892\n",
      "Epoch [64/100], Step [50/84], Loss: 0.1816\n",
      "Epoch [64/100], Step [51/84], Loss: 0.1508\n",
      "Epoch [64/100], Step [52/84], Loss: 0.1722\n",
      "Epoch [64/100], Step [53/84], Loss: 0.1575\n",
      "Epoch [64/100], Step [54/84], Loss: 0.1718\n",
      "Epoch [64/100], Step [55/84], Loss: 0.1784\n",
      "Epoch [64/100], Step [56/84], Loss: 0.1551\n",
      "Epoch [64/100], Step [57/84], Loss: 0.1630\n",
      "Epoch [64/100], Step [58/84], Loss: 0.1483\n",
      "Epoch [64/100], Step [59/84], Loss: 0.1484\n",
      "Epoch [64/100], Step [60/84], Loss: 0.1774\n",
      "Epoch [64/100], Step [61/84], Loss: 0.1834\n",
      "Epoch [64/100], Step [62/84], Loss: 0.1509\n",
      "Epoch [64/100], Step [63/84], Loss: 0.1568\n",
      "Epoch [64/100], Step [64/84], Loss: 0.1764\n",
      "Epoch [64/100], Step [65/84], Loss: 0.1592\n",
      "Epoch [64/100], Step [66/84], Loss: 0.1566\n",
      "Epoch [64/100], Step [67/84], Loss: 0.1685\n",
      "Epoch [64/100], Step [68/84], Loss: 0.1603\n",
      "Epoch [64/100], Step [69/84], Loss: 0.1476\n",
      "Epoch [64/100], Step [70/84], Loss: 0.1524\n",
      "Epoch [64/100], Step [71/84], Loss: 0.2005\n",
      "Epoch [64/100], Step [72/84], Loss: 0.1602\n",
      "Epoch [64/100], Step [73/84], Loss: 0.1622\n",
      "Epoch [64/100], Step [74/84], Loss: 0.1614\n",
      "Epoch [64/100], Step [75/84], Loss: 0.1509\n",
      "Epoch [64/100], Step [76/84], Loss: 0.1672\n",
      "Epoch [64/100], Step [77/84], Loss: 0.1531\n",
      "Epoch [64/100], Step [78/84], Loss: 0.1646\n",
      "Epoch [64/100], Step [79/84], Loss: 0.1644\n",
      "Epoch [64/100], Step [80/84], Loss: 0.1972\n",
      "Epoch [64/100], Step [81/84], Loss: 0.1551\n",
      "Epoch [64/100], Step [82/84], Loss: 0.1537\n",
      "Epoch [64/100], Step [83/84], Loss: 0.1571\n",
      "Epoch [64/100], Step [84/84], Loss: 0.1846\n",
      "Epoch [65/100], Step [1/84], Loss: 0.1575\n",
      "Epoch [65/100], Step [2/84], Loss: 0.1514\n",
      "Epoch [65/100], Step [3/84], Loss: 0.1465\n",
      "Epoch [65/100], Step [4/84], Loss: 0.1639\n",
      "Epoch [65/100], Step [5/84], Loss: 0.1464\n",
      "Epoch [65/100], Step [6/84], Loss: 0.1528\n",
      "Epoch [65/100], Step [7/84], Loss: 0.1701\n",
      "Epoch [65/100], Step [8/84], Loss: 0.1667\n",
      "Epoch [65/100], Step [9/84], Loss: 0.1498\n",
      "Epoch [65/100], Step [10/84], Loss: 0.1600\n",
      "Epoch [65/100], Step [11/84], Loss: 0.1488\n",
      "Epoch [65/100], Step [12/84], Loss: 0.1932\n",
      "Epoch [65/100], Step [13/84], Loss: 0.1572\n",
      "Epoch [65/100], Step [14/84], Loss: 0.1646\n",
      "Epoch [65/100], Step [15/84], Loss: 0.1566\n",
      "Epoch [65/100], Step [16/84], Loss: 0.1787\n",
      "Epoch [65/100], Step [17/84], Loss: 0.1878\n",
      "Epoch [65/100], Step [18/84], Loss: 0.1568\n",
      "Epoch [65/100], Step [19/84], Loss: 0.1513\n",
      "Epoch [65/100], Step [20/84], Loss: 0.1652\n",
      "Epoch [65/100], Step [21/84], Loss: 0.1593\n",
      "Epoch [65/100], Step [22/84], Loss: 0.1438\n",
      "Epoch [65/100], Step [23/84], Loss: 0.1613\n",
      "Epoch [65/100], Step [24/84], Loss: 0.1690\n",
      "Epoch [65/100], Step [25/84], Loss: 0.1570\n",
      "Epoch [65/100], Step [26/84], Loss: 0.1591\n",
      "Epoch [65/100], Step [27/84], Loss: 0.1797\n",
      "Epoch [65/100], Step [28/84], Loss: 0.1666\n",
      "Epoch [65/100], Step [29/84], Loss: 0.1711\n",
      "Epoch [65/100], Step [30/84], Loss: 0.1637\n",
      "Epoch [65/100], Step [31/84], Loss: 0.1635\n",
      "Epoch [65/100], Step [32/84], Loss: 0.1647\n",
      "Epoch [65/100], Step [33/84], Loss: 0.1497\n",
      "Epoch [65/100], Step [34/84], Loss: 0.1707\n",
      "Epoch [65/100], Step [35/84], Loss: 0.1831\n",
      "Epoch [65/100], Step [36/84], Loss: 0.1591\n",
      "Epoch [65/100], Step [37/84], Loss: 0.1802\n",
      "Epoch [65/100], Step [38/84], Loss: 0.1558\n",
      "Epoch [65/100], Step [39/84], Loss: 0.1521\n",
      "Epoch [65/100], Step [40/84], Loss: 0.1705\n",
      "Epoch [65/100], Step [41/84], Loss: 0.1397\n",
      "Epoch [65/100], Step [42/84], Loss: 0.1574\n",
      "Epoch [65/100], Step [43/84], Loss: 0.1451\n",
      "Epoch [65/100], Step [44/84], Loss: 0.1696\n",
      "Epoch [65/100], Step [45/84], Loss: 0.1631\n",
      "Epoch [65/100], Step [46/84], Loss: 0.1660\n",
      "Epoch [65/100], Step [47/84], Loss: 0.1520\n",
      "Epoch [65/100], Step [48/84], Loss: 0.1649\n",
      "Epoch [65/100], Step [49/84], Loss: 0.1653\n",
      "Epoch [65/100], Step [50/84], Loss: 0.1817\n",
      "Epoch [65/100], Step [51/84], Loss: 0.1587\n",
      "Epoch [65/100], Step [52/84], Loss: 0.1354\n",
      "Epoch [65/100], Step [53/84], Loss: 0.1654\n",
      "Epoch [65/100], Step [54/84], Loss: 0.1817\n",
      "Epoch [65/100], Step [55/84], Loss: 0.1729\n",
      "Epoch [65/100], Step [56/84], Loss: 0.1507\n",
      "Epoch [65/100], Step [57/84], Loss: 0.1534\n",
      "Epoch [65/100], Step [58/84], Loss: 0.1526\n",
      "Epoch [65/100], Step [59/84], Loss: 0.1489\n",
      "Epoch [65/100], Step [60/84], Loss: 0.1812\n",
      "Epoch [65/100], Step [61/84], Loss: 0.1612\n",
      "Epoch [65/100], Step [62/84], Loss: 0.1439\n",
      "Epoch [65/100], Step [63/84], Loss: 0.1722\n",
      "Epoch [65/100], Step [64/84], Loss: 0.1796\n",
      "Epoch [65/100], Step [65/84], Loss: 0.1491\n",
      "Epoch [65/100], Step [66/84], Loss: 0.1619\n",
      "Epoch [65/100], Step [67/84], Loss: 0.1585\n",
      "Epoch [65/100], Step [68/84], Loss: 0.1514\n",
      "Epoch [65/100], Step [69/84], Loss: 0.1793\n",
      "Epoch [65/100], Step [70/84], Loss: 0.1724\n",
      "Epoch [65/100], Step [71/84], Loss: 0.1646\n",
      "Epoch [65/100], Step [72/84], Loss: 0.1545\n",
      "Epoch [65/100], Step [73/84], Loss: 0.1559\n",
      "Epoch [65/100], Step [74/84], Loss: 0.1822\n",
      "Epoch [65/100], Step [75/84], Loss: 0.1693\n",
      "Epoch [65/100], Step [76/84], Loss: 0.1785\n",
      "Epoch [65/100], Step [77/84], Loss: 0.1617\n",
      "Epoch [65/100], Step [78/84], Loss: 0.1572\n",
      "Epoch [65/100], Step [79/84], Loss: 0.1594\n",
      "Epoch [65/100], Step [80/84], Loss: 0.1652\n",
      "Epoch [65/100], Step [81/84], Loss: 0.1836\n",
      "Epoch [65/100], Step [82/84], Loss: 0.1569\n",
      "Epoch [65/100], Step [83/84], Loss: 0.1703\n",
      "Epoch [65/100], Step [84/84], Loss: 0.2041\n",
      "Epoch [66/100], Step [1/84], Loss: 0.1554\n",
      "Epoch [66/100], Step [2/84], Loss: 0.1645\n",
      "Epoch [66/100], Step [3/84], Loss: 0.1469\n",
      "Epoch [66/100], Step [4/84], Loss: 0.1615\n",
      "Epoch [66/100], Step [5/84], Loss: 0.1697\n",
      "Epoch [66/100], Step [6/84], Loss: 0.1618\n",
      "Epoch [66/100], Step [7/84], Loss: 0.1649\n",
      "Epoch [66/100], Step [8/84], Loss: 0.1630\n",
      "Epoch [66/100], Step [9/84], Loss: 0.1612\n",
      "Epoch [66/100], Step [10/84], Loss: 0.1571\n",
      "Epoch [66/100], Step [11/84], Loss: 0.1603\n",
      "Epoch [66/100], Step [12/84], Loss: 0.1617\n",
      "Epoch [66/100], Step [13/84], Loss: 0.1579\n",
      "Epoch [66/100], Step [14/84], Loss: 0.1665\n",
      "Epoch [66/100], Step [15/84], Loss: 0.1625\n",
      "Epoch [66/100], Step [16/84], Loss: 0.1881\n",
      "Epoch [66/100], Step [17/84], Loss: 0.1509\n",
      "Epoch [66/100], Step [18/84], Loss: 0.1806\n",
      "Epoch [66/100], Step [19/84], Loss: 0.1472\n",
      "Epoch [66/100], Step [20/84], Loss: 0.1776\n",
      "Epoch [66/100], Step [21/84], Loss: 0.1675\n",
      "Epoch [66/100], Step [22/84], Loss: 0.1532\n",
      "Epoch [66/100], Step [23/84], Loss: 0.1603\n",
      "Epoch [66/100], Step [24/84], Loss: 0.1700\n",
      "Epoch [66/100], Step [25/84], Loss: 0.1668\n",
      "Epoch [66/100], Step [26/84], Loss: 0.1547\n",
      "Epoch [66/100], Step [27/84], Loss: 0.1528\n",
      "Epoch [66/100], Step [28/84], Loss: 0.1462\n",
      "Epoch [66/100], Step [29/84], Loss: 0.1526\n",
      "Epoch [66/100], Step [30/84], Loss: 0.1545\n",
      "Epoch [66/100], Step [31/84], Loss: 0.1555\n",
      "Epoch [66/100], Step [32/84], Loss: 0.1615\n",
      "Epoch [66/100], Step [33/84], Loss: 0.1563\n",
      "Epoch [66/100], Step [34/84], Loss: 0.1720\n",
      "Epoch [66/100], Step [35/84], Loss: 0.1588\n",
      "Epoch [66/100], Step [36/84], Loss: 0.1569\n",
      "Epoch [66/100], Step [37/84], Loss: 0.1421\n",
      "Epoch [66/100], Step [38/84], Loss: 0.1462\n",
      "Epoch [66/100], Step [39/84], Loss: 0.1520\n",
      "Epoch [66/100], Step [40/84], Loss: 0.1590\n",
      "Epoch [66/100], Step [41/84], Loss: 0.1722\n",
      "Epoch [66/100], Step [42/84], Loss: 0.1556\n",
      "Epoch [66/100], Step [43/84], Loss: 0.1655\n",
      "Epoch [66/100], Step [44/84], Loss: 0.1604\n",
      "Epoch [66/100], Step [45/84], Loss: 0.1571\n",
      "Epoch [66/100], Step [46/84], Loss: 0.1743\n",
      "Epoch [66/100], Step [47/84], Loss: 0.1626\n",
      "Epoch [66/100], Step [48/84], Loss: 0.1717\n",
      "Epoch [66/100], Step [49/84], Loss: 0.1535\n",
      "Epoch [66/100], Step [50/84], Loss: 0.1661\n",
      "Epoch [66/100], Step [51/84], Loss: 0.1557\n",
      "Epoch [66/100], Step [52/84], Loss: 0.1743\n",
      "Epoch [66/100], Step [53/84], Loss: 0.1563\n",
      "Epoch [66/100], Step [54/84], Loss: 0.1508\n",
      "Epoch [66/100], Step [55/84], Loss: 0.1812\n",
      "Epoch [66/100], Step [56/84], Loss: 0.1644\n",
      "Epoch [66/100], Step [57/84], Loss: 0.1597\n",
      "Epoch [66/100], Step [58/84], Loss: 0.1631\n",
      "Epoch [66/100], Step [59/84], Loss: 0.1759\n",
      "Epoch [66/100], Step [60/84], Loss: 0.1595\n",
      "Epoch [66/100], Step [61/84], Loss: 0.1558\n",
      "Epoch [66/100], Step [62/84], Loss: 0.1765\n",
      "Epoch [66/100], Step [63/84], Loss: 0.1828\n",
      "Epoch [66/100], Step [64/84], Loss: 0.1839\n",
      "Epoch [66/100], Step [65/84], Loss: 0.1467\n",
      "Epoch [66/100], Step [66/84], Loss: 0.1802\n",
      "Epoch [66/100], Step [67/84], Loss: 0.1843\n",
      "Epoch [66/100], Step [68/84], Loss: 0.1830\n",
      "Epoch [66/100], Step [69/84], Loss: 0.1913\n",
      "Epoch [66/100], Step [70/84], Loss: 0.1723\n",
      "Epoch [66/100], Step [71/84], Loss: 0.1657\n",
      "Epoch [66/100], Step [72/84], Loss: 0.1674\n",
      "Epoch [66/100], Step [73/84], Loss: 0.2122\n",
      "Epoch [66/100], Step [74/84], Loss: 0.1794\n",
      "Epoch [66/100], Step [75/84], Loss: 0.1627\n",
      "Epoch [66/100], Step [76/84], Loss: 0.1656\n",
      "Epoch [66/100], Step [77/84], Loss: 0.1770\n",
      "Epoch [66/100], Step [78/84], Loss: 0.1593\n",
      "Epoch [66/100], Step [79/84], Loss: 0.2023\n",
      "Epoch [66/100], Step [80/84], Loss: 0.1585\n",
      "Epoch [66/100], Step [81/84], Loss: 0.1817\n",
      "Epoch [66/100], Step [82/84], Loss: 0.2001\n",
      "Epoch [66/100], Step [83/84], Loss: 0.1658\n",
      "Epoch [66/100], Step [84/84], Loss: 0.1938\n",
      "Epoch [67/100], Step [1/84], Loss: 0.1563\n",
      "Epoch [67/100], Step [2/84], Loss: 0.1952\n",
      "Epoch [67/100], Step [3/84], Loss: 0.1855\n",
      "Epoch [67/100], Step [4/84], Loss: 0.1499\n",
      "Epoch [67/100], Step [5/84], Loss: 0.2040\n",
      "Epoch [67/100], Step [6/84], Loss: 0.1768\n",
      "Epoch [67/100], Step [7/84], Loss: 0.2057\n",
      "Epoch [67/100], Step [8/84], Loss: 0.1747\n",
      "Epoch [67/100], Step [9/84], Loss: 0.1718\n",
      "Epoch [67/100], Step [10/84], Loss: 0.1714\n",
      "Epoch [67/100], Step [11/84], Loss: 0.2021\n",
      "Epoch [67/100], Step [12/84], Loss: 0.1814\n",
      "Epoch [67/100], Step [13/84], Loss: 0.1651\n",
      "Epoch [67/100], Step [14/84], Loss: 0.1745\n",
      "Epoch [67/100], Step [15/84], Loss: 0.1706\n",
      "Epoch [67/100], Step [16/84], Loss: 0.1651\n",
      "Epoch [67/100], Step [17/84], Loss: 0.1604\n",
      "Epoch [67/100], Step [18/84], Loss: 0.1684\n",
      "Epoch [67/100], Step [19/84], Loss: 0.1872\n",
      "Epoch [67/100], Step [20/84], Loss: 0.1911\n",
      "Epoch [67/100], Step [21/84], Loss: 0.1699\n",
      "Epoch [67/100], Step [22/84], Loss: 0.1843\n",
      "Epoch [67/100], Step [23/84], Loss: 0.1525\n",
      "Epoch [67/100], Step [24/84], Loss: 0.1711\n",
      "Epoch [67/100], Step [25/84], Loss: 0.1678\n",
      "Epoch [67/100], Step [26/84], Loss: 0.2062\n",
      "Epoch [67/100], Step [27/84], Loss: 0.1685\n",
      "Epoch [67/100], Step [28/84], Loss: 0.1703\n",
      "Epoch [67/100], Step [29/84], Loss: 0.1604\n",
      "Epoch [67/100], Step [30/84], Loss: 0.1696\n",
      "Epoch [67/100], Step [31/84], Loss: 0.2139\n",
      "Epoch [67/100], Step [32/84], Loss: 0.1951\n",
      "Epoch [67/100], Step [33/84], Loss: 0.1730\n",
      "Epoch [67/100], Step [34/84], Loss: 0.1833\n",
      "Epoch [67/100], Step [35/84], Loss: 0.1741\n",
      "Epoch [67/100], Step [36/84], Loss: 0.2145\n",
      "Epoch [67/100], Step [37/84], Loss: 0.1757\n",
      "Epoch [67/100], Step [38/84], Loss: 0.1660\n",
      "Epoch [67/100], Step [39/84], Loss: 0.1672\n",
      "Epoch [67/100], Step [40/84], Loss: 0.1687\n",
      "Epoch [67/100], Step [41/84], Loss: 0.2126\n",
      "Epoch [67/100], Step [42/84], Loss: 0.1857\n",
      "Epoch [67/100], Step [43/84], Loss: 0.1752\n",
      "Epoch [67/100], Step [44/84], Loss: 0.1917\n",
      "Epoch [67/100], Step [45/84], Loss: 0.1767\n",
      "Epoch [67/100], Step [46/84], Loss: 0.1724\n",
      "Epoch [67/100], Step [47/84], Loss: 0.2034\n",
      "Epoch [67/100], Step [48/84], Loss: 0.1682\n",
      "Epoch [67/100], Step [49/84], Loss: 0.1858\n",
      "Epoch [67/100], Step [50/84], Loss: 0.1866\n",
      "Epoch [67/100], Step [51/84], Loss: 0.1966\n",
      "Epoch [67/100], Step [52/84], Loss: 0.1717\n",
      "Epoch [67/100], Step [53/84], Loss: 0.1866\n",
      "Epoch [67/100], Step [54/84], Loss: 0.1529\n",
      "Epoch [67/100], Step [55/84], Loss: 0.1650\n",
      "Epoch [67/100], Step [56/84], Loss: 0.1893\n",
      "Epoch [67/100], Step [57/84], Loss: 0.1703\n",
      "Epoch [67/100], Step [58/84], Loss: 0.1512\n",
      "Epoch [67/100], Step [59/84], Loss: 0.1785\n",
      "Epoch [67/100], Step [60/84], Loss: 0.1722\n",
      "Epoch [67/100], Step [61/84], Loss: 0.1683\n",
      "Epoch [67/100], Step [62/84], Loss: 0.2347\n",
      "Epoch [67/100], Step [63/84], Loss: 0.1735\n",
      "Epoch [67/100], Step [64/84], Loss: 0.1720\n",
      "Epoch [67/100], Step [65/84], Loss: 0.1761\n",
      "Epoch [67/100], Step [66/84], Loss: 0.1788\n",
      "Epoch [67/100], Step [67/84], Loss: 0.1682\n",
      "Epoch [67/100], Step [68/84], Loss: 0.1834\n",
      "Epoch [67/100], Step [69/84], Loss: 0.1935\n",
      "Epoch [67/100], Step [70/84], Loss: 0.1741\n",
      "Epoch [67/100], Step [71/84], Loss: 0.1639\n",
      "Epoch [67/100], Step [72/84], Loss: 0.1730\n",
      "Epoch [67/100], Step [73/84], Loss: 0.2077\n",
      "Epoch [67/100], Step [74/84], Loss: 0.1634\n",
      "Epoch [67/100], Step [75/84], Loss: 0.1702\n",
      "Epoch [67/100], Step [76/84], Loss: 0.1776\n",
      "Epoch [67/100], Step [77/84], Loss: 0.1610\n",
      "Epoch [67/100], Step [78/84], Loss: 0.1914\n",
      "Epoch [67/100], Step [79/84], Loss: 0.1571\n",
      "Epoch [67/100], Step [80/84], Loss: 0.1771\n",
      "Epoch [67/100], Step [81/84], Loss: 0.2127\n",
      "Epoch [67/100], Step [82/84], Loss: 0.1797\n",
      "Epoch [67/100], Step [83/84], Loss: 0.1799\n",
      "Epoch [67/100], Step [84/84], Loss: 0.2114\n",
      "Epoch [68/100], Step [1/84], Loss: 0.1779\n",
      "Epoch [68/100], Step [2/84], Loss: 0.1777\n",
      "Epoch [68/100], Step [3/84], Loss: 0.1654\n",
      "Epoch [68/100], Step [4/84], Loss: 0.1540\n",
      "Epoch [68/100], Step [5/84], Loss: 0.1732\n",
      "Epoch [68/100], Step [6/84], Loss: 0.1749\n",
      "Epoch [68/100], Step [7/84], Loss: 0.1724\n",
      "Epoch [68/100], Step [8/84], Loss: 0.1580\n",
      "Epoch [68/100], Step [9/84], Loss: 0.1618\n",
      "Epoch [68/100], Step [10/84], Loss: 0.1598\n",
      "Epoch [68/100], Step [11/84], Loss: 0.1881\n",
      "Epoch [68/100], Step [12/84], Loss: 0.1566\n",
      "Epoch [68/100], Step [13/84], Loss: 0.1958\n",
      "Epoch [68/100], Step [14/84], Loss: 0.1801\n",
      "Epoch [68/100], Step [15/84], Loss: 0.1791\n",
      "Epoch [68/100], Step [16/84], Loss: 0.1637\n",
      "Epoch [68/100], Step [17/84], Loss: 0.1827\n",
      "Epoch [68/100], Step [18/84], Loss: 0.1601\n",
      "Epoch [68/100], Step [19/84], Loss: 0.1703\n",
      "Epoch [68/100], Step [20/84], Loss: 0.1703\n",
      "Epoch [68/100], Step [21/84], Loss: 0.1767\n",
      "Epoch [68/100], Step [22/84], Loss: 0.1643\n",
      "Epoch [68/100], Step [23/84], Loss: 0.1976\n",
      "Epoch [68/100], Step [24/84], Loss: 0.1999\n",
      "Epoch [68/100], Step [25/84], Loss: 0.1778\n",
      "Epoch [68/100], Step [26/84], Loss: 0.1692\n",
      "Epoch [68/100], Step [27/84], Loss: 0.1541\n",
      "Epoch [68/100], Step [28/84], Loss: 0.1533\n",
      "Epoch [68/100], Step [29/84], Loss: 0.1409\n",
      "Epoch [68/100], Step [30/84], Loss: 0.1662\n",
      "Epoch [68/100], Step [31/84], Loss: 0.1783\n",
      "Epoch [68/100], Step [32/84], Loss: 0.1790\n",
      "Epoch [68/100], Step [33/84], Loss: 0.1743\n",
      "Epoch [68/100], Step [34/84], Loss: 0.1637\n",
      "Epoch [68/100], Step [35/84], Loss: 0.1919\n",
      "Epoch [68/100], Step [36/84], Loss: 0.1782\n",
      "Epoch [68/100], Step [37/84], Loss: 0.1753\n",
      "Epoch [68/100], Step [38/84], Loss: 0.1675\n",
      "Epoch [68/100], Step [39/84], Loss: 0.1543\n",
      "Epoch [68/100], Step [40/84], Loss: 0.1697\n",
      "Epoch [68/100], Step [41/84], Loss: 0.1556\n",
      "Epoch [68/100], Step [42/84], Loss: 0.1916\n",
      "Epoch [68/100], Step [43/84], Loss: 0.1837\n",
      "Epoch [68/100], Step [44/84], Loss: 0.1694\n",
      "Epoch [68/100], Step [45/84], Loss: 0.1659\n",
      "Epoch [68/100], Step [46/84], Loss: 0.1562\n",
      "Epoch [68/100], Step [47/84], Loss: 0.1925\n",
      "Epoch [68/100], Step [48/84], Loss: 0.1720\n",
      "Epoch [68/100], Step [49/84], Loss: 0.1960\n",
      "Epoch [68/100], Step [50/84], Loss: 0.1727\n",
      "Epoch [68/100], Step [51/84], Loss: 0.1705\n",
      "Epoch [68/100], Step [52/84], Loss: 0.1525\n",
      "Epoch [68/100], Step [53/84], Loss: 0.1712\n",
      "Epoch [68/100], Step [54/84], Loss: 0.1586\n",
      "Epoch [68/100], Step [55/84], Loss: 0.1734\n",
      "Epoch [68/100], Step [56/84], Loss: 0.1635\n",
      "Epoch [68/100], Step [57/84], Loss: 0.1847\n",
      "Epoch [68/100], Step [58/84], Loss: 0.1950\n",
      "Epoch [68/100], Step [59/84], Loss: 0.1815\n",
      "Epoch [68/100], Step [60/84], Loss: 0.1953\n",
      "Epoch [68/100], Step [61/84], Loss: 0.1584\n",
      "Epoch [68/100], Step [62/84], Loss: 0.1678\n",
      "Epoch [68/100], Step [63/84], Loss: 0.1475\n",
      "Epoch [68/100], Step [64/84], Loss: 0.1811\n",
      "Epoch [68/100], Step [65/84], Loss: 0.1575\n",
      "Epoch [68/100], Step [66/84], Loss: 0.1551\n",
      "Epoch [68/100], Step [67/84], Loss: 0.1598\n",
      "Epoch [68/100], Step [68/84], Loss: 0.2457\n",
      "Epoch [68/100], Step [69/84], Loss: 0.1509\n",
      "Epoch [68/100], Step [70/84], Loss: 0.1543\n",
      "Epoch [68/100], Step [71/84], Loss: 0.1530\n",
      "Epoch [68/100], Step [72/84], Loss: 0.1731\n",
      "Epoch [68/100], Step [73/84], Loss: 0.1665\n",
      "Epoch [68/100], Step [74/84], Loss: 0.1979\n",
      "Epoch [68/100], Step [75/84], Loss: 0.1691\n",
      "Epoch [68/100], Step [76/84], Loss: 0.1669\n",
      "Epoch [68/100], Step [77/84], Loss: 0.1686\n",
      "Epoch [68/100], Step [78/84], Loss: 0.1850\n",
      "Epoch [68/100], Step [79/84], Loss: 0.1781\n",
      "Epoch [68/100], Step [80/84], Loss: 0.1756\n",
      "Epoch [68/100], Step [81/84], Loss: 0.2049\n",
      "Epoch [68/100], Step [82/84], Loss: 0.1909\n",
      "Epoch [68/100], Step [83/84], Loss: 0.1558\n",
      "Epoch [68/100], Step [84/84], Loss: 0.2009\n",
      "Epoch [69/100], Step [1/84], Loss: 0.1716\n",
      "Epoch [69/100], Step [2/84], Loss: 0.1555\n",
      "Epoch [69/100], Step [3/84], Loss: 0.1512\n",
      "Epoch [69/100], Step [4/84], Loss: 0.1693\n",
      "Epoch [69/100], Step [5/84], Loss: 0.1732\n",
      "Epoch [69/100], Step [6/84], Loss: 0.1874\n",
      "Epoch [69/100], Step [7/84], Loss: 0.1868\n",
      "Epoch [69/100], Step [8/84], Loss: 0.1697\n",
      "Epoch [69/100], Step [9/84], Loss: 0.1640\n",
      "Epoch [69/100], Step [10/84], Loss: 0.1899\n",
      "Epoch [69/100], Step [11/84], Loss: 0.1713\n",
      "Epoch [69/100], Step [12/84], Loss: 0.1778\n",
      "Epoch [69/100], Step [13/84], Loss: 0.1762\n",
      "Epoch [69/100], Step [14/84], Loss: 0.1741\n",
      "Epoch [69/100], Step [15/84], Loss: 0.1748\n",
      "Epoch [69/100], Step [16/84], Loss: 0.1575\n",
      "Epoch [69/100], Step [17/84], Loss: 0.1595\n",
      "Epoch [69/100], Step [18/84], Loss: 0.1614\n",
      "Epoch [69/100], Step [19/84], Loss: 0.1567\n",
      "Epoch [69/100], Step [20/84], Loss: 0.1894\n",
      "Epoch [69/100], Step [21/84], Loss: 0.1632\n",
      "Epoch [69/100], Step [22/84], Loss: 0.1865\n",
      "Epoch [69/100], Step [23/84], Loss: 0.1610\n",
      "Epoch [69/100], Step [24/84], Loss: 0.1658\n",
      "Epoch [69/100], Step [25/84], Loss: 0.1727\n",
      "Epoch [69/100], Step [26/84], Loss: 0.1555\n",
      "Epoch [69/100], Step [27/84], Loss: 0.1687\n",
      "Epoch [69/100], Step [28/84], Loss: 0.1676\n",
      "Epoch [69/100], Step [29/84], Loss: 0.1858\n",
      "Epoch [69/100], Step [30/84], Loss: 0.1613\n",
      "Epoch [69/100], Step [31/84], Loss: 0.1685\n",
      "Epoch [69/100], Step [32/84], Loss: 0.1708\n",
      "Epoch [69/100], Step [33/84], Loss: 0.1628\n",
      "Epoch [69/100], Step [34/84], Loss: 0.1469\n",
      "Epoch [69/100], Step [35/84], Loss: 0.1619\n",
      "Epoch [69/100], Step [36/84], Loss: 0.1643\n",
      "Epoch [69/100], Step [37/84], Loss: 0.1659\n",
      "Epoch [69/100], Step [38/84], Loss: 0.1715\n",
      "Epoch [69/100], Step [39/84], Loss: 0.1642\n",
      "Epoch [69/100], Step [40/84], Loss: 0.1667\n",
      "Epoch [69/100], Step [41/84], Loss: 0.2111\n",
      "Epoch [69/100], Step [42/84], Loss: 0.1744\n",
      "Epoch [69/100], Step [43/84], Loss: 0.1602\n",
      "Epoch [69/100], Step [44/84], Loss: 0.1621\n",
      "Epoch [69/100], Step [45/84], Loss: 0.1682\n",
      "Epoch [69/100], Step [46/84], Loss: 0.1622\n",
      "Epoch [69/100], Step [47/84], Loss: 0.1752\n",
      "Epoch [69/100], Step [48/84], Loss: 0.1886\n",
      "Epoch [69/100], Step [49/84], Loss: 0.1686\n",
      "Epoch [69/100], Step [50/84], Loss: 0.1545\n",
      "Epoch [69/100], Step [51/84], Loss: 0.1588\n",
      "Epoch [69/100], Step [52/84], Loss: 0.1745\n",
      "Epoch [69/100], Step [53/84], Loss: 0.1602\n",
      "Epoch [69/100], Step [54/84], Loss: 0.1819\n",
      "Epoch [69/100], Step [55/84], Loss: 0.1675\n",
      "Epoch [69/100], Step [56/84], Loss: 0.1637\n",
      "Epoch [69/100], Step [57/84], Loss: 0.1539\n",
      "Epoch [69/100], Step [58/84], Loss: 0.1685\n",
      "Epoch [69/100], Step [59/84], Loss: 0.1589\n",
      "Epoch [69/100], Step [60/84], Loss: 0.1618\n",
      "Epoch [69/100], Step [61/84], Loss: 0.1643\n",
      "Epoch [69/100], Step [62/84], Loss: 0.1746\n",
      "Epoch [69/100], Step [63/84], Loss: 0.1711\n",
      "Epoch [69/100], Step [64/84], Loss: 0.1451\n",
      "Epoch [69/100], Step [65/84], Loss: 0.1821\n",
      "Epoch [69/100], Step [66/84], Loss: 0.1542\n",
      "Epoch [69/100], Step [67/84], Loss: 0.1572\n",
      "Epoch [69/100], Step [68/84], Loss: 0.1612\n",
      "Epoch [69/100], Step [69/84], Loss: 0.1889\n",
      "Epoch [69/100], Step [70/84], Loss: 0.1418\n",
      "Epoch [69/100], Step [71/84], Loss: 0.1600\n",
      "Epoch [69/100], Step [72/84], Loss: 0.1678\n",
      "Epoch [69/100], Step [73/84], Loss: 0.1681\n",
      "Epoch [69/100], Step [74/84], Loss: 0.1629\n",
      "Epoch [69/100], Step [75/84], Loss: 0.1527\n",
      "Epoch [69/100], Step [76/84], Loss: 0.1591\n",
      "Epoch [69/100], Step [77/84], Loss: 0.1597\n",
      "Epoch [69/100], Step [78/84], Loss: 0.1691\n",
      "Epoch [69/100], Step [79/84], Loss: 0.1780\n",
      "Epoch [69/100], Step [80/84], Loss: 0.1627\n",
      "Epoch [69/100], Step [81/84], Loss: 0.1639\n",
      "Epoch [69/100], Step [82/84], Loss: 0.1976\n",
      "Epoch [69/100], Step [83/84], Loss: 0.1765\n",
      "Epoch [69/100], Step [84/84], Loss: 0.1603\n",
      "Epoch [70/100], Step [1/84], Loss: 0.1552\n",
      "Epoch [70/100], Step [2/84], Loss: 0.1633\n",
      "Epoch [70/100], Step [3/84], Loss: 0.1626\n",
      "Epoch [70/100], Step [4/84], Loss: 0.1569\n",
      "Epoch [70/100], Step [5/84], Loss: 0.1868\n",
      "Epoch [70/100], Step [6/84], Loss: 0.1840\n",
      "Epoch [70/100], Step [7/84], Loss: 0.1668\n",
      "Epoch [70/100], Step [8/84], Loss: 0.1564\n",
      "Epoch [70/100], Step [9/84], Loss: 0.1574\n",
      "Epoch [70/100], Step [10/84], Loss: 0.1613\n",
      "Epoch [70/100], Step [11/84], Loss: 0.1855\n",
      "Epoch [70/100], Step [12/84], Loss: 0.1927\n",
      "Epoch [70/100], Step [13/84], Loss: 0.1673\n",
      "Epoch [70/100], Step [14/84], Loss: 0.1563\n",
      "Epoch [70/100], Step [15/84], Loss: 0.1531\n",
      "Epoch [70/100], Step [16/84], Loss: 0.1816\n",
      "Epoch [70/100], Step [17/84], Loss: 0.1614\n",
      "Epoch [70/100], Step [18/84], Loss: 0.1636\n",
      "Epoch [70/100], Step [19/84], Loss: 0.1627\n",
      "Epoch [70/100], Step [20/84], Loss: 0.1493\n",
      "Epoch [70/100], Step [21/84], Loss: 0.1841\n",
      "Epoch [70/100], Step [22/84], Loss: 0.1752\n",
      "Epoch [70/100], Step [23/84], Loss: 0.1532\n",
      "Epoch [70/100], Step [24/84], Loss: 0.1562\n",
      "Epoch [70/100], Step [25/84], Loss: 0.1643\n",
      "Epoch [70/100], Step [26/84], Loss: 0.1931\n",
      "Epoch [70/100], Step [27/84], Loss: 0.1516\n",
      "Epoch [70/100], Step [28/84], Loss: 0.1567\n",
      "Epoch [70/100], Step [29/84], Loss: 0.1669\n",
      "Epoch [70/100], Step [30/84], Loss: 0.1799\n",
      "Epoch [70/100], Step [31/84], Loss: 0.1652\n",
      "Epoch [70/100], Step [32/84], Loss: 0.1528\n",
      "Epoch [70/100], Step [33/84], Loss: 0.1585\n",
      "Epoch [70/100], Step [34/84], Loss: 0.1574\n",
      "Epoch [70/100], Step [35/84], Loss: 0.1455\n",
      "Epoch [70/100], Step [36/84], Loss: 0.1630\n",
      "Epoch [70/100], Step [37/84], Loss: 0.1728\n",
      "Epoch [70/100], Step [38/84], Loss: 0.1650\n",
      "Epoch [70/100], Step [39/84], Loss: 0.1383\n",
      "Epoch [70/100], Step [40/84], Loss: 0.1861\n",
      "Epoch [70/100], Step [41/84], Loss: 0.1496\n",
      "Epoch [70/100], Step [42/84], Loss: 0.1519\n",
      "Epoch [70/100], Step [43/84], Loss: 0.1523\n",
      "Epoch [70/100], Step [44/84], Loss: 0.1473\n",
      "Epoch [70/100], Step [45/84], Loss: 0.1635\n",
      "Epoch [70/100], Step [46/84], Loss: 0.1720\n",
      "Epoch [70/100], Step [47/84], Loss: 0.1501\n",
      "Epoch [70/100], Step [48/84], Loss: 0.1666\n",
      "Epoch [70/100], Step [49/84], Loss: 0.1579\n",
      "Epoch [70/100], Step [50/84], Loss: 0.1873\n",
      "Epoch [70/100], Step [51/84], Loss: 0.1713\n",
      "Epoch [70/100], Step [52/84], Loss: 0.1752\n",
      "Epoch [70/100], Step [53/84], Loss: 0.1827\n",
      "Epoch [70/100], Step [54/84], Loss: 0.1728\n",
      "Epoch [70/100], Step [55/84], Loss: 0.1572\n",
      "Epoch [70/100], Step [56/84], Loss: 0.1602\n",
      "Epoch [70/100], Step [57/84], Loss: 0.1619\n",
      "Epoch [70/100], Step [58/84], Loss: 0.1659\n",
      "Epoch [70/100], Step [59/84], Loss: 0.1751\n",
      "Epoch [70/100], Step [60/84], Loss: 0.1872\n",
      "Epoch [70/100], Step [61/84], Loss: 0.1579\n",
      "Epoch [70/100], Step [62/84], Loss: 0.1452\n",
      "Epoch [70/100], Step [63/84], Loss: 0.1747\n",
      "Epoch [70/100], Step [64/84], Loss: 0.1664\n",
      "Epoch [70/100], Step [65/84], Loss: 0.1556\n",
      "Epoch [70/100], Step [66/84], Loss: 0.1603\n",
      "Epoch [70/100], Step [67/84], Loss: 0.1512\n",
      "Epoch [70/100], Step [68/84], Loss: 0.1719\n",
      "Epoch [70/100], Step [69/84], Loss: 0.1856\n",
      "Epoch [70/100], Step [70/84], Loss: 0.1795\n",
      "Epoch [70/100], Step [71/84], Loss: 0.1516\n",
      "Epoch [70/100], Step [72/84], Loss: 0.1580\n",
      "Epoch [70/100], Step [73/84], Loss: 0.1890\n",
      "Epoch [70/100], Step [74/84], Loss: 0.1826\n",
      "Epoch [70/100], Step [75/84], Loss: 0.1603\n",
      "Epoch [70/100], Step [76/84], Loss: 0.1491\n",
      "Epoch [70/100], Step [77/84], Loss: 0.1509\n",
      "Epoch [70/100], Step [78/84], Loss: 0.1645\n",
      "Epoch [70/100], Step [79/84], Loss: 0.1830\n",
      "Epoch [70/100], Step [80/84], Loss: 0.1589\n",
      "Epoch [70/100], Step [81/84], Loss: 0.1508\n",
      "Epoch [70/100], Step [82/84], Loss: 0.1801\n",
      "Epoch [70/100], Step [83/84], Loss: 0.1636\n",
      "Epoch [70/100], Step [84/84], Loss: 0.1696\n",
      "Epoch [71/100], Step [1/84], Loss: 0.1655\n",
      "Epoch [71/100], Step [2/84], Loss: 0.1555\n",
      "Epoch [71/100], Step [3/84], Loss: 0.1603\n",
      "Epoch [71/100], Step [4/84], Loss: 0.1747\n",
      "Epoch [71/100], Step [5/84], Loss: 0.1769\n",
      "Epoch [71/100], Step [6/84], Loss: 0.1685\n",
      "Epoch [71/100], Step [7/84], Loss: 0.1778\n",
      "Epoch [71/100], Step [8/84], Loss: 0.1518\n",
      "Epoch [71/100], Step [9/84], Loss: 0.1578\n",
      "Epoch [71/100], Step [10/84], Loss: 0.1659\n",
      "Epoch [71/100], Step [11/84], Loss: 0.1573\n",
      "Epoch [71/100], Step [12/84], Loss: 0.1729\n",
      "Epoch [71/100], Step [13/84], Loss: 0.1840\n",
      "Epoch [71/100], Step [14/84], Loss: 0.1724\n",
      "Epoch [71/100], Step [15/84], Loss: 0.1552\n",
      "Epoch [71/100], Step [16/84], Loss: 0.1534\n",
      "Epoch [71/100], Step [17/84], Loss: 0.1587\n",
      "Epoch [71/100], Step [18/84], Loss: 0.1578\n",
      "Epoch [71/100], Step [19/84], Loss: 0.1488\n",
      "Epoch [71/100], Step [20/84], Loss: 0.1489\n",
      "Epoch [71/100], Step [21/84], Loss: 0.1565\n",
      "Epoch [71/100], Step [22/84], Loss: 0.1742\n",
      "Epoch [71/100], Step [23/84], Loss: 0.1841\n",
      "Epoch [71/100], Step [24/84], Loss: 0.1621\n",
      "Epoch [71/100], Step [25/84], Loss: 0.1596\n",
      "Epoch [71/100], Step [26/84], Loss: 0.1583\n",
      "Epoch [71/100], Step [27/84], Loss: 0.1690\n",
      "Epoch [71/100], Step [28/84], Loss: 0.1626\n",
      "Epoch [71/100], Step [29/84], Loss: 0.1535\n",
      "Epoch [71/100], Step [30/84], Loss: 0.1598\n",
      "Epoch [71/100], Step [31/84], Loss: 0.1542\n",
      "Epoch [71/100], Step [32/84], Loss: 0.1548\n",
      "Epoch [71/100], Step [33/84], Loss: 0.1686\n",
      "Epoch [71/100], Step [34/84], Loss: 0.1539\n",
      "Epoch [71/100], Step [35/84], Loss: 0.1494\n",
      "Epoch [71/100], Step [36/84], Loss: 0.1890\n",
      "Epoch [71/100], Step [37/84], Loss: 0.1543\n",
      "Epoch [71/100], Step [38/84], Loss: 0.1492\n",
      "Epoch [71/100], Step [39/84], Loss: 0.1659\n",
      "Epoch [71/100], Step [40/84], Loss: 0.1758\n",
      "Epoch [71/100], Step [41/84], Loss: 0.1604\n",
      "Epoch [71/100], Step [42/84], Loss: 0.1711\n",
      "Epoch [71/100], Step [43/84], Loss: 0.1636\n",
      "Epoch [71/100], Step [44/84], Loss: 0.1499\n",
      "Epoch [71/100], Step [45/84], Loss: 0.1668\n",
      "Epoch [71/100], Step [46/84], Loss: 0.1510\n",
      "Epoch [71/100], Step [47/84], Loss: 0.1668\n",
      "Epoch [71/100], Step [48/84], Loss: 0.1685\n",
      "Epoch [71/100], Step [49/84], Loss: 0.1632\n",
      "Epoch [71/100], Step [50/84], Loss: 0.1574\n",
      "Epoch [71/100], Step [51/84], Loss: 0.1667\n",
      "Epoch [71/100], Step [52/84], Loss: 0.1717\n",
      "Epoch [71/100], Step [53/84], Loss: 0.1608\n",
      "Epoch [71/100], Step [54/84], Loss: 0.1510\n",
      "Epoch [71/100], Step [55/84], Loss: 0.1550\n",
      "Epoch [71/100], Step [56/84], Loss: 0.1723\n",
      "Epoch [71/100], Step [57/84], Loss: 0.1465\n",
      "Epoch [71/100], Step [58/84], Loss: 0.1476\n",
      "Epoch [71/100], Step [59/84], Loss: 0.1502\n",
      "Epoch [71/100], Step [60/84], Loss: 0.1581\n",
      "Epoch [71/100], Step [61/84], Loss: 0.1569\n",
      "Epoch [71/100], Step [62/84], Loss: 0.1519\n",
      "Epoch [71/100], Step [63/84], Loss: 0.1615\n",
      "Epoch [71/100], Step [64/84], Loss: 0.1845\n",
      "Epoch [71/100], Step [65/84], Loss: 0.1651\n",
      "Epoch [71/100], Step [66/84], Loss: 0.1643\n",
      "Epoch [71/100], Step [67/84], Loss: 0.1657\n",
      "Epoch [71/100], Step [68/84], Loss: 0.1477\n",
      "Epoch [71/100], Step [69/84], Loss: 0.2048\n",
      "Epoch [71/100], Step [70/84], Loss: 0.1801\n",
      "Epoch [71/100], Step [71/84], Loss: 0.1610\n",
      "Epoch [71/100], Step [72/84], Loss: 0.1556\n",
      "Epoch [71/100], Step [73/84], Loss: 0.1483\n",
      "Epoch [71/100], Step [74/84], Loss: 0.1761\n",
      "Epoch [71/100], Step [75/84], Loss: 0.1483\n",
      "Epoch [71/100], Step [76/84], Loss: 0.1614\n",
      "Epoch [71/100], Step [77/84], Loss: 0.1646\n",
      "Epoch [71/100], Step [78/84], Loss: 0.1787\n",
      "Epoch [71/100], Step [79/84], Loss: 0.1583\n",
      "Epoch [71/100], Step [80/84], Loss: 0.1771\n",
      "Epoch [71/100], Step [81/84], Loss: 0.1488\n",
      "Epoch [71/100], Step [82/84], Loss: 0.1812\n",
      "Epoch [71/100], Step [83/84], Loss: 0.1456\n",
      "Epoch [71/100], Step [84/84], Loss: 0.1894\n",
      "Epoch [72/100], Step [1/84], Loss: 0.1576\n",
      "Epoch [72/100], Step [2/84], Loss: 0.1392\n",
      "Epoch [72/100], Step [3/84], Loss: 0.1645\n",
      "Epoch [72/100], Step [4/84], Loss: 0.1538\n",
      "Epoch [72/100], Step [5/84], Loss: 0.1537\n",
      "Epoch [72/100], Step [6/84], Loss: 0.1909\n",
      "Epoch [72/100], Step [7/84], Loss: 0.1518\n",
      "Epoch [72/100], Step [8/84], Loss: 0.1794\n",
      "Epoch [72/100], Step [9/84], Loss: 0.1569\n",
      "Epoch [72/100], Step [10/84], Loss: 0.2015\n",
      "Epoch [72/100], Step [11/84], Loss: 0.1590\n",
      "Epoch [72/100], Step [12/84], Loss: 0.1623\n",
      "Epoch [72/100], Step [13/84], Loss: 0.1340\n",
      "Epoch [72/100], Step [14/84], Loss: 0.1539\n",
      "Epoch [72/100], Step [15/84], Loss: 0.1452\n",
      "Epoch [72/100], Step [16/84], Loss: 0.1485\n",
      "Epoch [72/100], Step [17/84], Loss: 0.1657\n",
      "Epoch [72/100], Step [18/84], Loss: 0.1632\n",
      "Epoch [72/100], Step [19/84], Loss: 0.1477\n",
      "Epoch [72/100], Step [20/84], Loss: 0.1440\n",
      "Epoch [72/100], Step [21/84], Loss: 0.1489\n",
      "Epoch [72/100], Step [22/84], Loss: 0.1659\n",
      "Epoch [72/100], Step [23/84], Loss: 0.1762\n",
      "Epoch [72/100], Step [24/84], Loss: 0.1485\n",
      "Epoch [72/100], Step [25/84], Loss: 0.1715\n",
      "Epoch [72/100], Step [26/84], Loss: 0.1504\n",
      "Epoch [72/100], Step [27/84], Loss: 0.1540\n",
      "Epoch [72/100], Step [28/84], Loss: 0.1606\n",
      "Epoch [72/100], Step [29/84], Loss: 0.1595\n",
      "Epoch [72/100], Step [30/84], Loss: 0.1615\n",
      "Epoch [72/100], Step [31/84], Loss: 0.1658\n",
      "Epoch [72/100], Step [32/84], Loss: 0.1495\n",
      "Epoch [72/100], Step [33/84], Loss: 0.1710\n",
      "Epoch [72/100], Step [34/84], Loss: 0.1443\n",
      "Epoch [72/100], Step [35/84], Loss: 0.1568\n",
      "Epoch [72/100], Step [36/84], Loss: 0.1914\n",
      "Epoch [72/100], Step [37/84], Loss: 0.1480\n",
      "Epoch [72/100], Step [38/84], Loss: 0.1476\n",
      "Epoch [72/100], Step [39/84], Loss: 0.1616\n",
      "Epoch [72/100], Step [40/84], Loss: 0.1733\n",
      "Epoch [72/100], Step [41/84], Loss: 0.1672\n",
      "Epoch [72/100], Step [42/84], Loss: 0.1509\n",
      "Epoch [72/100], Step [43/84], Loss: 0.1722\n",
      "Epoch [72/100], Step [44/84], Loss: 0.1548\n",
      "Epoch [72/100], Step [45/84], Loss: 0.1476\n",
      "Epoch [72/100], Step [46/84], Loss: 0.1620\n",
      "Epoch [72/100], Step [47/84], Loss: 0.1397\n",
      "Epoch [72/100], Step [48/84], Loss: 0.1451\n",
      "Epoch [72/100], Step [49/84], Loss: 0.1696\n",
      "Epoch [72/100], Step [50/84], Loss: 0.1586\n",
      "Epoch [72/100], Step [51/84], Loss: 0.1553\n",
      "Epoch [72/100], Step [52/84], Loss: 0.1489\n",
      "Epoch [72/100], Step [53/84], Loss: 0.1603\n",
      "Epoch [72/100], Step [54/84], Loss: 0.2057\n",
      "Epoch [72/100], Step [55/84], Loss: 0.1816\n",
      "Epoch [72/100], Step [56/84], Loss: 0.1520\n",
      "Epoch [72/100], Step [57/84], Loss: 0.1541\n",
      "Epoch [72/100], Step [58/84], Loss: 0.1565\n",
      "Epoch [72/100], Step [59/84], Loss: 0.1472\n",
      "Epoch [72/100], Step [60/84], Loss: 0.1727\n",
      "Epoch [72/100], Step [61/84], Loss: 0.1517\n",
      "Epoch [72/100], Step [62/84], Loss: 0.1685\n",
      "Epoch [72/100], Step [63/84], Loss: 0.1462\n",
      "Epoch [72/100], Step [64/84], Loss: 0.1577\n",
      "Epoch [72/100], Step [65/84], Loss: 0.1605\n",
      "Epoch [72/100], Step [66/84], Loss: 0.1675\n",
      "Epoch [72/100], Step [67/84], Loss: 0.1742\n",
      "Epoch [72/100], Step [68/84], Loss: 0.1657\n",
      "Epoch [72/100], Step [69/84], Loss: 0.1746\n",
      "Epoch [72/100], Step [70/84], Loss: 0.1424\n",
      "Epoch [72/100], Step [71/84], Loss: 0.1781\n",
      "Epoch [72/100], Step [72/84], Loss: 0.1865\n",
      "Epoch [72/100], Step [73/84], Loss: 0.1492\n",
      "Epoch [72/100], Step [74/84], Loss: 0.1784\n",
      "Epoch [72/100], Step [75/84], Loss: 0.1461\n",
      "Epoch [72/100], Step [76/84], Loss: 0.1762\n",
      "Epoch [72/100], Step [77/84], Loss: 0.1655\n",
      "Epoch [72/100], Step [78/84], Loss: 0.1608\n",
      "Epoch [72/100], Step [79/84], Loss: 0.1758\n",
      "Epoch [72/100], Step [80/84], Loss: 0.1591\n",
      "Epoch [72/100], Step [81/84], Loss: 0.1644\n",
      "Epoch [72/100], Step [82/84], Loss: 0.1530\n",
      "Epoch [72/100], Step [83/84], Loss: 0.1541\n",
      "Epoch [72/100], Step [84/84], Loss: 0.1344\n",
      "Epoch [73/100], Step [1/84], Loss: 0.1373\n",
      "Epoch [73/100], Step [2/84], Loss: 0.1662\n",
      "Epoch [73/100], Step [3/84], Loss: 0.1428\n",
      "Epoch [73/100], Step [4/84], Loss: 0.1806\n",
      "Epoch [73/100], Step [5/84], Loss: 0.1558\n",
      "Epoch [73/100], Step [6/84], Loss: 0.1555\n",
      "Epoch [73/100], Step [7/84], Loss: 0.1576\n",
      "Epoch [73/100], Step [8/84], Loss: 0.1461\n",
      "Epoch [73/100], Step [9/84], Loss: 0.1436\n",
      "Epoch [73/100], Step [10/84], Loss: 0.1592\n",
      "Epoch [73/100], Step [11/84], Loss: 0.1550\n",
      "Epoch [73/100], Step [12/84], Loss: 0.1556\n",
      "Epoch [73/100], Step [13/84], Loss: 0.1366\n",
      "Epoch [73/100], Step [14/84], Loss: 0.1470\n",
      "Epoch [73/100], Step [15/84], Loss: 0.1471\n",
      "Epoch [73/100], Step [16/84], Loss: 0.1431\n",
      "Epoch [73/100], Step [17/84], Loss: 0.1378\n",
      "Epoch [73/100], Step [18/84], Loss: 0.1281\n",
      "Epoch [73/100], Step [19/84], Loss: 0.1377\n",
      "Epoch [73/100], Step [20/84], Loss: 0.1628\n",
      "Epoch [73/100], Step [21/84], Loss: 0.1431\n",
      "Epoch [73/100], Step [22/84], Loss: 0.1370\n",
      "Epoch [73/100], Step [23/84], Loss: 0.1552\n",
      "Epoch [73/100], Step [24/84], Loss: 0.1608\n",
      "Epoch [73/100], Step [25/84], Loss: 0.1625\n",
      "Epoch [73/100], Step [26/84], Loss: 0.1416\n",
      "Epoch [73/100], Step [27/84], Loss: 0.1918\n",
      "Epoch [73/100], Step [28/84], Loss: 0.1582\n",
      "Epoch [73/100], Step [29/84], Loss: 0.1744\n",
      "Epoch [73/100], Step [30/84], Loss: 0.1824\n",
      "Epoch [73/100], Step [31/84], Loss: 0.1553\n",
      "Epoch [73/100], Step [32/84], Loss: 0.1744\n",
      "Epoch [73/100], Step [33/84], Loss: 0.1496\n",
      "Epoch [73/100], Step [34/84], Loss: 0.1790\n",
      "Epoch [73/100], Step [35/84], Loss: 0.1408\n",
      "Epoch [73/100], Step [36/84], Loss: 0.1626\n",
      "Epoch [73/100], Step [37/84], Loss: 0.1400\n",
      "Epoch [73/100], Step [38/84], Loss: 0.1583\n",
      "Epoch [73/100], Step [39/84], Loss: 0.1736\n",
      "Epoch [73/100], Step [40/84], Loss: 0.1582\n",
      "Epoch [73/100], Step [41/84], Loss: 0.1796\n",
      "Epoch [73/100], Step [42/84], Loss: 0.1466\n",
      "Epoch [73/100], Step [43/84], Loss: 0.1443\n",
      "Epoch [73/100], Step [44/84], Loss: 0.1723\n",
      "Epoch [73/100], Step [45/84], Loss: 0.1519\n",
      "Epoch [73/100], Step [46/84], Loss: 0.1471\n",
      "Epoch [73/100], Step [47/84], Loss: 0.1492\n",
      "Epoch [73/100], Step [48/84], Loss: 0.1557\n",
      "Epoch [73/100], Step [49/84], Loss: 0.1471\n",
      "Epoch [73/100], Step [50/84], Loss: 0.1442\n",
      "Epoch [73/100], Step [51/84], Loss: 0.1497\n",
      "Epoch [73/100], Step [52/84], Loss: 0.1390\n",
      "Epoch [73/100], Step [53/84], Loss: 0.1627\n",
      "Epoch [73/100], Step [54/84], Loss: 0.1517\n",
      "Epoch [73/100], Step [55/84], Loss: 0.1475\n",
      "Epoch [73/100], Step [56/84], Loss: 0.1595\n",
      "Epoch [73/100], Step [57/84], Loss: 0.1511\n",
      "Epoch [73/100], Step [58/84], Loss: 0.1473\n",
      "Epoch [73/100], Step [59/84], Loss: 0.1700\n",
      "Epoch [73/100], Step [60/84], Loss: 0.1890\n",
      "Epoch [73/100], Step [61/84], Loss: 0.1577\n",
      "Epoch [73/100], Step [62/84], Loss: 0.1575\n",
      "Epoch [73/100], Step [63/84], Loss: 0.1490\n",
      "Epoch [73/100], Step [64/84], Loss: 0.1363\n",
      "Epoch [73/100], Step [65/84], Loss: 0.1538\n",
      "Epoch [73/100], Step [66/84], Loss: 0.1777\n",
      "Epoch [73/100], Step [67/84], Loss: 0.1464\n",
      "Epoch [73/100], Step [68/84], Loss: 0.1453\n",
      "Epoch [73/100], Step [69/84], Loss: 0.1721\n",
      "Epoch [73/100], Step [70/84], Loss: 0.1495\n",
      "Epoch [73/100], Step [71/84], Loss: 0.1388\n",
      "Epoch [73/100], Step [72/84], Loss: 0.1653\n",
      "Epoch [73/100], Step [73/84], Loss: 0.1460\n",
      "Epoch [73/100], Step [74/84], Loss: 0.1868\n",
      "Epoch [73/100], Step [75/84], Loss: 0.1536\n",
      "Epoch [73/100], Step [76/84], Loss: 0.1535\n",
      "Epoch [73/100], Step [77/84], Loss: 0.1506\n",
      "Epoch [73/100], Step [78/84], Loss: 0.1494\n",
      "Epoch [73/100], Step [79/84], Loss: 0.1741\n",
      "Epoch [73/100], Step [80/84], Loss: 0.1534\n",
      "Epoch [73/100], Step [81/84], Loss: 0.1499\n",
      "Epoch [73/100], Step [82/84], Loss: 0.1755\n",
      "Epoch [73/100], Step [83/84], Loss: 0.1475\n",
      "Epoch [73/100], Step [84/84], Loss: 0.1516\n",
      "Epoch [74/100], Step [1/84], Loss: 0.1724\n",
      "Epoch [74/100], Step [2/84], Loss: 0.1599\n",
      "Epoch [74/100], Step [3/84], Loss: 0.1446\n",
      "Epoch [74/100], Step [4/84], Loss: 0.1976\n",
      "Epoch [74/100], Step [5/84], Loss: 0.1530\n",
      "Epoch [74/100], Step [6/84], Loss: 0.1459\n",
      "Epoch [74/100], Step [7/84], Loss: 0.1580\n",
      "Epoch [74/100], Step [8/84], Loss: 0.1659\n",
      "Epoch [74/100], Step [9/84], Loss: 0.1621\n",
      "Epoch [74/100], Step [10/84], Loss: 0.1499\n",
      "Epoch [74/100], Step [11/84], Loss: 0.1770\n",
      "Epoch [74/100], Step [12/84], Loss: 0.1644\n",
      "Epoch [74/100], Step [13/84], Loss: 0.1433\n",
      "Epoch [74/100], Step [14/84], Loss: 0.1684\n",
      "Epoch [74/100], Step [15/84], Loss: 0.1496\n",
      "Epoch [74/100], Step [16/84], Loss: 0.1437\n",
      "Epoch [74/100], Step [17/84], Loss: 0.1558\n",
      "Epoch [74/100], Step [18/84], Loss: 0.1771\n",
      "Epoch [74/100], Step [19/84], Loss: 0.1545\n",
      "Epoch [74/100], Step [20/84], Loss: 0.1402\n",
      "Epoch [74/100], Step [21/84], Loss: 0.1294\n",
      "Epoch [74/100], Step [22/84], Loss: 0.1456\n",
      "Epoch [74/100], Step [23/84], Loss: 0.1516\n",
      "Epoch [74/100], Step [24/84], Loss: 0.1710\n",
      "Epoch [74/100], Step [25/84], Loss: 0.1450\n",
      "Epoch [74/100], Step [26/84], Loss: 0.1536\n",
      "Epoch [74/100], Step [27/84], Loss: 0.1429\n",
      "Epoch [74/100], Step [28/84], Loss: 0.1711\n",
      "Epoch [74/100], Step [29/84], Loss: 0.1548\n",
      "Epoch [74/100], Step [30/84], Loss: 0.1525\n",
      "Epoch [74/100], Step [31/84], Loss: 0.1466\n",
      "Epoch [74/100], Step [32/84], Loss: 0.1371\n",
      "Epoch [74/100], Step [33/84], Loss: 0.1432\n",
      "Epoch [74/100], Step [34/84], Loss: 0.1515\n",
      "Epoch [74/100], Step [35/84], Loss: 0.1610\n",
      "Epoch [74/100], Step [36/84], Loss: 0.1385\n",
      "Epoch [74/100], Step [37/84], Loss: 0.1491\n",
      "Epoch [74/100], Step [38/84], Loss: 0.1537\n",
      "Epoch [74/100], Step [39/84], Loss: 0.1404\n",
      "Epoch [74/100], Step [40/84], Loss: 0.1636\n",
      "Epoch [74/100], Step [41/84], Loss: 0.1503\n",
      "Epoch [74/100], Step [42/84], Loss: 0.1501\n",
      "Epoch [74/100], Step [43/84], Loss: 0.1519\n",
      "Epoch [74/100], Step [44/84], Loss: 0.1352\n",
      "Epoch [74/100], Step [45/84], Loss: 0.1479\n",
      "Epoch [74/100], Step [46/84], Loss: 0.1463\n",
      "Epoch [74/100], Step [47/84], Loss: 0.1574\n",
      "Epoch [74/100], Step [48/84], Loss: 0.1641\n",
      "Epoch [74/100], Step [49/84], Loss: 0.1590\n",
      "Epoch [74/100], Step [50/84], Loss: 0.1486\n",
      "Epoch [74/100], Step [51/84], Loss: 0.1523\n",
      "Epoch [74/100], Step [52/84], Loss: 0.1393\n",
      "Epoch [74/100], Step [53/84], Loss: 0.1488\n",
      "Epoch [74/100], Step [54/84], Loss: 0.1955\n",
      "Epoch [74/100], Step [55/84], Loss: 0.1658\n",
      "Epoch [74/100], Step [56/84], Loss: 0.1446\n",
      "Epoch [74/100], Step [57/84], Loss: 0.1455\n",
      "Epoch [74/100], Step [58/84], Loss: 0.1412\n",
      "Epoch [74/100], Step [59/84], Loss: 0.1711\n",
      "Epoch [74/100], Step [60/84], Loss: 0.1484\n",
      "Epoch [74/100], Step [61/84], Loss: 0.1422\n",
      "Epoch [74/100], Step [62/84], Loss: 0.1704\n",
      "Epoch [74/100], Step [63/84], Loss: 0.1487\n",
      "Epoch [74/100], Step [64/84], Loss: 0.1554\n",
      "Epoch [74/100], Step [65/84], Loss: 0.1384\n",
      "Epoch [74/100], Step [66/84], Loss: 0.1400\n",
      "Epoch [74/100], Step [67/84], Loss: 0.1615\n",
      "Epoch [74/100], Step [68/84], Loss: 0.1639\n",
      "Epoch [74/100], Step [69/84], Loss: 0.1461\n",
      "Epoch [74/100], Step [70/84], Loss: 0.1330\n",
      "Epoch [74/100], Step [71/84], Loss: 0.1620\n",
      "Epoch [74/100], Step [72/84], Loss: 0.1539\n",
      "Epoch [74/100], Step [73/84], Loss: 0.1527\n",
      "Epoch [74/100], Step [74/84], Loss: 0.1466\n",
      "Epoch [74/100], Step [75/84], Loss: 0.1332\n",
      "Epoch [74/100], Step [76/84], Loss: 0.1366\n",
      "Epoch [74/100], Step [77/84], Loss: 0.1753\n",
      "Epoch [74/100], Step [78/84], Loss: 0.1377\n",
      "Epoch [74/100], Step [79/84], Loss: 0.1399\n",
      "Epoch [74/100], Step [80/84], Loss: 0.1470\n",
      "Epoch [74/100], Step [81/84], Loss: 0.1481\n",
      "Epoch [74/100], Step [82/84], Loss: 0.1416\n",
      "Epoch [74/100], Step [83/84], Loss: 0.1557\n",
      "Epoch [74/100], Step [84/84], Loss: 0.1659\n",
      "Epoch [75/100], Step [1/84], Loss: 0.1661\n",
      "Epoch [75/100], Step [2/84], Loss: 0.1511\n",
      "Epoch [75/100], Step [3/84], Loss: 0.1424\n",
      "Epoch [75/100], Step [4/84], Loss: 0.1436\n",
      "Epoch [75/100], Step [5/84], Loss: 0.1406\n",
      "Epoch [75/100], Step [6/84], Loss: 0.1340\n",
      "Epoch [75/100], Step [7/84], Loss: 0.1413\n",
      "Epoch [75/100], Step [8/84], Loss: 0.1421\n",
      "Epoch [75/100], Step [9/84], Loss: 0.1368\n",
      "Epoch [75/100], Step [10/84], Loss: 0.1395\n",
      "Epoch [75/100], Step [11/84], Loss: 0.1390\n",
      "Epoch [75/100], Step [12/84], Loss: 0.1483\n",
      "Epoch [75/100], Step [13/84], Loss: 0.1484\n",
      "Epoch [75/100], Step [14/84], Loss: 0.1362\n",
      "Epoch [75/100], Step [15/84], Loss: 0.1571\n",
      "Epoch [75/100], Step [16/84], Loss: 0.1372\n",
      "Epoch [75/100], Step [17/84], Loss: 0.1586\n",
      "Epoch [75/100], Step [18/84], Loss: 0.1582\n",
      "Epoch [75/100], Step [19/84], Loss: 0.1420\n",
      "Epoch [75/100], Step [20/84], Loss: 0.1541\n",
      "Epoch [75/100], Step [21/84], Loss: 0.1403\n",
      "Epoch [75/100], Step [22/84], Loss: 0.1376\n",
      "Epoch [75/100], Step [23/84], Loss: 0.1621\n",
      "Epoch [75/100], Step [24/84], Loss: 0.1409\n",
      "Epoch [75/100], Step [25/84], Loss: 0.1505\n",
      "Epoch [75/100], Step [26/84], Loss: 0.1588\n",
      "Epoch [75/100], Step [27/84], Loss: 0.1519\n",
      "Epoch [75/100], Step [28/84], Loss: 0.1426\n",
      "Epoch [75/100], Step [29/84], Loss: 0.1349\n",
      "Epoch [75/100], Step [30/84], Loss: 0.1619\n",
      "Epoch [75/100], Step [31/84], Loss: 0.1580\n",
      "Epoch [75/100], Step [32/84], Loss: 0.1385\n",
      "Epoch [75/100], Step [33/84], Loss: 0.1737\n",
      "Epoch [75/100], Step [34/84], Loss: 0.1381\n",
      "Epoch [75/100], Step [35/84], Loss: 0.1542\n",
      "Epoch [75/100], Step [36/84], Loss: 0.1480\n",
      "Epoch [75/100], Step [37/84], Loss: 0.1406\n",
      "Epoch [75/100], Step [38/84], Loss: 0.1459\n",
      "Epoch [75/100], Step [39/84], Loss: 0.1511\n",
      "Epoch [75/100], Step [40/84], Loss: 0.1276\n",
      "Epoch [75/100], Step [41/84], Loss: 0.1374\n",
      "Epoch [75/100], Step [42/84], Loss: 0.1320\n",
      "Epoch [75/100], Step [43/84], Loss: 0.1755\n",
      "Epoch [75/100], Step [44/84], Loss: 0.1570\n",
      "Epoch [75/100], Step [45/84], Loss: 0.1577\n",
      "Epoch [75/100], Step [46/84], Loss: 0.1398\n",
      "Epoch [75/100], Step [47/84], Loss: 0.1477\n",
      "Epoch [75/100], Step [48/84], Loss: 0.1531\n",
      "Epoch [75/100], Step [49/84], Loss: 0.1636\n",
      "Epoch [75/100], Step [50/84], Loss: 0.1568\n",
      "Epoch [75/100], Step [51/84], Loss: 0.1308\n",
      "Epoch [75/100], Step [52/84], Loss: 0.1358\n",
      "Epoch [75/100], Step [53/84], Loss: 0.1535\n",
      "Epoch [75/100], Step [54/84], Loss: 0.1762\n",
      "Epoch [75/100], Step [55/84], Loss: 0.1300\n",
      "Epoch [75/100], Step [56/84], Loss: 0.1426\n",
      "Epoch [75/100], Step [57/84], Loss: 0.1340\n",
      "Epoch [75/100], Step [58/84], Loss: 0.1544\n",
      "Epoch [75/100], Step [59/84], Loss: 0.2195\n",
      "Epoch [75/100], Step [60/84], Loss: 0.1373\n",
      "Epoch [75/100], Step [61/84], Loss: 0.1418\n",
      "Epoch [75/100], Step [62/84], Loss: 0.1487\n",
      "Epoch [75/100], Step [63/84], Loss: 0.1389\n",
      "Epoch [75/100], Step [64/84], Loss: 0.1398\n",
      "Epoch [75/100], Step [65/84], Loss: 0.1363\n",
      "Epoch [75/100], Step [66/84], Loss: 0.1275\n",
      "Epoch [75/100], Step [67/84], Loss: 0.1481\n",
      "Epoch [75/100], Step [68/84], Loss: 0.1652\n",
      "Epoch [75/100], Step [69/84], Loss: 0.1563\n",
      "Epoch [75/100], Step [70/84], Loss: 0.1581\n",
      "Epoch [75/100], Step [71/84], Loss: 0.1501\n",
      "Epoch [75/100], Step [72/84], Loss: 0.1525\n",
      "Epoch [75/100], Step [73/84], Loss: 0.1367\n",
      "Epoch [75/100], Step [74/84], Loss: 0.1417\n",
      "Epoch [75/100], Step [75/84], Loss: 0.1413\n",
      "Epoch [75/100], Step [76/84], Loss: 0.1374\n",
      "Epoch [75/100], Step [77/84], Loss: 0.1587\n",
      "Epoch [75/100], Step [78/84], Loss: 0.1657\n",
      "Epoch [75/100], Step [79/84], Loss: 0.1662\n",
      "Epoch [75/100], Step [80/84], Loss: 0.1627\n",
      "Epoch [75/100], Step [81/84], Loss: 0.1541\n",
      "Epoch [75/100], Step [82/84], Loss: 0.1548\n",
      "Epoch [75/100], Step [83/84], Loss: 0.1659\n",
      "Epoch [75/100], Step [84/84], Loss: 0.2324\n",
      "Epoch [76/100], Step [1/84], Loss: 0.1469\n",
      "Epoch [76/100], Step [2/84], Loss: 0.1574\n",
      "Epoch [76/100], Step [3/84], Loss: 0.1701\n",
      "Epoch [76/100], Step [4/84], Loss: 0.1459\n",
      "Epoch [76/100], Step [5/84], Loss: 0.1483\n",
      "Epoch [76/100], Step [6/84], Loss: 0.1535\n",
      "Epoch [76/100], Step [7/84], Loss: 0.1533\n",
      "Epoch [76/100], Step [8/84], Loss: 0.1526\n",
      "Epoch [76/100], Step [9/84], Loss: 0.1417\n",
      "Epoch [76/100], Step [10/84], Loss: 0.1570\n",
      "Epoch [76/100], Step [11/84], Loss: 0.1612\n",
      "Epoch [76/100], Step [12/84], Loss: 0.1677\n",
      "Epoch [76/100], Step [13/84], Loss: 0.1578\n",
      "Epoch [76/100], Step [14/84], Loss: 0.1506\n",
      "Epoch [76/100], Step [15/84], Loss: 0.1718\n",
      "Epoch [76/100], Step [16/84], Loss: 0.1431\n",
      "Epoch [76/100], Step [17/84], Loss: 0.1561\n",
      "Epoch [76/100], Step [18/84], Loss: 0.1620\n",
      "Epoch [76/100], Step [19/84], Loss: 0.1587\n",
      "Epoch [76/100], Step [20/84], Loss: 0.1439\n",
      "Epoch [76/100], Step [21/84], Loss: 0.1500\n",
      "Epoch [76/100], Step [22/84], Loss: 0.1458\n",
      "Epoch [76/100], Step [23/84], Loss: 0.1459\n",
      "Epoch [76/100], Step [24/84], Loss: 0.1669\n",
      "Epoch [76/100], Step [25/84], Loss: 0.1453\n",
      "Epoch [76/100], Step [26/84], Loss: 0.1308\n",
      "Epoch [76/100], Step [27/84], Loss: 0.1448\n",
      "Epoch [76/100], Step [28/84], Loss: 0.1373\n",
      "Epoch [76/100], Step [29/84], Loss: 0.1440\n",
      "Epoch [76/100], Step [30/84], Loss: 0.1315\n",
      "Epoch [76/100], Step [31/84], Loss: 0.1453\n",
      "Epoch [76/100], Step [32/84], Loss: 0.1625\n",
      "Epoch [76/100], Step [33/84], Loss: 0.1500\n",
      "Epoch [76/100], Step [34/84], Loss: 0.1430\n",
      "Epoch [76/100], Step [35/84], Loss: 0.1482\n",
      "Epoch [76/100], Step [36/84], Loss: 0.1396\n",
      "Epoch [76/100], Step [37/84], Loss: 0.1620\n",
      "Epoch [76/100], Step [38/84], Loss: 0.1621\n",
      "Epoch [76/100], Step [39/84], Loss: 0.1412\n",
      "Epoch [76/100], Step [40/84], Loss: 0.1414\n",
      "Epoch [76/100], Step [41/84], Loss: 0.1302\n",
      "Epoch [76/100], Step [42/84], Loss: 0.1384\n",
      "Epoch [76/100], Step [43/84], Loss: 0.1527\n",
      "Epoch [76/100], Step [44/84], Loss: 0.1701\n",
      "Epoch [76/100], Step [45/84], Loss: 0.1741\n",
      "Epoch [76/100], Step [46/84], Loss: 0.1525\n",
      "Epoch [76/100], Step [47/84], Loss: 0.1443\n",
      "Epoch [76/100], Step [48/84], Loss: 0.1359\n",
      "Epoch [76/100], Step [49/84], Loss: 0.1527\n",
      "Epoch [76/100], Step [50/84], Loss: 0.1519\n",
      "Epoch [76/100], Step [51/84], Loss: 0.1351\n",
      "Epoch [76/100], Step [52/84], Loss: 0.1438\n",
      "Epoch [76/100], Step [53/84], Loss: 0.1481\n",
      "Epoch [76/100], Step [54/84], Loss: 0.1430\n",
      "Epoch [76/100], Step [55/84], Loss: 0.1700\n",
      "Epoch [76/100], Step [56/84], Loss: 0.1707\n",
      "Epoch [76/100], Step [57/84], Loss: 0.1526\n",
      "Epoch [76/100], Step [58/84], Loss: 0.1341\n",
      "Epoch [76/100], Step [59/84], Loss: 0.1550\n",
      "Epoch [76/100], Step [60/84], Loss: 0.1634\n",
      "Epoch [76/100], Step [61/84], Loss: 0.1592\n",
      "Epoch [76/100], Step [62/84], Loss: 0.1492\n",
      "Epoch [76/100], Step [63/84], Loss: 0.1613\n",
      "Epoch [76/100], Step [64/84], Loss: 0.1501\n",
      "Epoch [76/100], Step [65/84], Loss: 0.1455\n",
      "Epoch [76/100], Step [66/84], Loss: 0.1596\n",
      "Epoch [76/100], Step [67/84], Loss: 0.1476\n",
      "Epoch [76/100], Step [68/84], Loss: 0.1375\n",
      "Epoch [76/100], Step [69/84], Loss: 0.1494\n",
      "Epoch [76/100], Step [70/84], Loss: 0.1639\n",
      "Epoch [76/100], Step [71/84], Loss: 0.1689\n",
      "Epoch [76/100], Step [72/84], Loss: 0.1574\n",
      "Epoch [76/100], Step [73/84], Loss: 0.1681\n",
      "Epoch [76/100], Step [74/84], Loss: 0.1596\n",
      "Epoch [76/100], Step [75/84], Loss: 0.1445\n",
      "Epoch [76/100], Step [76/84], Loss: 0.1994\n",
      "Epoch [76/100], Step [77/84], Loss: 0.1373\n",
      "Epoch [76/100], Step [78/84], Loss: 0.1334\n",
      "Epoch [76/100], Step [79/84], Loss: 0.1536\n",
      "Epoch [76/100], Step [80/84], Loss: 0.1303\n",
      "Epoch [76/100], Step [81/84], Loss: 0.1398\n",
      "Epoch [76/100], Step [82/84], Loss: 0.1364\n",
      "Epoch [76/100], Step [83/84], Loss: 0.1613\n",
      "Epoch [76/100], Step [84/84], Loss: 0.1831\n",
      "Epoch [77/100], Step [1/84], Loss: 0.1371\n",
      "Epoch [77/100], Step [2/84], Loss: 0.1598\n",
      "Epoch [77/100], Step [3/84], Loss: 0.1518\n",
      "Epoch [77/100], Step [4/84], Loss: 0.1498\n",
      "Epoch [77/100], Step [5/84], Loss: 0.1468\n",
      "Epoch [77/100], Step [6/84], Loss: 0.1445\n",
      "Epoch [77/100], Step [7/84], Loss: 0.1529\n",
      "Epoch [77/100], Step [8/84], Loss: 0.1386\n",
      "Epoch [77/100], Step [9/84], Loss: 0.1391\n",
      "Epoch [77/100], Step [10/84], Loss: 0.1452\n",
      "Epoch [77/100], Step [11/84], Loss: 0.1268\n",
      "Epoch [77/100], Step [12/84], Loss: 0.1443\n",
      "Epoch [77/100], Step [13/84], Loss: 0.1299\n",
      "Epoch [77/100], Step [14/84], Loss: 0.1479\n",
      "Epoch [77/100], Step [15/84], Loss: 0.1921\n",
      "Epoch [77/100], Step [16/84], Loss: 0.1386\n",
      "Epoch [77/100], Step [17/84], Loss: 0.1695\n",
      "Epoch [77/100], Step [18/84], Loss: 0.1367\n",
      "Epoch [77/100], Step [19/84], Loss: 0.1500\n",
      "Epoch [77/100], Step [20/84], Loss: 0.1605\n",
      "Epoch [77/100], Step [21/84], Loss: 0.1359\n",
      "Epoch [77/100], Step [22/84], Loss: 0.1381\n",
      "Epoch [77/100], Step [23/84], Loss: 0.1495\n",
      "Epoch [77/100], Step [24/84], Loss: 0.1485\n",
      "Epoch [77/100], Step [25/84], Loss: 0.1298\n",
      "Epoch [77/100], Step [26/84], Loss: 0.1427\n",
      "Epoch [77/100], Step [27/84], Loss: 0.1389\n",
      "Epoch [77/100], Step [28/84], Loss: 0.1523\n",
      "Epoch [77/100], Step [29/84], Loss: 0.1586\n",
      "Epoch [77/100], Step [30/84], Loss: 0.1549\n",
      "Epoch [77/100], Step [31/84], Loss: 0.1463\n",
      "Epoch [77/100], Step [32/84], Loss: 0.1564\n",
      "Epoch [77/100], Step [33/84], Loss: 0.1633\n",
      "Epoch [77/100], Step [34/84], Loss: 0.1477\n",
      "Epoch [77/100], Step [35/84], Loss: 0.1611\n",
      "Epoch [77/100], Step [36/84], Loss: 0.1744\n",
      "Epoch [77/100], Step [37/84], Loss: 0.1491\n",
      "Epoch [77/100], Step [38/84], Loss: 0.1526\n",
      "Epoch [77/100], Step [39/84], Loss: 0.1566\n",
      "Epoch [77/100], Step [40/84], Loss: 0.1432\n",
      "Epoch [77/100], Step [41/84], Loss: 0.1510\n",
      "Epoch [77/100], Step [42/84], Loss: 0.1516\n",
      "Epoch [77/100], Step [43/84], Loss: 0.1641\n",
      "Epoch [77/100], Step [44/84], Loss: 0.1527\n",
      "Epoch [77/100], Step [45/84], Loss: 0.1487\n",
      "Epoch [77/100], Step [46/84], Loss: 0.1368\n",
      "Epoch [77/100], Step [47/84], Loss: 0.1326\n",
      "Epoch [77/100], Step [48/84], Loss: 0.1431\n",
      "Epoch [77/100], Step [49/84], Loss: 0.1430\n",
      "Epoch [77/100], Step [50/84], Loss: 0.1513\n",
      "Epoch [77/100], Step [51/84], Loss: 0.1577\n",
      "Epoch [77/100], Step [52/84], Loss: 0.1569\n",
      "Epoch [77/100], Step [53/84], Loss: 0.1461\n",
      "Epoch [77/100], Step [54/84], Loss: 0.1347\n",
      "Epoch [77/100], Step [55/84], Loss: 0.1431\n",
      "Epoch [77/100], Step [56/84], Loss: 0.1384\n",
      "Epoch [77/100], Step [57/84], Loss: 0.1410\n",
      "Epoch [77/100], Step [58/84], Loss: 0.1624\n",
      "Epoch [77/100], Step [59/84], Loss: 0.1401\n",
      "Epoch [77/100], Step [60/84], Loss: 0.1443\n",
      "Epoch [77/100], Step [61/84], Loss: 0.1623\n",
      "Epoch [77/100], Step [62/84], Loss: 0.1224\n",
      "Epoch [77/100], Step [63/84], Loss: 0.1343\n",
      "Epoch [77/100], Step [64/84], Loss: 0.1307\n",
      "Epoch [77/100], Step [65/84], Loss: 0.1386\n",
      "Epoch [77/100], Step [66/84], Loss: 0.1467\n",
      "Epoch [77/100], Step [67/84], Loss: 0.1325\n",
      "Epoch [77/100], Step [68/84], Loss: 0.1400\n",
      "Epoch [77/100], Step [69/84], Loss: 0.1543\n",
      "Epoch [77/100], Step [70/84], Loss: 0.1382\n",
      "Epoch [77/100], Step [71/84], Loss: 0.1344\n",
      "Epoch [77/100], Step [72/84], Loss: 0.1409\n",
      "Epoch [77/100], Step [73/84], Loss: 0.1336\n",
      "Epoch [77/100], Step [74/84], Loss: 0.1645\n",
      "Epoch [77/100], Step [75/84], Loss: 0.1364\n",
      "Epoch [77/100], Step [76/84], Loss: 0.1367\n",
      "Epoch [77/100], Step [77/84], Loss: 0.1504\n",
      "Epoch [77/100], Step [78/84], Loss: 0.1590\n",
      "Epoch [77/100], Step [79/84], Loss: 0.1398\n",
      "Epoch [77/100], Step [80/84], Loss: 0.1527\n",
      "Epoch [77/100], Step [81/84], Loss: 0.1466\n",
      "Epoch [77/100], Step [82/84], Loss: 0.1524\n",
      "Epoch [77/100], Step [83/84], Loss: 0.1605\n",
      "Epoch [77/100], Step [84/84], Loss: 0.1382\n",
      "Epoch [78/100], Step [1/84], Loss: 0.1280\n",
      "Epoch [78/100], Step [2/84], Loss: 0.1411\n",
      "Epoch [78/100], Step [3/84], Loss: 0.1420\n",
      "Epoch [78/100], Step [4/84], Loss: 0.1469\n",
      "Epoch [78/100], Step [5/84], Loss: 0.1540\n",
      "Epoch [78/100], Step [6/84], Loss: 0.1455\n",
      "Epoch [78/100], Step [7/84], Loss: 0.1458\n",
      "Epoch [78/100], Step [8/84], Loss: 0.1644\n",
      "Epoch [78/100], Step [9/84], Loss: 0.1618\n",
      "Epoch [78/100], Step [10/84], Loss: 0.1327\n",
      "Epoch [78/100], Step [11/84], Loss: 0.1350\n",
      "Epoch [78/100], Step [12/84], Loss: 0.1451\n",
      "Epoch [78/100], Step [13/84], Loss: 0.1413\n",
      "Epoch [78/100], Step [14/84], Loss: 0.1509\n",
      "Epoch [78/100], Step [15/84], Loss: 0.1425\n",
      "Epoch [78/100], Step [16/84], Loss: 0.1242\n",
      "Epoch [78/100], Step [17/84], Loss: 0.1355\n",
      "Epoch [78/100], Step [18/84], Loss: 0.1604\n",
      "Epoch [78/100], Step [19/84], Loss: 0.1446\n",
      "Epoch [78/100], Step [20/84], Loss: 0.1340\n",
      "Epoch [78/100], Step [21/84], Loss: 0.1348\n",
      "Epoch [78/100], Step [22/84], Loss: 0.1459\n",
      "Epoch [78/100], Step [23/84], Loss: 0.1501\n",
      "Epoch [78/100], Step [24/84], Loss: 0.1483\n",
      "Epoch [78/100], Step [25/84], Loss: 0.1426\n",
      "Epoch [78/100], Step [26/84], Loss: 0.1437\n",
      "Epoch [78/100], Step [27/84], Loss: 0.1337\n",
      "Epoch [78/100], Step [28/84], Loss: 0.1330\n",
      "Epoch [78/100], Step [29/84], Loss: 0.1301\n",
      "Epoch [78/100], Step [30/84], Loss: 0.1299\n",
      "Epoch [78/100], Step [31/84], Loss: 0.1320\n",
      "Epoch [78/100], Step [32/84], Loss: 0.1436\n",
      "Epoch [78/100], Step [33/84], Loss: 0.1647\n",
      "Epoch [78/100], Step [34/84], Loss: 0.1223\n",
      "Epoch [78/100], Step [35/84], Loss: 0.1311\n",
      "Epoch [78/100], Step [36/84], Loss: 0.1405\n",
      "Epoch [78/100], Step [37/84], Loss: 0.1336\n",
      "Epoch [78/100], Step [38/84], Loss: 0.1393\n",
      "Epoch [78/100], Step [39/84], Loss: 0.1366\n",
      "Epoch [78/100], Step [40/84], Loss: 0.1341\n",
      "Epoch [78/100], Step [41/84], Loss: 0.1352\n",
      "Epoch [78/100], Step [42/84], Loss: 0.1558\n",
      "Epoch [78/100], Step [43/84], Loss: 0.1335\n",
      "Epoch [78/100], Step [44/84], Loss: 0.1359\n",
      "Epoch [78/100], Step [45/84], Loss: 0.1876\n",
      "Epoch [78/100], Step [46/84], Loss: 0.1437\n",
      "Epoch [78/100], Step [47/84], Loss: 0.1496\n",
      "Epoch [78/100], Step [48/84], Loss: 0.1329\n",
      "Epoch [78/100], Step [49/84], Loss: 0.1352\n",
      "Epoch [78/100], Step [50/84], Loss: 0.1390\n",
      "Epoch [78/100], Step [51/84], Loss: 0.1562\n",
      "Epoch [78/100], Step [52/84], Loss: 0.1515\n",
      "Epoch [78/100], Step [53/84], Loss: 0.1262\n",
      "Epoch [78/100], Step [54/84], Loss: 0.1403\n",
      "Epoch [78/100], Step [55/84], Loss: 0.1395\n",
      "Epoch [78/100], Step [56/84], Loss: 0.1514\n",
      "Epoch [78/100], Step [57/84], Loss: 0.1548\n",
      "Epoch [78/100], Step [58/84], Loss: 0.1448\n",
      "Epoch [78/100], Step [59/84], Loss: 0.1422\n",
      "Epoch [78/100], Step [60/84], Loss: 0.1263\n",
      "Epoch [78/100], Step [61/84], Loss: 0.1393\n",
      "Epoch [78/100], Step [62/84], Loss: 0.1335\n",
      "Epoch [78/100], Step [63/84], Loss: 0.1446\n",
      "Epoch [78/100], Step [64/84], Loss: 0.1495\n",
      "Epoch [78/100], Step [65/84], Loss: 0.1369\n",
      "Epoch [78/100], Step [66/84], Loss: 0.1454\n",
      "Epoch [78/100], Step [67/84], Loss: 0.1480\n",
      "Epoch [78/100], Step [68/84], Loss: 0.1270\n",
      "Epoch [78/100], Step [69/84], Loss: 0.2083\n",
      "Epoch [78/100], Step [70/84], Loss: 0.1660\n",
      "Epoch [78/100], Step [71/84], Loss: 0.1327\n",
      "Epoch [78/100], Step [72/84], Loss: 0.1494\n",
      "Epoch [78/100], Step [73/84], Loss: 0.1421\n",
      "Epoch [78/100], Step [74/84], Loss: 0.1143\n",
      "Epoch [78/100], Step [75/84], Loss: 0.1369\n",
      "Epoch [78/100], Step [76/84], Loss: 0.1520\n",
      "Epoch [78/100], Step [77/84], Loss: 0.1474\n",
      "Epoch [78/100], Step [78/84], Loss: 0.1387\n",
      "Epoch [78/100], Step [79/84], Loss: 0.1288\n",
      "Epoch [78/100], Step [80/84], Loss: 0.1432\n",
      "Epoch [78/100], Step [81/84], Loss: 0.1528\n",
      "Epoch [78/100], Step [82/84], Loss: 0.1599\n",
      "Epoch [78/100], Step [83/84], Loss: 0.1477\n",
      "Epoch [78/100], Step [84/84], Loss: 0.1457\n",
      "Epoch [79/100], Step [1/84], Loss: 0.1370\n",
      "Epoch [79/100], Step [2/84], Loss: 0.1454\n",
      "Epoch [79/100], Step [3/84], Loss: 0.1373\n",
      "Epoch [79/100], Step [4/84], Loss: 0.1471\n",
      "Epoch [79/100], Step [5/84], Loss: 0.1580\n",
      "Epoch [79/100], Step [6/84], Loss: 0.1483\n",
      "Epoch [79/100], Step [7/84], Loss: 0.1492\n",
      "Epoch [79/100], Step [8/84], Loss: 0.1317\n",
      "Epoch [79/100], Step [9/84], Loss: 0.1438\n",
      "Epoch [79/100], Step [10/84], Loss: 0.1562\n",
      "Epoch [79/100], Step [11/84], Loss: 0.1293\n",
      "Epoch [79/100], Step [12/84], Loss: 0.1499\n",
      "Epoch [79/100], Step [13/84], Loss: 0.1415\n",
      "Epoch [79/100], Step [14/84], Loss: 0.1415\n",
      "Epoch [79/100], Step [15/84], Loss: 0.1346\n",
      "Epoch [79/100], Step [16/84], Loss: 0.1647\n",
      "Epoch [79/100], Step [17/84], Loss: 0.1522\n",
      "Epoch [79/100], Step [18/84], Loss: 0.1343\n",
      "Epoch [79/100], Step [19/84], Loss: 0.1287\n",
      "Epoch [79/100], Step [20/84], Loss: 0.1374\n",
      "Epoch [79/100], Step [21/84], Loss: 0.1330\n",
      "Epoch [79/100], Step [22/84], Loss: 0.1295\n",
      "Epoch [79/100], Step [23/84], Loss: 0.1430\n",
      "Epoch [79/100], Step [24/84], Loss: 0.1294\n",
      "Epoch [79/100], Step [25/84], Loss: 0.1334\n",
      "Epoch [79/100], Step [26/84], Loss: 0.1323\n",
      "Epoch [79/100], Step [27/84], Loss: 0.1625\n",
      "Epoch [79/100], Step [28/84], Loss: 0.1344\n",
      "Epoch [79/100], Step [29/84], Loss: 0.1238\n",
      "Epoch [79/100], Step [30/84], Loss: 0.1355\n",
      "Epoch [79/100], Step [31/84], Loss: 0.1320\n",
      "Epoch [79/100], Step [32/84], Loss: 0.1391\n",
      "Epoch [79/100], Step [33/84], Loss: 0.1806\n",
      "Epoch [79/100], Step [34/84], Loss: 0.1470\n",
      "Epoch [79/100], Step [35/84], Loss: 0.1356\n",
      "Epoch [79/100], Step [36/84], Loss: 0.1426\n",
      "Epoch [79/100], Step [37/84], Loss: 0.1528\n",
      "Epoch [79/100], Step [38/84], Loss: 0.1455\n",
      "Epoch [79/100], Step [39/84], Loss: 0.1454\n",
      "Epoch [79/100], Step [40/84], Loss: 0.1394\n",
      "Epoch [79/100], Step [41/84], Loss: 0.1388\n",
      "Epoch [79/100], Step [42/84], Loss: 0.1341\n",
      "Epoch [79/100], Step [43/84], Loss: 0.1383\n",
      "Epoch [79/100], Step [44/84], Loss: 0.1400\n",
      "Epoch [79/100], Step [45/84], Loss: 0.1530\n",
      "Epoch [79/100], Step [46/84], Loss: 0.1389\n",
      "Epoch [79/100], Step [47/84], Loss: 0.1739\n",
      "Epoch [79/100], Step [48/84], Loss: 0.1279\n",
      "Epoch [79/100], Step [49/84], Loss: 0.1361\n",
      "Epoch [79/100], Step [50/84], Loss: 0.1594\n",
      "Epoch [79/100], Step [51/84], Loss: 0.1562\n",
      "Epoch [79/100], Step [52/84], Loss: 0.1442\n",
      "Epoch [79/100], Step [53/84], Loss: 0.1507\n",
      "Epoch [79/100], Step [54/84], Loss: 0.1288\n",
      "Epoch [79/100], Step [55/84], Loss: 0.1270\n",
      "Epoch [79/100], Step [56/84], Loss: 0.1336\n",
      "Epoch [79/100], Step [57/84], Loss: 0.1439\n",
      "Epoch [79/100], Step [58/84], Loss: 0.1405\n",
      "Epoch [79/100], Step [59/84], Loss: 0.1375\n",
      "Epoch [79/100], Step [60/84], Loss: 0.1399\n",
      "Epoch [79/100], Step [61/84], Loss: 0.1379\n",
      "Epoch [79/100], Step [62/84], Loss: 0.1412\n",
      "Epoch [79/100], Step [63/84], Loss: 0.1288\n",
      "Epoch [79/100], Step [64/84], Loss: 0.1340\n",
      "Epoch [79/100], Step [65/84], Loss: 0.1678\n",
      "Epoch [79/100], Step [66/84], Loss: 0.1231\n",
      "Epoch [79/100], Step [67/84], Loss: 0.1375\n",
      "Epoch [79/100], Step [68/84], Loss: 0.1369\n",
      "Epoch [79/100], Step [69/84], Loss: 0.1442\n",
      "Epoch [79/100], Step [70/84], Loss: 0.1384\n",
      "Epoch [79/100], Step [71/84], Loss: 0.1326\n",
      "Epoch [79/100], Step [72/84], Loss: 0.1262\n",
      "Epoch [79/100], Step [73/84], Loss: 0.1249\n",
      "Epoch [79/100], Step [74/84], Loss: 0.1375\n",
      "Epoch [79/100], Step [75/84], Loss: 0.1273\n",
      "Epoch [79/100], Step [76/84], Loss: 0.1358\n",
      "Epoch [79/100], Step [77/84], Loss: 0.1675\n",
      "Epoch [79/100], Step [78/84], Loss: 0.1391\n",
      "Epoch [79/100], Step [79/84], Loss: 0.1584\n",
      "Epoch [79/100], Step [80/84], Loss: 0.1507\n",
      "Epoch [79/100], Step [81/84], Loss: 0.1570\n",
      "Epoch [79/100], Step [82/84], Loss: 0.1697\n",
      "Epoch [79/100], Step [83/84], Loss: 0.1868\n",
      "Epoch [79/100], Step [84/84], Loss: 0.1386\n",
      "Epoch [80/100], Step [1/84], Loss: 0.1292\n",
      "Epoch [80/100], Step [2/84], Loss: 0.1251\n",
      "Epoch [80/100], Step [3/84], Loss: 0.1419\n",
      "Epoch [80/100], Step [4/84], Loss: 0.1385\n",
      "Epoch [80/100], Step [5/84], Loss: 0.1338\n",
      "Epoch [80/100], Step [6/84], Loss: 0.1411\n",
      "Epoch [80/100], Step [7/84], Loss: 0.1384\n",
      "Epoch [80/100], Step [8/84], Loss: 0.1547\n",
      "Epoch [80/100], Step [9/84], Loss: 0.1515\n",
      "Epoch [80/100], Step [10/84], Loss: 0.1172\n",
      "Epoch [80/100], Step [11/84], Loss: 0.1529\n",
      "Epoch [80/100], Step [12/84], Loss: 0.1318\n",
      "Epoch [80/100], Step [13/84], Loss: 0.1425\n",
      "Epoch [80/100], Step [14/84], Loss: 0.1264\n",
      "Epoch [80/100], Step [15/84], Loss: 0.1434\n",
      "Epoch [80/100], Step [16/84], Loss: 0.1305\n",
      "Epoch [80/100], Step [17/84], Loss: 0.1271\n",
      "Epoch [80/100], Step [18/84], Loss: 0.1366\n",
      "Epoch [80/100], Step [19/84], Loss: 0.1454\n",
      "Epoch [80/100], Step [20/84], Loss: 0.1435\n",
      "Epoch [80/100], Step [21/84], Loss: 0.1358\n",
      "Epoch [80/100], Step [22/84], Loss: 0.1115\n",
      "Epoch [80/100], Step [23/84], Loss: 0.1471\n",
      "Epoch [80/100], Step [24/84], Loss: 0.1559\n",
      "Epoch [80/100], Step [25/84], Loss: 0.1647\n",
      "Epoch [80/100], Step [26/84], Loss: 0.1390\n",
      "Epoch [80/100], Step [27/84], Loss: 0.1471\n",
      "Epoch [80/100], Step [28/84], Loss: 0.1384\n",
      "Epoch [80/100], Step [29/84], Loss: 0.1526\n",
      "Epoch [80/100], Step [30/84], Loss: 0.1404\n",
      "Epoch [80/100], Step [31/84], Loss: 0.1454\n",
      "Epoch [80/100], Step [32/84], Loss: 0.1546\n",
      "Epoch [80/100], Step [33/84], Loss: 0.1231\n",
      "Epoch [80/100], Step [34/84], Loss: 0.1453\n",
      "Epoch [80/100], Step [35/84], Loss: 0.1529\n",
      "Epoch [80/100], Step [36/84], Loss: 0.1368\n",
      "Epoch [80/100], Step [37/84], Loss: 0.1409\n",
      "Epoch [80/100], Step [38/84], Loss: 0.1281\n",
      "Epoch [80/100], Step [39/84], Loss: 0.1473\n",
      "Epoch [80/100], Step [40/84], Loss: 0.1402\n",
      "Epoch [80/100], Step [41/84], Loss: 0.1426\n",
      "Epoch [80/100], Step [42/84], Loss: 0.1312\n",
      "Epoch [80/100], Step [43/84], Loss: 0.1281\n",
      "Epoch [80/100], Step [44/84], Loss: 0.1405\n",
      "Epoch [80/100], Step [45/84], Loss: 0.1392\n",
      "Epoch [80/100], Step [46/84], Loss: 0.1330\n",
      "Epoch [80/100], Step [47/84], Loss: 0.1327\n",
      "Epoch [80/100], Step [48/84], Loss: 0.1288\n",
      "Epoch [80/100], Step [49/84], Loss: 0.1493\n",
      "Epoch [80/100], Step [50/84], Loss: 0.1426\n",
      "Epoch [80/100], Step [51/84], Loss: 0.1401\n",
      "Epoch [80/100], Step [52/84], Loss: 0.1149\n",
      "Epoch [80/100], Step [53/84], Loss: 0.1397\n",
      "Epoch [80/100], Step [54/84], Loss: 0.1391\n",
      "Epoch [80/100], Step [55/84], Loss: 0.1300\n",
      "Epoch [80/100], Step [56/84], Loss: 0.1440\n",
      "Epoch [80/100], Step [57/84], Loss: 0.1253\n",
      "Epoch [80/100], Step [58/84], Loss: 0.1411\n",
      "Epoch [80/100], Step [59/84], Loss: 0.1363\n",
      "Epoch [80/100], Step [60/84], Loss: 0.1439\n",
      "Epoch [80/100], Step [61/84], Loss: 0.1513\n",
      "Epoch [80/100], Step [62/84], Loss: 0.1353\n",
      "Epoch [80/100], Step [63/84], Loss: 0.1379\n",
      "Epoch [80/100], Step [64/84], Loss: 0.1390\n",
      "Epoch [80/100], Step [65/84], Loss: 0.1490\n",
      "Epoch [80/100], Step [66/84], Loss: 0.1288\n",
      "Epoch [80/100], Step [67/84], Loss: 0.1573\n",
      "Epoch [80/100], Step [68/84], Loss: 0.1253\n",
      "Epoch [80/100], Step [69/84], Loss: 0.1374\n",
      "Epoch [80/100], Step [70/84], Loss: 0.1373\n",
      "Epoch [80/100], Step [71/84], Loss: 0.1265\n",
      "Epoch [80/100], Step [72/84], Loss: 0.1305\n",
      "Epoch [80/100], Step [73/84], Loss: 0.1193\n",
      "Epoch [80/100], Step [74/84], Loss: 0.1315\n",
      "Epoch [80/100], Step [75/84], Loss: 0.1425\n",
      "Epoch [80/100], Step [76/84], Loss: 0.1623\n",
      "Epoch [80/100], Step [77/84], Loss: 0.1411\n",
      "Epoch [80/100], Step [78/84], Loss: 0.1309\n",
      "Epoch [80/100], Step [79/84], Loss: 0.1263\n",
      "Epoch [80/100], Step [80/84], Loss: 0.1602\n",
      "Epoch [80/100], Step [81/84], Loss: 0.1342\n",
      "Epoch [80/100], Step [82/84], Loss: 0.1372\n",
      "Epoch [80/100], Step [83/84], Loss: 0.1299\n",
      "Epoch [80/100], Step [84/84], Loss: 0.1467\n",
      "Epoch [81/100], Step [1/84], Loss: 0.1780\n",
      "Epoch [81/100], Step [2/84], Loss: 0.1244\n",
      "Epoch [81/100], Step [3/84], Loss: 0.1621\n",
      "Epoch [81/100], Step [4/84], Loss: 0.1394\n",
      "Epoch [81/100], Step [5/84], Loss: 0.1637\n",
      "Epoch [81/100], Step [6/84], Loss: 0.1396\n",
      "Epoch [81/100], Step [7/84], Loss: 0.1457\n",
      "Epoch [81/100], Step [8/84], Loss: 0.1609\n",
      "Epoch [81/100], Step [9/84], Loss: 0.1476\n",
      "Epoch [81/100], Step [10/84], Loss: 0.1332\n",
      "Epoch [81/100], Step [11/84], Loss: 0.1253\n",
      "Epoch [81/100], Step [12/84], Loss: 0.1600\n",
      "Epoch [81/100], Step [13/84], Loss: 0.1259\n",
      "Epoch [81/100], Step [14/84], Loss: 0.1396\n",
      "Epoch [81/100], Step [15/84], Loss: 0.1434\n",
      "Epoch [81/100], Step [16/84], Loss: 0.1291\n",
      "Epoch [81/100], Step [17/84], Loss: 0.1469\n",
      "Epoch [81/100], Step [18/84], Loss: 0.1460\n",
      "Epoch [81/100], Step [19/84], Loss: 0.1376\n",
      "Epoch [81/100], Step [20/84], Loss: 0.1915\n",
      "Epoch [81/100], Step [21/84], Loss: 0.1453\n",
      "Epoch [81/100], Step [22/84], Loss: 0.1364\n",
      "Epoch [81/100], Step [23/84], Loss: 0.1667\n",
      "Epoch [81/100], Step [24/84], Loss: 0.1318\n",
      "Epoch [81/100], Step [25/84], Loss: 0.1216\n",
      "Epoch [81/100], Step [26/84], Loss: 0.1349\n",
      "Epoch [81/100], Step [27/84], Loss: 0.1395\n",
      "Epoch [81/100], Step [28/84], Loss: 0.1262\n",
      "Epoch [81/100], Step [29/84], Loss: 0.1133\n",
      "Epoch [81/100], Step [30/84], Loss: 0.1532\n",
      "Epoch [81/100], Step [31/84], Loss: 0.1294\n",
      "Epoch [81/100], Step [32/84], Loss: 0.1256\n",
      "Epoch [81/100], Step [33/84], Loss: 0.1391\n",
      "Epoch [81/100], Step [34/84], Loss: 0.1266\n",
      "Epoch [81/100], Step [35/84], Loss: 0.1280\n",
      "Epoch [81/100], Step [36/84], Loss: 0.1227\n",
      "Epoch [81/100], Step [37/84], Loss: 0.1227\n",
      "Epoch [81/100], Step [38/84], Loss: 0.1412\n",
      "Epoch [81/100], Step [39/84], Loss: 0.1449\n",
      "Epoch [81/100], Step [40/84], Loss: 0.1397\n",
      "Epoch [81/100], Step [41/84], Loss: 0.1397\n",
      "Epoch [81/100], Step [42/84], Loss: 0.1278\n",
      "Epoch [81/100], Step [43/84], Loss: 0.1309\n",
      "Epoch [81/100], Step [44/84], Loss: 0.1225\n",
      "Epoch [81/100], Step [45/84], Loss: 0.1197\n",
      "Epoch [81/100], Step [46/84], Loss: 0.1508\n",
      "Epoch [81/100], Step [47/84], Loss: 0.1294\n",
      "Epoch [81/100], Step [48/84], Loss: 0.1471\n",
      "Epoch [81/100], Step [49/84], Loss: 0.1260\n",
      "Epoch [81/100], Step [50/84], Loss: 0.1233\n",
      "Epoch [81/100], Step [51/84], Loss: 0.1377\n",
      "Epoch [81/100], Step [52/84], Loss: 0.1318\n",
      "Epoch [81/100], Step [53/84], Loss: 0.1324\n",
      "Epoch [81/100], Step [54/84], Loss: 0.1283\n",
      "Epoch [81/100], Step [55/84], Loss: 0.1407\n",
      "Epoch [81/100], Step [56/84], Loss: 0.1348\n",
      "Epoch [81/100], Step [57/84], Loss: 0.1353\n",
      "Epoch [81/100], Step [58/84], Loss: 0.1445\n",
      "Epoch [81/100], Step [59/84], Loss: 0.1403\n",
      "Epoch [81/100], Step [60/84], Loss: 0.1350\n",
      "Epoch [81/100], Step [61/84], Loss: 0.1441\n",
      "Epoch [81/100], Step [62/84], Loss: 0.1519\n",
      "Epoch [81/100], Step [63/84], Loss: 0.1482\n",
      "Epoch [81/100], Step [64/84], Loss: 0.1307\n",
      "Epoch [81/100], Step [65/84], Loss: 0.1234\n",
      "Epoch [81/100], Step [66/84], Loss: 0.1303\n",
      "Epoch [81/100], Step [67/84], Loss: 0.1170\n",
      "Epoch [81/100], Step [68/84], Loss: 0.1291\n",
      "Epoch [81/100], Step [69/84], Loss: 0.1276\n",
      "Epoch [81/100], Step [70/84], Loss: 0.1246\n",
      "Epoch [81/100], Step [71/84], Loss: 0.1247\n",
      "Epoch [81/100], Step [72/84], Loss: 0.1302\n",
      "Epoch [81/100], Step [73/84], Loss: 0.1271\n",
      "Epoch [81/100], Step [74/84], Loss: 0.1335\n",
      "Epoch [81/100], Step [75/84], Loss: 0.1426\n",
      "Epoch [81/100], Step [76/84], Loss: 0.1263\n",
      "Epoch [81/100], Step [77/84], Loss: 0.1337\n",
      "Epoch [81/100], Step [78/84], Loss: 0.1523\n",
      "Epoch [81/100], Step [79/84], Loss: 0.1269\n",
      "Epoch [81/100], Step [80/84], Loss: 0.1282\n",
      "Epoch [81/100], Step [81/84], Loss: 0.1204\n",
      "Epoch [81/100], Step [82/84], Loss: 0.1317\n",
      "Epoch [81/100], Step [83/84], Loss: 0.1391\n",
      "Epoch [81/100], Step [84/84], Loss: 0.1618\n",
      "Epoch [82/100], Step [1/84], Loss: 0.1653\n",
      "Epoch [82/100], Step [2/84], Loss: 0.1293\n",
      "Epoch [82/100], Step [3/84], Loss: 0.1481\n",
      "Epoch [82/100], Step [4/84], Loss: 0.1363\n",
      "Epoch [82/100], Step [5/84], Loss: 0.1257\n",
      "Epoch [82/100], Step [6/84], Loss: 0.1350\n",
      "Epoch [82/100], Step [7/84], Loss: 0.1303\n",
      "Epoch [82/100], Step [8/84], Loss: 0.1176\n",
      "Epoch [82/100], Step [9/84], Loss: 0.1322\n",
      "Epoch [82/100], Step [10/84], Loss: 0.1335\n",
      "Epoch [82/100], Step [11/84], Loss: 0.1416\n",
      "Epoch [82/100], Step [12/84], Loss: 0.1419\n",
      "Epoch [82/100], Step [13/84], Loss: 0.1474\n",
      "Epoch [82/100], Step [14/84], Loss: 0.1306\n",
      "Epoch [82/100], Step [15/84], Loss: 0.1260\n",
      "Epoch [82/100], Step [16/84], Loss: 0.1330\n",
      "Epoch [82/100], Step [17/84], Loss: 0.1252\n",
      "Epoch [82/100], Step [18/84], Loss: 0.1453\n",
      "Epoch [82/100], Step [19/84], Loss: 0.1253\n",
      "Epoch [82/100], Step [20/84], Loss: 0.1225\n",
      "Epoch [82/100], Step [21/84], Loss: 0.1165\n",
      "Epoch [82/100], Step [22/84], Loss: 0.1388\n",
      "Epoch [82/100], Step [23/84], Loss: 0.1396\n",
      "Epoch [82/100], Step [24/84], Loss: 0.1205\n",
      "Epoch [82/100], Step [25/84], Loss: 0.1221\n",
      "Epoch [82/100], Step [26/84], Loss: 0.1388\n",
      "Epoch [82/100], Step [27/84], Loss: 0.1406\n",
      "Epoch [82/100], Step [28/84], Loss: 0.1405\n",
      "Epoch [82/100], Step [29/84], Loss: 0.1268\n",
      "Epoch [82/100], Step [30/84], Loss: 0.1350\n",
      "Epoch [82/100], Step [31/84], Loss: 0.1264\n",
      "Epoch [82/100], Step [32/84], Loss: 0.1263\n",
      "Epoch [82/100], Step [33/84], Loss: 0.1221\n",
      "Epoch [82/100], Step [34/84], Loss: 0.1509\n",
      "Epoch [82/100], Step [35/84], Loss: 0.1359\n",
      "Epoch [82/100], Step [36/84], Loss: 0.1169\n",
      "Epoch [82/100], Step [37/84], Loss: 0.1116\n",
      "Epoch [82/100], Step [38/84], Loss: 0.1221\n",
      "Epoch [82/100], Step [39/84], Loss: 0.1288\n",
      "Epoch [82/100], Step [40/84], Loss: 0.1590\n",
      "Epoch [82/100], Step [41/84], Loss: 0.1207\n",
      "Epoch [82/100], Step [42/84], Loss: 0.1231\n",
      "Epoch [82/100], Step [43/84], Loss: 0.1196\n",
      "Epoch [82/100], Step [44/84], Loss: 0.1344\n",
      "Epoch [82/100], Step [45/84], Loss: 0.1269\n",
      "Epoch [82/100], Step [46/84], Loss: 0.1262\n",
      "Epoch [82/100], Step [47/84], Loss: 0.1186\n",
      "Epoch [82/100], Step [48/84], Loss: 0.1146\n",
      "Epoch [82/100], Step [49/84], Loss: 0.1176\n",
      "Epoch [82/100], Step [50/84], Loss: 0.1217\n",
      "Epoch [82/100], Step [51/84], Loss: 0.1154\n",
      "Epoch [82/100], Step [52/84], Loss: 0.1460\n",
      "Epoch [82/100], Step [53/84], Loss: 0.1125\n",
      "Epoch [82/100], Step [54/84], Loss: 0.1263\n",
      "Epoch [82/100], Step [55/84], Loss: 0.1681\n",
      "Epoch [82/100], Step [56/84], Loss: 0.1374\n",
      "Epoch [82/100], Step [57/84], Loss: 0.1252\n",
      "Epoch [82/100], Step [58/84], Loss: 0.1111\n",
      "Epoch [82/100], Step [59/84], Loss: 0.1192\n",
      "Epoch [82/100], Step [60/84], Loss: 0.1289\n",
      "Epoch [82/100], Step [61/84], Loss: 0.1421\n",
      "Epoch [82/100], Step [62/84], Loss: 0.1413\n",
      "Epoch [82/100], Step [63/84], Loss: 0.1220\n",
      "Epoch [82/100], Step [64/84], Loss: 0.1281\n",
      "Epoch [82/100], Step [65/84], Loss: 0.1380\n",
      "Epoch [82/100], Step [66/84], Loss: 0.1108\n",
      "Epoch [82/100], Step [67/84], Loss: 0.1244\n",
      "Epoch [82/100], Step [68/84], Loss: 0.1187\n",
      "Epoch [82/100], Step [69/84], Loss: 0.1300\n",
      "Epoch [82/100], Step [70/84], Loss: 0.1299\n",
      "Epoch [82/100], Step [71/84], Loss: 0.1361\n",
      "Epoch [82/100], Step [72/84], Loss: 0.1311\n",
      "Epoch [82/100], Step [73/84], Loss: 0.1459\n",
      "Epoch [82/100], Step [74/84], Loss: 0.1323\n",
      "Epoch [82/100], Step [75/84], Loss: 0.1356\n",
      "Epoch [82/100], Step [76/84], Loss: 0.1281\n",
      "Epoch [82/100], Step [77/84], Loss: 0.1468\n",
      "Epoch [82/100], Step [78/84], Loss: 0.1182\n",
      "Epoch [82/100], Step [79/84], Loss: 0.1092\n",
      "Epoch [82/100], Step [80/84], Loss: 0.1465\n",
      "Epoch [82/100], Step [81/84], Loss: 0.1545\n",
      "Epoch [82/100], Step [82/84], Loss: 0.1280\n",
      "Epoch [82/100], Step [83/84], Loss: 0.1291\n",
      "Epoch [82/100], Step [84/84], Loss: 0.1356\n",
      "Epoch [83/100], Step [1/84], Loss: 0.1411\n",
      "Epoch [83/100], Step [2/84], Loss: 0.1219\n",
      "Epoch [83/100], Step [3/84], Loss: 0.1155\n",
      "Epoch [83/100], Step [4/84], Loss: 0.1313\n",
      "Epoch [83/100], Step [5/84], Loss: 0.1429\n",
      "Epoch [83/100], Step [6/84], Loss: 0.1177\n",
      "Epoch [83/100], Step [7/84], Loss: 0.1215\n",
      "Epoch [83/100], Step [8/84], Loss: 0.1232\n",
      "Epoch [83/100], Step [9/84], Loss: 0.1370\n",
      "Epoch [83/100], Step [10/84], Loss: 0.1305\n",
      "Epoch [83/100], Step [11/84], Loss: 0.1327\n",
      "Epoch [83/100], Step [12/84], Loss: 0.1640\n",
      "Epoch [83/100], Step [13/84], Loss: 0.1300\n",
      "Epoch [83/100], Step [14/84], Loss: 0.1312\n",
      "Epoch [83/100], Step [15/84], Loss: 0.1207\n",
      "Epoch [83/100], Step [16/84], Loss: 0.1366\n",
      "Epoch [83/100], Step [17/84], Loss: 0.1360\n",
      "Epoch [83/100], Step [18/84], Loss: 0.1302\n",
      "Epoch [83/100], Step [19/84], Loss: 0.1287\n",
      "Epoch [83/100], Step [20/84], Loss: 0.1291\n",
      "Epoch [83/100], Step [21/84], Loss: 0.1353\n",
      "Epoch [83/100], Step [22/84], Loss: 0.1302\n",
      "Epoch [83/100], Step [23/84], Loss: 0.1283\n",
      "Epoch [83/100], Step [24/84], Loss: 0.1170\n",
      "Epoch [83/100], Step [25/84], Loss: 0.1236\n",
      "Epoch [83/100], Step [26/84], Loss: 0.1250\n",
      "Epoch [83/100], Step [27/84], Loss: 0.1260\n",
      "Epoch [83/100], Step [28/84], Loss: 0.1191\n",
      "Epoch [83/100], Step [29/84], Loss: 0.1395\n",
      "Epoch [83/100], Step [30/84], Loss: 0.1292\n",
      "Epoch [83/100], Step [31/84], Loss: 0.1351\n",
      "Epoch [83/100], Step [32/84], Loss: 0.1286\n",
      "Epoch [83/100], Step [33/84], Loss: 0.1194\n",
      "Epoch [83/100], Step [34/84], Loss: 0.1334\n",
      "Epoch [83/100], Step [35/84], Loss: 0.1129\n",
      "Epoch [83/100], Step [36/84], Loss: 0.1250\n",
      "Epoch [83/100], Step [37/84], Loss: 0.1406\n",
      "Epoch [83/100], Step [38/84], Loss: 0.1396\n",
      "Epoch [83/100], Step [39/84], Loss: 0.1235\n",
      "Epoch [83/100], Step [40/84], Loss: 0.1214\n",
      "Epoch [83/100], Step [41/84], Loss: 0.1291\n",
      "Epoch [83/100], Step [42/84], Loss: 0.1521\n",
      "Epoch [83/100], Step [43/84], Loss: 0.1219\n",
      "Epoch [83/100], Step [44/84], Loss: 0.1361\n",
      "Epoch [83/100], Step [45/84], Loss: 0.1313\n",
      "Epoch [83/100], Step [46/84], Loss: 0.1371\n",
      "Epoch [83/100], Step [47/84], Loss: 0.1253\n",
      "Epoch [83/100], Step [48/84], Loss: 0.1465\n",
      "Epoch [83/100], Step [49/84], Loss: 0.1239\n",
      "Epoch [83/100], Step [50/84], Loss: 0.1337\n",
      "Epoch [83/100], Step [51/84], Loss: 0.1497\n",
      "Epoch [83/100], Step [52/84], Loss: 0.1315\n",
      "Epoch [83/100], Step [53/84], Loss: 0.1370\n",
      "Epoch [83/100], Step [54/84], Loss: 0.1554\n",
      "Epoch [83/100], Step [55/84], Loss: 0.1298\n",
      "Epoch [83/100], Step [56/84], Loss: 0.1245\n",
      "Epoch [83/100], Step [57/84], Loss: 0.1386\n",
      "Epoch [83/100], Step [58/84], Loss: 0.1238\n",
      "Epoch [83/100], Step [59/84], Loss: 0.1421\n",
      "Epoch [83/100], Step [60/84], Loss: 0.1464\n",
      "Epoch [83/100], Step [61/84], Loss: 0.1386\n",
      "Epoch [83/100], Step [62/84], Loss: 0.1244\n",
      "Epoch [83/100], Step [63/84], Loss: 0.1627\n",
      "Epoch [83/100], Step [64/84], Loss: 0.1174\n",
      "Epoch [83/100], Step [65/84], Loss: 0.1319\n",
      "Epoch [83/100], Step [66/84], Loss: 0.1632\n",
      "Epoch [83/100], Step [67/84], Loss: 0.1625\n",
      "Epoch [83/100], Step [68/84], Loss: 0.1323\n",
      "Epoch [83/100], Step [69/84], Loss: 0.1172\n",
      "Epoch [83/100], Step [70/84], Loss: 0.1263\n",
      "Epoch [83/100], Step [71/84], Loss: 0.1519\n",
      "Epoch [83/100], Step [72/84], Loss: 0.1332\n",
      "Epoch [83/100], Step [73/84], Loss: 0.1348\n",
      "Epoch [83/100], Step [74/84], Loss: 0.1153\n",
      "Epoch [83/100], Step [75/84], Loss: 0.1290\n",
      "Epoch [83/100], Step [76/84], Loss: 0.1213\n",
      "Epoch [83/100], Step [77/84], Loss: 0.1052\n",
      "Epoch [83/100], Step [78/84], Loss: 0.1262\n",
      "Epoch [83/100], Step [79/84], Loss: 0.1279\n",
      "Epoch [83/100], Step [80/84], Loss: 0.1252\n",
      "Epoch [83/100], Step [81/84], Loss: 0.1151\n",
      "Epoch [83/100], Step [82/84], Loss: 0.1213\n",
      "Epoch [83/100], Step [83/84], Loss: 0.1135\n",
      "Epoch [83/100], Step [84/84], Loss: 0.1252\n",
      "Epoch [84/100], Step [1/84], Loss: 0.1199\n",
      "Epoch [84/100], Step [2/84], Loss: 0.1272\n",
      "Epoch [84/100], Step [3/84], Loss: 0.1444\n",
      "Epoch [84/100], Step [4/84], Loss: 0.1596\n",
      "Epoch [84/100], Step [5/84], Loss: 0.1228\n",
      "Epoch [84/100], Step [6/84], Loss: 0.1201\n",
      "Epoch [84/100], Step [7/84], Loss: 0.1609\n",
      "Epoch [84/100], Step [8/84], Loss: 0.1515\n",
      "Epoch [84/100], Step [9/84], Loss: 0.1420\n",
      "Epoch [84/100], Step [10/84], Loss: 0.1216\n",
      "Epoch [84/100], Step [11/84], Loss: 0.1425\n",
      "Epoch [84/100], Step [12/84], Loss: 0.1366\n",
      "Epoch [84/100], Step [13/84], Loss: 0.1697\n",
      "Epoch [84/100], Step [14/84], Loss: 0.1213\n",
      "Epoch [84/100], Step [15/84], Loss: 0.1258\n",
      "Epoch [84/100], Step [16/84], Loss: 0.1201\n",
      "Epoch [84/100], Step [17/84], Loss: 0.1382\n",
      "Epoch [84/100], Step [18/84], Loss: 0.1144\n",
      "Epoch [84/100], Step [19/84], Loss: 0.1111\n",
      "Epoch [84/100], Step [20/84], Loss: 0.1293\n",
      "Epoch [84/100], Step [21/84], Loss: 0.1148\n",
      "Epoch [84/100], Step [22/84], Loss: 0.1496\n",
      "Epoch [84/100], Step [23/84], Loss: 0.1535\n",
      "Epoch [84/100], Step [24/84], Loss: 0.1195\n",
      "Epoch [84/100], Step [25/84], Loss: 0.1369\n",
      "Epoch [84/100], Step [26/84], Loss: 0.1192\n",
      "Epoch [84/100], Step [27/84], Loss: 0.1234\n",
      "Epoch [84/100], Step [28/84], Loss: 0.1354\n",
      "Epoch [84/100], Step [29/84], Loss: 0.1122\n",
      "Epoch [84/100], Step [30/84], Loss: 0.1159\n",
      "Epoch [84/100], Step [31/84], Loss: 0.1065\n",
      "Epoch [84/100], Step [32/84], Loss: 0.1242\n",
      "Epoch [84/100], Step [33/84], Loss: 0.1236\n",
      "Epoch [84/100], Step [34/84], Loss: 0.1318\n",
      "Epoch [84/100], Step [35/84], Loss: 0.1311\n",
      "Epoch [84/100], Step [36/84], Loss: 0.1041\n",
      "Epoch [84/100], Step [37/84], Loss: 0.1055\n",
      "Epoch [84/100], Step [38/84], Loss: 0.1168\n",
      "Epoch [84/100], Step [39/84], Loss: 0.1083\n",
      "Epoch [84/100], Step [40/84], Loss: 0.1216\n",
      "Epoch [84/100], Step [41/84], Loss: 0.1492\n",
      "Epoch [84/100], Step [42/84], Loss: 0.1322\n",
      "Epoch [84/100], Step [43/84], Loss: 0.1221\n",
      "Epoch [84/100], Step [44/84], Loss: 0.1102\n",
      "Epoch [84/100], Step [45/84], Loss: 0.1240\n",
      "Epoch [84/100], Step [46/84], Loss: 0.1206\n",
      "Epoch [84/100], Step [47/84], Loss: 0.1316\n",
      "Epoch [84/100], Step [48/84], Loss: 0.1274\n",
      "Epoch [84/100], Step [49/84], Loss: 0.1397\n",
      "Epoch [84/100], Step [50/84], Loss: 0.1192\n",
      "Epoch [84/100], Step [51/84], Loss: 0.1550\n",
      "Epoch [84/100], Step [52/84], Loss: 0.1108\n",
      "Epoch [84/100], Step [53/84], Loss: 0.1298\n",
      "Epoch [84/100], Step [54/84], Loss: 0.1270\n",
      "Epoch [84/100], Step [55/84], Loss: 0.1629\n",
      "Epoch [84/100], Step [56/84], Loss: 0.1249\n",
      "Epoch [84/100], Step [57/84], Loss: 0.1245\n",
      "Epoch [84/100], Step [58/84], Loss: 0.1341\n",
      "Epoch [84/100], Step [59/84], Loss: 0.1357\n",
      "Epoch [84/100], Step [60/84], Loss: 0.1306\n",
      "Epoch [84/100], Step [61/84], Loss: 0.1255\n",
      "Epoch [84/100], Step [62/84], Loss: 0.1300\n",
      "Epoch [84/100], Step [63/84], Loss: 0.1611\n",
      "Epoch [84/100], Step [64/84], Loss: 0.1473\n",
      "Epoch [84/100], Step [65/84], Loss: 0.1185\n",
      "Epoch [84/100], Step [66/84], Loss: 0.1260\n",
      "Epoch [84/100], Step [67/84], Loss: 0.1398\n",
      "Epoch [84/100], Step [68/84], Loss: 0.1126\n",
      "Epoch [84/100], Step [69/84], Loss: 0.1397\n",
      "Epoch [84/100], Step [70/84], Loss: 0.1309\n",
      "Epoch [84/100], Step [71/84], Loss: 0.1266\n",
      "Epoch [84/100], Step [72/84], Loss: 0.1328\n",
      "Epoch [84/100], Step [73/84], Loss: 0.1321\n",
      "Epoch [84/100], Step [74/84], Loss: 0.1159\n",
      "Epoch [84/100], Step [75/84], Loss: 0.1260\n",
      "Epoch [84/100], Step [76/84], Loss: 0.1303\n",
      "Epoch [84/100], Step [77/84], Loss: 0.1385\n",
      "Epoch [84/100], Step [78/84], Loss: 0.1232\n",
      "Epoch [84/100], Step [79/84], Loss: 0.1140\n",
      "Epoch [84/100], Step [80/84], Loss: 0.1482\n",
      "Epoch [84/100], Step [81/84], Loss: 0.1289\n",
      "Epoch [84/100], Step [82/84], Loss: 0.1280\n",
      "Epoch [84/100], Step [83/84], Loss: 0.1443\n",
      "Epoch [84/100], Step [84/84], Loss: 0.1219\n",
      "Epoch [85/100], Step [1/84], Loss: 0.1225\n",
      "Epoch [85/100], Step [2/84], Loss: 0.1332\n",
      "Epoch [85/100], Step [3/84], Loss: 0.1118\n",
      "Epoch [85/100], Step [4/84], Loss: 0.1150\n",
      "Epoch [85/100], Step [5/84], Loss: 0.1053\n",
      "Epoch [85/100], Step [6/84], Loss: 0.1119\n",
      "Epoch [85/100], Step [7/84], Loss: 0.1095\n",
      "Epoch [85/100], Step [8/84], Loss: 0.1252\n",
      "Epoch [85/100], Step [9/84], Loss: 0.1145\n",
      "Epoch [85/100], Step [10/84], Loss: 0.1274\n",
      "Epoch [85/100], Step [11/84], Loss: 0.1126\n",
      "Epoch [85/100], Step [12/84], Loss: 0.1143\n",
      "Epoch [85/100], Step [13/84], Loss: 0.1333\n",
      "Epoch [85/100], Step [14/84], Loss: 0.1073\n",
      "Epoch [85/100], Step [15/84], Loss: 0.1314\n",
      "Epoch [85/100], Step [16/84], Loss: 0.1041\n",
      "Epoch [85/100], Step [17/84], Loss: 0.1133\n",
      "Epoch [85/100], Step [18/84], Loss: 0.1390\n",
      "Epoch [85/100], Step [19/84], Loss: 0.1158\n",
      "Epoch [85/100], Step [20/84], Loss: 0.1302\n",
      "Epoch [85/100], Step [21/84], Loss: 0.1146\n",
      "Epoch [85/100], Step [22/84], Loss: 0.1490\n",
      "Epoch [85/100], Step [23/84], Loss: 0.1119\n",
      "Epoch [85/100], Step [24/84], Loss: 0.1304\n",
      "Epoch [85/100], Step [25/84], Loss: 0.1264\n",
      "Epoch [85/100], Step [26/84], Loss: 0.1266\n",
      "Epoch [85/100], Step [27/84], Loss: 0.1437\n",
      "Epoch [85/100], Step [28/84], Loss: 0.1254\n",
      "Epoch [85/100], Step [29/84], Loss: 0.1371\n",
      "Epoch [85/100], Step [30/84], Loss: 0.1175\n",
      "Epoch [85/100], Step [31/84], Loss: 0.1176\n",
      "Epoch [85/100], Step [32/84], Loss: 0.1465\n",
      "Epoch [85/100], Step [33/84], Loss: 0.1255\n",
      "Epoch [85/100], Step [34/84], Loss: 0.1050\n",
      "Epoch [85/100], Step [35/84], Loss: 0.1268\n",
      "Epoch [85/100], Step [36/84], Loss: 0.1182\n",
      "Epoch [85/100], Step [37/84], Loss: 0.1237\n",
      "Epoch [85/100], Step [38/84], Loss: 0.1292\n",
      "Epoch [85/100], Step [39/84], Loss: 0.1153\n",
      "Epoch [85/100], Step [40/84], Loss: 0.1220\n",
      "Epoch [85/100], Step [41/84], Loss: 0.1388\n",
      "Epoch [85/100], Step [42/84], Loss: 0.1241\n",
      "Epoch [85/100], Step [43/84], Loss: 0.1366\n",
      "Epoch [85/100], Step [44/84], Loss: 0.1156\n",
      "Epoch [85/100], Step [45/84], Loss: 0.1260\n",
      "Epoch [85/100], Step [46/84], Loss: 0.1184\n",
      "Epoch [85/100], Step [47/84], Loss: 0.1181\n",
      "Epoch [85/100], Step [48/84], Loss: 0.1094\n",
      "Epoch [85/100], Step [49/84], Loss: 0.1339\n",
      "Epoch [85/100], Step [50/84], Loss: 0.1444\n",
      "Epoch [85/100], Step [51/84], Loss: 0.1257\n",
      "Epoch [85/100], Step [52/84], Loss: 0.1234\n",
      "Epoch [85/100], Step [53/84], Loss: 0.1250\n",
      "Epoch [85/100], Step [54/84], Loss: 0.1117\n",
      "Epoch [85/100], Step [55/84], Loss: 0.1304\n",
      "Epoch [85/100], Step [56/84], Loss: 0.1500\n",
      "Epoch [85/100], Step [57/84], Loss: 0.1173\n",
      "Epoch [85/100], Step [58/84], Loss: 0.1333\n",
      "Epoch [85/100], Step [59/84], Loss: 0.1284\n",
      "Epoch [85/100], Step [60/84], Loss: 0.1419\n",
      "Epoch [85/100], Step [61/84], Loss: 0.1145\n",
      "Epoch [85/100], Step [62/84], Loss: 0.1586\n",
      "Epoch [85/100], Step [63/84], Loss: 0.1298\n",
      "Epoch [85/100], Step [64/84], Loss: 0.1333\n",
      "Epoch [85/100], Step [65/84], Loss: 0.1206\n",
      "Epoch [85/100], Step [66/84], Loss: 0.1214\n",
      "Epoch [85/100], Step [67/84], Loss: 0.1112\n",
      "Epoch [85/100], Step [68/84], Loss: 0.1359\n",
      "Epoch [85/100], Step [69/84], Loss: 0.1120\n",
      "Epoch [85/100], Step [70/84], Loss: 0.1154\n",
      "Epoch [85/100], Step [71/84], Loss: 0.1173\n",
      "Epoch [85/100], Step [72/84], Loss: 0.1150\n",
      "Epoch [85/100], Step [73/84], Loss: 0.1129\n",
      "Epoch [85/100], Step [74/84], Loss: 0.1311\n",
      "Epoch [85/100], Step [75/84], Loss: 0.1228\n",
      "Epoch [85/100], Step [76/84], Loss: 0.1204\n",
      "Epoch [85/100], Step [77/84], Loss: 0.1338\n",
      "Epoch [85/100], Step [78/84], Loss: 0.1121\n",
      "Epoch [85/100], Step [79/84], Loss: 0.1062\n",
      "Epoch [85/100], Step [80/84], Loss: 0.1163\n",
      "Epoch [85/100], Step [81/84], Loss: 0.1479\n",
      "Epoch [85/100], Step [82/84], Loss: 0.1160\n",
      "Epoch [85/100], Step [83/84], Loss: 0.1382\n",
      "Epoch [85/100], Step [84/84], Loss: 0.1923\n",
      "Epoch [86/100], Step [1/84], Loss: 0.1372\n",
      "Epoch [86/100], Step [2/84], Loss: 0.1349\n",
      "Epoch [86/100], Step [3/84], Loss: 0.1576\n",
      "Epoch [86/100], Step [4/84], Loss: 0.1314\n",
      "Epoch [86/100], Step [5/84], Loss: 0.1325\n",
      "Epoch [86/100], Step [6/84], Loss: 0.1141\n",
      "Epoch [86/100], Step [7/84], Loss: 0.1281\n",
      "Epoch [86/100], Step [8/84], Loss: 0.1260\n",
      "Epoch [86/100], Step [9/84], Loss: 0.1108\n",
      "Epoch [86/100], Step [10/84], Loss: 0.1236\n",
      "Epoch [86/100], Step [11/84], Loss: 0.1248\n",
      "Epoch [86/100], Step [12/84], Loss: 0.1192\n",
      "Epoch [86/100], Step [13/84], Loss: 0.1364\n",
      "Epoch [86/100], Step [14/84], Loss: 0.1434\n",
      "Epoch [86/100], Step [15/84], Loss: 0.1017\n",
      "Epoch [86/100], Step [16/84], Loss: 0.1212\n",
      "Epoch [86/100], Step [17/84], Loss: 0.1079\n",
      "Epoch [86/100], Step [18/84], Loss: 0.1050\n",
      "Epoch [86/100], Step [19/84], Loss: 0.1207\n",
      "Epoch [86/100], Step [20/84], Loss: 0.1289\n",
      "Epoch [86/100], Step [21/84], Loss: 0.1200\n",
      "Epoch [86/100], Step [22/84], Loss: 0.1476\n",
      "Epoch [86/100], Step [23/84], Loss: 0.1016\n",
      "Epoch [86/100], Step [24/84], Loss: 0.1088\n",
      "Epoch [86/100], Step [25/84], Loss: 0.1249\n",
      "Epoch [86/100], Step [26/84], Loss: 0.1318\n",
      "Epoch [86/100], Step [27/84], Loss: 0.1359\n",
      "Epoch [86/100], Step [28/84], Loss: 0.1230\n",
      "Epoch [86/100], Step [29/84], Loss: 0.1189\n",
      "Epoch [86/100], Step [30/84], Loss: 0.1082\n",
      "Epoch [86/100], Step [31/84], Loss: 0.1091\n",
      "Epoch [86/100], Step [32/84], Loss: 0.1037\n",
      "Epoch [86/100], Step [33/84], Loss: 0.1270\n",
      "Epoch [86/100], Step [34/84], Loss: 0.1017\n",
      "Epoch [86/100], Step [35/84], Loss: 0.1147\n",
      "Epoch [86/100], Step [36/84], Loss: 0.1273\n",
      "Epoch [86/100], Step [37/84], Loss: 0.1384\n",
      "Epoch [86/100], Step [38/84], Loss: 0.1176\n",
      "Epoch [86/100], Step [39/84], Loss: 0.1147\n",
      "Epoch [86/100], Step [40/84], Loss: 0.1546\n",
      "Epoch [86/100], Step [41/84], Loss: 0.1052\n",
      "Epoch [86/100], Step [42/84], Loss: 0.1222\n",
      "Epoch [86/100], Step [43/84], Loss: 0.1293\n",
      "Epoch [86/100], Step [44/84], Loss: 0.1289\n",
      "Epoch [86/100], Step [45/84], Loss: 0.1204\n",
      "Epoch [86/100], Step [46/84], Loss: 0.1299\n",
      "Epoch [86/100], Step [47/84], Loss: 0.1192\n",
      "Epoch [86/100], Step [48/84], Loss: 0.1390\n",
      "Epoch [86/100], Step [49/84], Loss: 0.1386\n",
      "Epoch [86/100], Step [50/84], Loss: 0.1166\n",
      "Epoch [86/100], Step [51/84], Loss: 0.1176\n",
      "Epoch [86/100], Step [52/84], Loss: 0.1565\n",
      "Epoch [86/100], Step [53/84], Loss: 0.1310\n",
      "Epoch [86/100], Step [54/84], Loss: 0.1215\n",
      "Epoch [86/100], Step [55/84], Loss: 0.1172\n",
      "Epoch [86/100], Step [56/84], Loss: 0.1167\n",
      "Epoch [86/100], Step [57/84], Loss: 0.1203\n",
      "Epoch [86/100], Step [58/84], Loss: 0.1647\n",
      "Epoch [86/100], Step [59/84], Loss: 0.1319\n",
      "Epoch [86/100], Step [60/84], Loss: 0.1366\n",
      "Epoch [86/100], Step [61/84], Loss: 0.1305\n",
      "Epoch [86/100], Step [62/84], Loss: 0.1131\n",
      "Epoch [86/100], Step [63/84], Loss: 0.1142\n",
      "Epoch [86/100], Step [64/84], Loss: 0.1275\n",
      "Epoch [86/100], Step [65/84], Loss: 0.1474\n",
      "Epoch [86/100], Step [66/84], Loss: 0.1130\n",
      "Epoch [86/100], Step [67/84], Loss: 0.1378\n",
      "Epoch [86/100], Step [68/84], Loss: 0.1147\n",
      "Epoch [86/100], Step [69/84], Loss: 0.1178\n",
      "Epoch [86/100], Step [70/84], Loss: 0.1229\n",
      "Epoch [86/100], Step [71/84], Loss: 0.1160\n",
      "Epoch [86/100], Step [72/84], Loss: 0.1108\n",
      "Epoch [86/100], Step [73/84], Loss: 0.1145\n",
      "Epoch [86/100], Step [74/84], Loss: 0.1524\n",
      "Epoch [86/100], Step [75/84], Loss: 0.1197\n",
      "Epoch [86/100], Step [76/84], Loss: 0.1062\n",
      "Epoch [86/100], Step [77/84], Loss: 0.1459\n",
      "Epoch [86/100], Step [78/84], Loss: 0.1159\n",
      "Epoch [86/100], Step [79/84], Loss: 0.1286\n",
      "Epoch [86/100], Step [80/84], Loss: 0.1167\n",
      "Epoch [86/100], Step [81/84], Loss: 0.1283\n",
      "Epoch [86/100], Step [82/84], Loss: 0.1147\n",
      "Epoch [86/100], Step [83/84], Loss: 0.1227\n",
      "Epoch [86/100], Step [84/84], Loss: 0.1346\n",
      "Epoch [87/100], Step [1/84], Loss: 0.1144\n",
      "Epoch [87/100], Step [2/84], Loss: 0.1198\n",
      "Epoch [87/100], Step [3/84], Loss: 0.1105\n",
      "Epoch [87/100], Step [4/84], Loss: 0.1205\n",
      "Epoch [87/100], Step [5/84], Loss: 0.1126\n",
      "Epoch [87/100], Step [6/84], Loss: 0.1352\n",
      "Epoch [87/100], Step [7/84], Loss: 0.1119\n",
      "Epoch [87/100], Step [8/84], Loss: 0.1160\n",
      "Epoch [87/100], Step [9/84], Loss: 0.1401\n",
      "Epoch [87/100], Step [10/84], Loss: 0.1206\n",
      "Epoch [87/100], Step [11/84], Loss: 0.1394\n",
      "Epoch [87/100], Step [12/84], Loss: 0.1097\n",
      "Epoch [87/100], Step [13/84], Loss: 0.1295\n",
      "Epoch [87/100], Step [14/84], Loss: 0.1175\n",
      "Epoch [87/100], Step [15/84], Loss: 0.1318\n",
      "Epoch [87/100], Step [16/84], Loss: 0.1266\n",
      "Epoch [87/100], Step [17/84], Loss: 0.1103\n",
      "Epoch [87/100], Step [18/84], Loss: 0.1292\n",
      "Epoch [87/100], Step [19/84], Loss: 0.1042\n",
      "Epoch [87/100], Step [20/84], Loss: 0.1138\n",
      "Epoch [87/100], Step [21/84], Loss: 0.1153\n",
      "Epoch [87/100], Step [22/84], Loss: 0.1364\n",
      "Epoch [87/100], Step [23/84], Loss: 0.1115\n",
      "Epoch [87/100], Step [24/84], Loss: 0.1077\n",
      "Epoch [87/100], Step [25/84], Loss: 0.1122\n",
      "Epoch [87/100], Step [26/84], Loss: 0.1350\n",
      "Epoch [87/100], Step [27/84], Loss: 0.1091\n",
      "Epoch [87/100], Step [28/84], Loss: 0.1270\n",
      "Epoch [87/100], Step [29/84], Loss: 0.1100\n",
      "Epoch [87/100], Step [30/84], Loss: 0.1261\n",
      "Epoch [87/100], Step [31/84], Loss: 0.1164\n",
      "Epoch [87/100], Step [32/84], Loss: 0.1082\n",
      "Epoch [87/100], Step [33/84], Loss: 0.1114\n",
      "Epoch [87/100], Step [34/84], Loss: 0.1140\n",
      "Epoch [87/100], Step [35/84], Loss: 0.1339\n",
      "Epoch [87/100], Step [36/84], Loss: 0.1323\n",
      "Epoch [87/100], Step [37/84], Loss: 0.1186\n",
      "Epoch [87/100], Step [38/84], Loss: 0.1337\n",
      "Epoch [87/100], Step [39/84], Loss: 0.1098\n",
      "Epoch [87/100], Step [40/84], Loss: 0.0965\n",
      "Epoch [87/100], Step [41/84], Loss: 0.1098\n",
      "Epoch [87/100], Step [42/84], Loss: 0.1113\n",
      "Epoch [87/100], Step [43/84], Loss: 0.1116\n",
      "Epoch [87/100], Step [44/84], Loss: 0.1032\n",
      "Epoch [87/100], Step [45/84], Loss: 0.1086\n",
      "Epoch [87/100], Step [46/84], Loss: 0.1073\n",
      "Epoch [87/100], Step [47/84], Loss: 0.1170\n",
      "Epoch [87/100], Step [48/84], Loss: 0.0996\n",
      "Epoch [87/100], Step [49/84], Loss: 0.1140\n",
      "Epoch [87/100], Step [50/84], Loss: 0.1141\n",
      "Epoch [87/100], Step [51/84], Loss: 0.0993\n",
      "Epoch [87/100], Step [52/84], Loss: 0.1505\n",
      "Epoch [87/100], Step [53/84], Loss: 0.1309\n",
      "Epoch [87/100], Step [54/84], Loss: 0.1188\n",
      "Epoch [87/100], Step [55/84], Loss: 0.1220\n",
      "Epoch [87/100], Step [56/84], Loss: 0.1560\n",
      "Epoch [87/100], Step [57/84], Loss: 0.1064\n",
      "Epoch [87/100], Step [58/84], Loss: 0.1094\n",
      "Epoch [87/100], Step [59/84], Loss: 0.1149\n",
      "Epoch [87/100], Step [60/84], Loss: 0.1137\n",
      "Epoch [87/100], Step [61/84], Loss: 0.1036\n",
      "Epoch [87/100], Step [62/84], Loss: 0.1075\n",
      "Epoch [87/100], Step [63/84], Loss: 0.1254\n",
      "Epoch [87/100], Step [64/84], Loss: 0.1338\n",
      "Epoch [87/100], Step [65/84], Loss: 0.1122\n",
      "Epoch [87/100], Step [66/84], Loss: 0.1122\n",
      "Epoch [87/100], Step [67/84], Loss: 0.1066\n",
      "Epoch [87/100], Step [68/84], Loss: 0.1258\n",
      "Epoch [87/100], Step [69/84], Loss: 0.1241\n",
      "Epoch [87/100], Step [70/84], Loss: 0.1166\n",
      "Epoch [87/100], Step [71/84], Loss: 0.1043\n",
      "Epoch [87/100], Step [72/84], Loss: 0.1149\n",
      "Epoch [87/100], Step [73/84], Loss: 0.1204\n",
      "Epoch [87/100], Step [74/84], Loss: 0.0997\n",
      "Epoch [87/100], Step [75/84], Loss: 0.1365\n",
      "Epoch [87/100], Step [76/84], Loss: 0.1206\n",
      "Epoch [87/100], Step [77/84], Loss: 0.1176\n",
      "Epoch [87/100], Step [78/84], Loss: 0.1179\n",
      "Epoch [87/100], Step [79/84], Loss: 0.1181\n",
      "Epoch [87/100], Step [80/84], Loss: 0.1311\n",
      "Epoch [87/100], Step [81/84], Loss: 0.1276\n",
      "Epoch [87/100], Step [82/84], Loss: 0.1346\n",
      "Epoch [87/100], Step [83/84], Loss: 0.1173\n",
      "Epoch [87/100], Step [84/84], Loss: 0.1134\n",
      "Epoch [88/100], Step [1/84], Loss: 0.1101\n",
      "Epoch [88/100], Step [2/84], Loss: 0.1074\n",
      "Epoch [88/100], Step [3/84], Loss: 0.1521\n",
      "Epoch [88/100], Step [4/84], Loss: 0.1279\n",
      "Epoch [88/100], Step [5/84], Loss: 0.1174\n",
      "Epoch [88/100], Step [6/84], Loss: 0.1372\n",
      "Epoch [88/100], Step [7/84], Loss: 0.1243\n",
      "Epoch [88/100], Step [8/84], Loss: 0.1078\n",
      "Epoch [88/100], Step [9/84], Loss: 0.1245\n",
      "Epoch [88/100], Step [10/84], Loss: 0.1329\n",
      "Epoch [88/100], Step [11/84], Loss: 0.1224\n",
      "Epoch [88/100], Step [12/84], Loss: 0.1080\n",
      "Epoch [88/100], Step [13/84], Loss: 0.1133\n",
      "Epoch [88/100], Step [14/84], Loss: 0.1244\n",
      "Epoch [88/100], Step [15/84], Loss: 0.1134\n",
      "Epoch [88/100], Step [16/84], Loss: 0.1027\n",
      "Epoch [88/100], Step [17/84], Loss: 0.1148\n",
      "Epoch [88/100], Step [18/84], Loss: 0.1201\n",
      "Epoch [88/100], Step [19/84], Loss: 0.0991\n",
      "Epoch [88/100], Step [20/84], Loss: 0.1210\n",
      "Epoch [88/100], Step [21/84], Loss: 0.1206\n",
      "Epoch [88/100], Step [22/84], Loss: 0.1296\n",
      "Epoch [88/100], Step [23/84], Loss: 0.1121\n",
      "Epoch [88/100], Step [24/84], Loss: 0.0967\n",
      "Epoch [88/100], Step [25/84], Loss: 0.1119\n",
      "Epoch [88/100], Step [26/84], Loss: 0.1021\n",
      "Epoch [88/100], Step [27/84], Loss: 0.1093\n",
      "Epoch [88/100], Step [28/84], Loss: 0.1122\n",
      "Epoch [88/100], Step [29/84], Loss: 0.1039\n",
      "Epoch [88/100], Step [30/84], Loss: 0.1221\n",
      "Epoch [88/100], Step [31/84], Loss: 0.1239\n",
      "Epoch [88/100], Step [32/84], Loss: 0.1176\n",
      "Epoch [88/100], Step [33/84], Loss: 0.1089\n",
      "Epoch [88/100], Step [34/84], Loss: 0.1305\n",
      "Epoch [88/100], Step [35/84], Loss: 0.1456\n",
      "Epoch [88/100], Step [36/84], Loss: 0.1391\n",
      "Epoch [88/100], Step [37/84], Loss: 0.1289\n",
      "Epoch [88/100], Step [38/84], Loss: 0.1125\n",
      "Epoch [88/100], Step [39/84], Loss: 0.1022\n",
      "Epoch [88/100], Step [40/84], Loss: 0.1364\n",
      "Epoch [88/100], Step [41/84], Loss: 0.0997\n",
      "Epoch [88/100], Step [42/84], Loss: 0.1052\n",
      "Epoch [88/100], Step [43/84], Loss: 0.1334\n",
      "Epoch [88/100], Step [44/84], Loss: 0.1056\n",
      "Epoch [88/100], Step [45/84], Loss: 0.1083\n",
      "Epoch [88/100], Step [46/84], Loss: 0.1289\n",
      "Epoch [88/100], Step [47/84], Loss: 0.1191\n",
      "Epoch [88/100], Step [48/84], Loss: 0.1017\n",
      "Epoch [88/100], Step [49/84], Loss: 0.1206\n",
      "Epoch [88/100], Step [50/84], Loss: 0.0963\n",
      "Epoch [88/100], Step [51/84], Loss: 0.1122\n",
      "Epoch [88/100], Step [52/84], Loss: 0.1175\n",
      "Epoch [88/100], Step [53/84], Loss: 0.1149\n",
      "Epoch [88/100], Step [54/84], Loss: 0.1237\n",
      "Epoch [88/100], Step [55/84], Loss: 0.1524\n",
      "Epoch [88/100], Step [56/84], Loss: 0.1084\n",
      "Epoch [88/100], Step [57/84], Loss: 0.1063\n",
      "Epoch [88/100], Step [58/84], Loss: 0.1078\n",
      "Epoch [88/100], Step [59/84], Loss: 0.1035\n",
      "Epoch [88/100], Step [60/84], Loss: 0.1072\n",
      "Epoch [88/100], Step [61/84], Loss: 0.1265\n",
      "Epoch [88/100], Step [62/84], Loss: 0.1189\n",
      "Epoch [88/100], Step [63/84], Loss: 0.1161\n",
      "Epoch [88/100], Step [64/84], Loss: 0.1209\n",
      "Epoch [88/100], Step [65/84], Loss: 0.1395\n",
      "Epoch [88/100], Step [66/84], Loss: 0.1088\n",
      "Epoch [88/100], Step [67/84], Loss: 0.1406\n",
      "Epoch [88/100], Step [68/84], Loss: 0.1138\n",
      "Epoch [88/100], Step [69/84], Loss: 0.1212\n",
      "Epoch [88/100], Step [70/84], Loss: 0.1113\n",
      "Epoch [88/100], Step [71/84], Loss: 0.1106\n",
      "Epoch [88/100], Step [72/84], Loss: 0.1023\n",
      "Epoch [88/100], Step [73/84], Loss: 0.1101\n",
      "Epoch [88/100], Step [74/84], Loss: 0.1129\n",
      "Epoch [88/100], Step [75/84], Loss: 0.1260\n",
      "Epoch [88/100], Step [76/84], Loss: 0.1269\n",
      "Epoch [88/100], Step [77/84], Loss: 0.0979\n",
      "Epoch [88/100], Step [78/84], Loss: 0.1098\n",
      "Epoch [88/100], Step [79/84], Loss: 0.1387\n",
      "Epoch [88/100], Step [80/84], Loss: 0.1276\n",
      "Epoch [88/100], Step [81/84], Loss: 0.1069\n",
      "Epoch [88/100], Step [82/84], Loss: 0.1214\n",
      "Epoch [88/100], Step [83/84], Loss: 0.1152\n",
      "Epoch [88/100], Step [84/84], Loss: 0.1871\n",
      "Epoch [89/100], Step [1/84], Loss: 0.1014\n",
      "Epoch [89/100], Step [2/84], Loss: 0.1017\n",
      "Epoch [89/100], Step [3/84], Loss: 0.1223\n",
      "Epoch [89/100], Step [4/84], Loss: 0.1470\n",
      "Epoch [89/100], Step [5/84], Loss: 0.1535\n",
      "Epoch [89/100], Step [6/84], Loss: 0.1192\n",
      "Epoch [89/100], Step [7/84], Loss: 0.1249\n",
      "Epoch [89/100], Step [8/84], Loss: 0.1096\n",
      "Epoch [89/100], Step [9/84], Loss: 0.1422\n",
      "Epoch [89/100], Step [10/84], Loss: 0.1769\n",
      "Epoch [89/100], Step [11/84], Loss: 0.1371\n",
      "Epoch [89/100], Step [12/84], Loss: 0.1175\n",
      "Epoch [89/100], Step [13/84], Loss: 0.1378\n",
      "Epoch [89/100], Step [14/84], Loss: 0.1302\n",
      "Epoch [89/100], Step [15/84], Loss: 0.1145\n",
      "Epoch [89/100], Step [16/84], Loss: 0.1242\n",
      "Epoch [89/100], Step [17/84], Loss: 0.1167\n",
      "Epoch [89/100], Step [18/84], Loss: 0.1702\n",
      "Epoch [89/100], Step [19/84], Loss: 0.1239\n",
      "Epoch [89/100], Step [20/84], Loss: 0.1148\n",
      "Epoch [89/100], Step [21/84], Loss: 0.1242\n",
      "Epoch [89/100], Step [22/84], Loss: 0.1192\n",
      "Epoch [89/100], Step [23/84], Loss: 0.1115\n",
      "Epoch [89/100], Step [24/84], Loss: 0.1233\n",
      "Epoch [89/100], Step [25/84], Loss: 0.1150\n",
      "Epoch [89/100], Step [26/84], Loss: 0.1236\n",
      "Epoch [89/100], Step [27/84], Loss: 0.1276\n",
      "Epoch [89/100], Step [28/84], Loss: 0.1354\n",
      "Epoch [89/100], Step [29/84], Loss: 0.1099\n",
      "Epoch [89/100], Step [30/84], Loss: 0.1005\n",
      "Epoch [89/100], Step [31/84], Loss: 0.1235\n",
      "Epoch [89/100], Step [32/84], Loss: 0.1150\n",
      "Epoch [89/100], Step [33/84], Loss: 0.1070\n",
      "Epoch [89/100], Step [34/84], Loss: 0.1085\n",
      "Epoch [89/100], Step [35/84], Loss: 0.1197\n",
      "Epoch [89/100], Step [36/84], Loss: 0.1206\n",
      "Epoch [89/100], Step [37/84], Loss: 0.1408\n",
      "Epoch [89/100], Step [38/84], Loss: 0.1231\n",
      "Epoch [89/100], Step [39/84], Loss: 0.1143\n",
      "Epoch [89/100], Step [40/84], Loss: 0.1376\n",
      "Epoch [89/100], Step [41/84], Loss: 0.1238\n",
      "Epoch [89/100], Step [42/84], Loss: 0.1334\n",
      "Epoch [89/100], Step [43/84], Loss: 0.1176\n",
      "Epoch [89/100], Step [44/84], Loss: 0.1302\n",
      "Epoch [89/100], Step [45/84], Loss: 0.1104\n",
      "Epoch [89/100], Step [46/84], Loss: 0.1048\n",
      "Epoch [89/100], Step [47/84], Loss: 0.1361\n",
      "Epoch [89/100], Step [48/84], Loss: 0.1288\n",
      "Epoch [89/100], Step [49/84], Loss: 0.1136\n",
      "Epoch [89/100], Step [50/84], Loss: 0.1038\n",
      "Epoch [89/100], Step [51/84], Loss: 0.1063\n",
      "Epoch [89/100], Step [52/84], Loss: 0.1186\n",
      "Epoch [89/100], Step [53/84], Loss: 0.1379\n",
      "Epoch [89/100], Step [54/84], Loss: 0.1181\n",
      "Epoch [89/100], Step [55/84], Loss: 0.1117\n",
      "Epoch [89/100], Step [56/84], Loss: 0.1272\n",
      "Epoch [89/100], Step [57/84], Loss: 0.1196\n",
      "Epoch [89/100], Step [58/84], Loss: 0.0958\n",
      "Epoch [89/100], Step [59/84], Loss: 0.1277\n",
      "Epoch [89/100], Step [60/84], Loss: 0.1165\n",
      "Epoch [89/100], Step [61/84], Loss: 0.1125\n",
      "Epoch [89/100], Step [62/84], Loss: 0.1064\n",
      "Epoch [89/100], Step [63/84], Loss: 0.1006\n",
      "Epoch [89/100], Step [64/84], Loss: 0.1163\n",
      "Epoch [89/100], Step [65/84], Loss: 0.1242\n",
      "Epoch [89/100], Step [66/84], Loss: 0.1019\n",
      "Epoch [89/100], Step [67/84], Loss: 0.1021\n",
      "Epoch [89/100], Step [68/84], Loss: 0.1075\n",
      "Epoch [89/100], Step [69/84], Loss: 0.1587\n",
      "Epoch [89/100], Step [70/84], Loss: 0.0989\n",
      "Epoch [89/100], Step [71/84], Loss: 0.1219\n",
      "Epoch [89/100], Step [72/84], Loss: 0.1157\n",
      "Epoch [89/100], Step [73/84], Loss: 0.1191\n",
      "Epoch [89/100], Step [74/84], Loss: 0.1050\n",
      "Epoch [89/100], Step [75/84], Loss: 0.1170\n",
      "Epoch [89/100], Step [76/84], Loss: 0.1393\n",
      "Epoch [89/100], Step [77/84], Loss: 0.1101\n",
      "Epoch [89/100], Step [78/84], Loss: 0.1225\n",
      "Epoch [89/100], Step [79/84], Loss: 0.1077\n",
      "Epoch [89/100], Step [80/84], Loss: 0.1104\n",
      "Epoch [89/100], Step [81/84], Loss: 0.1284\n",
      "Epoch [89/100], Step [82/84], Loss: 0.1120\n",
      "Epoch [89/100], Step [83/84], Loss: 0.1306\n",
      "Epoch [89/100], Step [84/84], Loss: 0.1308\n",
      "Epoch [90/100], Step [1/84], Loss: 0.1448\n",
      "Epoch [90/100], Step [2/84], Loss: 0.1193\n",
      "Epoch [90/100], Step [3/84], Loss: 0.0983\n",
      "Epoch [90/100], Step [4/84], Loss: 0.1133\n",
      "Epoch [90/100], Step [5/84], Loss: 0.1185\n",
      "Epoch [90/100], Step [6/84], Loss: 0.1177\n",
      "Epoch [90/100], Step [7/84], Loss: 0.1153\n",
      "Epoch [90/100], Step [8/84], Loss: 0.1268\n",
      "Epoch [90/100], Step [9/84], Loss: 0.1301\n",
      "Epoch [90/100], Step [10/84], Loss: 0.1395\n",
      "Epoch [90/100], Step [11/84], Loss: 0.1172\n",
      "Epoch [90/100], Step [12/84], Loss: 0.1200\n",
      "Epoch [90/100], Step [13/84], Loss: 0.1119\n",
      "Epoch [90/100], Step [14/84], Loss: 0.1132\n",
      "Epoch [90/100], Step [15/84], Loss: 0.1247\n",
      "Epoch [90/100], Step [16/84], Loss: 0.1016\n",
      "Epoch [90/100], Step [17/84], Loss: 0.1129\n",
      "Epoch [90/100], Step [18/84], Loss: 0.1207\n",
      "Epoch [90/100], Step [19/84], Loss: 0.1143\n",
      "Epoch [90/100], Step [20/84], Loss: 0.1116\n",
      "Epoch [90/100], Step [21/84], Loss: 0.0883\n",
      "Epoch [90/100], Step [22/84], Loss: 0.0954\n",
      "Epoch [90/100], Step [23/84], Loss: 0.1015\n",
      "Epoch [90/100], Step [24/84], Loss: 0.1072\n",
      "Epoch [90/100], Step [25/84], Loss: 0.1149\n",
      "Epoch [90/100], Step [26/84], Loss: 0.1314\n",
      "Epoch [90/100], Step [27/84], Loss: 0.1111\n",
      "Epoch [90/100], Step [28/84], Loss: 0.1009\n",
      "Epoch [90/100], Step [29/84], Loss: 0.1060\n",
      "Epoch [90/100], Step [30/84], Loss: 0.1103\n",
      "Epoch [90/100], Step [31/84], Loss: 0.1115\n",
      "Epoch [90/100], Step [32/84], Loss: 0.0996\n",
      "Epoch [90/100], Step [33/84], Loss: 0.0956\n",
      "Epoch [90/100], Step [34/84], Loss: 0.1055\n",
      "Epoch [90/100], Step [35/84], Loss: 0.1165\n",
      "Epoch [90/100], Step [36/84], Loss: 0.1131\n",
      "Epoch [90/100], Step [37/84], Loss: 0.1349\n",
      "Epoch [90/100], Step [38/84], Loss: 0.1134\n",
      "Epoch [90/100], Step [39/84], Loss: 0.1167\n",
      "Epoch [90/100], Step [40/84], Loss: 0.1058\n",
      "Epoch [90/100], Step [41/84], Loss: 0.1089\n",
      "Epoch [90/100], Step [42/84], Loss: 0.1167\n",
      "Epoch [90/100], Step [43/84], Loss: 0.1125\n",
      "Epoch [90/100], Step [44/84], Loss: 0.1101\n",
      "Epoch [90/100], Step [45/84], Loss: 0.1106\n",
      "Epoch [90/100], Step [46/84], Loss: 0.1053\n",
      "Epoch [90/100], Step [47/84], Loss: 0.1021\n",
      "Epoch [90/100], Step [48/84], Loss: 0.1191\n",
      "Epoch [90/100], Step [49/84], Loss: 0.1016\n",
      "Epoch [90/100], Step [50/84], Loss: 0.0975\n",
      "Epoch [90/100], Step [51/84], Loss: 0.1154\n",
      "Epoch [90/100], Step [52/84], Loss: 0.0992\n",
      "Epoch [90/100], Step [53/84], Loss: 0.1005\n",
      "Epoch [90/100], Step [54/84], Loss: 0.1027\n",
      "Epoch [90/100], Step [55/84], Loss: 0.1240\n",
      "Epoch [90/100], Step [56/84], Loss: 0.1726\n",
      "Epoch [90/100], Step [57/84], Loss: 0.0967\n",
      "Epoch [90/100], Step [58/84], Loss: 0.0950\n",
      "Epoch [90/100], Step [59/84], Loss: 0.1310\n",
      "Epoch [90/100], Step [60/84], Loss: 0.1132\n",
      "Epoch [90/100], Step [61/84], Loss: 0.1421\n",
      "Epoch [90/100], Step [62/84], Loss: 0.1071\n",
      "Epoch [90/100], Step [63/84], Loss: 0.1156\n",
      "Epoch [90/100], Step [64/84], Loss: 0.0992\n",
      "Epoch [90/100], Step [65/84], Loss: 0.1141\n",
      "Epoch [90/100], Step [66/84], Loss: 0.1078\n",
      "Epoch [90/100], Step [67/84], Loss: 0.1138\n",
      "Epoch [90/100], Step [68/84], Loss: 0.1030\n",
      "Epoch [90/100], Step [69/84], Loss: 0.1107\n",
      "Epoch [90/100], Step [70/84], Loss: 0.1078\n",
      "Epoch [90/100], Step [71/84], Loss: 0.1196\n",
      "Epoch [90/100], Step [72/84], Loss: 0.1017\n",
      "Epoch [90/100], Step [73/84], Loss: 0.1132\n",
      "Epoch [90/100], Step [74/84], Loss: 0.1112\n",
      "Epoch [90/100], Step [75/84], Loss: 0.1190\n",
      "Epoch [90/100], Step [76/84], Loss: 0.1226\n",
      "Epoch [90/100], Step [77/84], Loss: 0.1288\n",
      "Epoch [90/100], Step [78/84], Loss: 0.1014\n",
      "Epoch [90/100], Step [79/84], Loss: 0.1486\n",
      "Epoch [90/100], Step [80/84], Loss: 0.1090\n",
      "Epoch [90/100], Step [81/84], Loss: 0.1168\n",
      "Epoch [90/100], Step [82/84], Loss: 0.1042\n",
      "Epoch [90/100], Step [83/84], Loss: 0.1109\n",
      "Epoch [90/100], Step [84/84], Loss: 0.1468\n",
      "Epoch [91/100], Step [1/84], Loss: 0.1123\n",
      "Epoch [91/100], Step [2/84], Loss: 0.0988\n",
      "Epoch [91/100], Step [3/84], Loss: 0.1006\n",
      "Epoch [91/100], Step [4/84], Loss: 0.1382\n",
      "Epoch [91/100], Step [5/84], Loss: 0.1095\n",
      "Epoch [91/100], Step [6/84], Loss: 0.0997\n",
      "Epoch [91/100], Step [7/84], Loss: 0.1199\n",
      "Epoch [91/100], Step [8/84], Loss: 0.1008\n",
      "Epoch [91/100], Step [9/84], Loss: 0.0952\n",
      "Epoch [91/100], Step [10/84], Loss: 0.1103\n",
      "Epoch [91/100], Step [11/84], Loss: 0.1238\n",
      "Epoch [91/100], Step [12/84], Loss: 0.0960\n",
      "Epoch [91/100], Step [13/84], Loss: 0.1147\n",
      "Epoch [91/100], Step [14/84], Loss: 0.1285\n",
      "Epoch [91/100], Step [15/84], Loss: 0.1283\n",
      "Epoch [91/100], Step [16/84], Loss: 0.1151\n",
      "Epoch [91/100], Step [17/84], Loss: 0.0998\n",
      "Epoch [91/100], Step [18/84], Loss: 0.1054\n",
      "Epoch [91/100], Step [19/84], Loss: 0.1019\n",
      "Epoch [91/100], Step [20/84], Loss: 0.1058\n",
      "Epoch [91/100], Step [21/84], Loss: 0.1071\n",
      "Epoch [91/100], Step [22/84], Loss: 0.1133\n",
      "Epoch [91/100], Step [23/84], Loss: 0.1075\n",
      "Epoch [91/100], Step [24/84], Loss: 0.1653\n",
      "Epoch [91/100], Step [25/84], Loss: 0.0983\n",
      "Epoch [91/100], Step [26/84], Loss: 0.1128\n",
      "Epoch [91/100], Step [27/84], Loss: 0.1093\n",
      "Epoch [91/100], Step [28/84], Loss: 0.1134\n",
      "Epoch [91/100], Step [29/84], Loss: 0.1223\n",
      "Epoch [91/100], Step [30/84], Loss: 0.0989\n",
      "Epoch [91/100], Step [31/84], Loss: 0.1288\n",
      "Epoch [91/100], Step [32/84], Loss: 0.1038\n",
      "Epoch [91/100], Step [33/84], Loss: 0.1138\n",
      "Epoch [91/100], Step [34/84], Loss: 0.0963\n",
      "Epoch [91/100], Step [35/84], Loss: 0.0989\n",
      "Epoch [91/100], Step [36/84], Loss: 0.1067\n",
      "Epoch [91/100], Step [37/84], Loss: 0.0914\n",
      "Epoch [91/100], Step [38/84], Loss: 0.1007\n",
      "Epoch [91/100], Step [39/84], Loss: 0.0939\n",
      "Epoch [91/100], Step [40/84], Loss: 0.1024\n",
      "Epoch [91/100], Step [41/84], Loss: 0.0941\n",
      "Epoch [91/100], Step [42/84], Loss: 0.1074\n",
      "Epoch [91/100], Step [43/84], Loss: 0.0916\n",
      "Epoch [91/100], Step [44/84], Loss: 0.1009\n",
      "Epoch [91/100], Step [45/84], Loss: 0.1246\n",
      "Epoch [91/100], Step [46/84], Loss: 0.1062\n",
      "Epoch [91/100], Step [47/84], Loss: 0.1134\n",
      "Epoch [91/100], Step [48/84], Loss: 0.1009\n",
      "Epoch [91/100], Step [49/84], Loss: 0.0940\n",
      "Epoch [91/100], Step [50/84], Loss: 0.1068\n",
      "Epoch [91/100], Step [51/84], Loss: 0.1044\n",
      "Epoch [91/100], Step [52/84], Loss: 0.1059\n",
      "Epoch [91/100], Step [53/84], Loss: 0.0938\n",
      "Epoch [91/100], Step [54/84], Loss: 0.1164\n",
      "Epoch [91/100], Step [55/84], Loss: 0.1002\n",
      "Epoch [91/100], Step [56/84], Loss: 0.0983\n",
      "Epoch [91/100], Step [57/84], Loss: 0.1001\n",
      "Epoch [91/100], Step [58/84], Loss: 0.0917\n",
      "Epoch [91/100], Step [59/84], Loss: 0.1180\n",
      "Epoch [91/100], Step [60/84], Loss: 0.1210\n",
      "Epoch [91/100], Step [61/84], Loss: 0.1230\n",
      "Epoch [91/100], Step [62/84], Loss: 0.1322\n",
      "Epoch [91/100], Step [63/84], Loss: 0.0922\n",
      "Epoch [91/100], Step [64/84], Loss: 0.1281\n",
      "Epoch [91/100], Step [65/84], Loss: 0.1217\n",
      "Epoch [91/100], Step [66/84], Loss: 0.1032\n",
      "Epoch [91/100], Step [67/84], Loss: 0.1266\n",
      "Epoch [91/100], Step [68/84], Loss: 0.1087\n",
      "Epoch [91/100], Step [69/84], Loss: 0.0974\n",
      "Epoch [91/100], Step [70/84], Loss: 0.0994\n",
      "Epoch [91/100], Step [71/84], Loss: 0.1267\n",
      "Epoch [91/100], Step [72/84], Loss: 0.1083\n",
      "Epoch [91/100], Step [73/84], Loss: 0.1168\n",
      "Epoch [91/100], Step [74/84], Loss: 0.1110\n",
      "Epoch [91/100], Step [75/84], Loss: 0.1040\n",
      "Epoch [91/100], Step [76/84], Loss: 0.1277\n",
      "Epoch [91/100], Step [77/84], Loss: 0.1106\n",
      "Epoch [91/100], Step [78/84], Loss: 0.1378\n",
      "Epoch [91/100], Step [79/84], Loss: 0.1283\n",
      "Epoch [91/100], Step [80/84], Loss: 0.1217\n",
      "Epoch [91/100], Step [81/84], Loss: 0.1129\n",
      "Epoch [91/100], Step [82/84], Loss: 0.1016\n",
      "Epoch [91/100], Step [83/84], Loss: 0.1053\n",
      "Epoch [91/100], Step [84/84], Loss: 0.1441\n",
      "Epoch [92/100], Step [1/84], Loss: 0.1056\n",
      "Epoch [92/100], Step [2/84], Loss: 0.1022\n",
      "Epoch [92/100], Step [3/84], Loss: 0.1573\n",
      "Epoch [92/100], Step [4/84], Loss: 0.1167\n",
      "Epoch [92/100], Step [5/84], Loss: 0.1162\n",
      "Epoch [92/100], Step [6/84], Loss: 0.1405\n",
      "Epoch [92/100], Step [7/84], Loss: 0.1098\n",
      "Epoch [92/100], Step [8/84], Loss: 0.1054\n",
      "Epoch [92/100], Step [9/84], Loss: 0.1322\n",
      "Epoch [92/100], Step [10/84], Loss: 0.1165\n",
      "Epoch [92/100], Step [11/84], Loss: 0.1439\n",
      "Epoch [92/100], Step [12/84], Loss: 0.1215\n",
      "Epoch [92/100], Step [13/84], Loss: 0.1502\n",
      "Epoch [92/100], Step [14/84], Loss: 0.1036\n",
      "Epoch [92/100], Step [15/84], Loss: 0.1203\n",
      "Epoch [92/100], Step [16/84], Loss: 0.1097\n",
      "Epoch [92/100], Step [17/84], Loss: 0.1257\n",
      "Epoch [92/100], Step [18/84], Loss: 0.1045\n",
      "Epoch [92/100], Step [19/84], Loss: 0.0988\n",
      "Epoch [92/100], Step [20/84], Loss: 0.1071\n",
      "Epoch [92/100], Step [21/84], Loss: 0.2011\n",
      "Epoch [92/100], Step [22/84], Loss: 0.1005\n",
      "Epoch [92/100], Step [23/84], Loss: 0.1128\n",
      "Epoch [92/100], Step [24/84], Loss: 0.1132\n",
      "Epoch [92/100], Step [25/84], Loss: 0.1151\n",
      "Epoch [92/100], Step [26/84], Loss: 0.1038\n",
      "Epoch [92/100], Step [27/84], Loss: 0.1365\n",
      "Epoch [92/100], Step [28/84], Loss: 0.1065\n",
      "Epoch [92/100], Step [29/84], Loss: 0.1123\n",
      "Epoch [92/100], Step [30/84], Loss: 0.1045\n",
      "Epoch [92/100], Step [31/84], Loss: 0.1496\n",
      "Epoch [92/100], Step [32/84], Loss: 0.1041\n",
      "Epoch [92/100], Step [33/84], Loss: 0.1128\n",
      "Epoch [92/100], Step [34/84], Loss: 0.1047\n",
      "Epoch [92/100], Step [35/84], Loss: 0.0991\n",
      "Epoch [92/100], Step [36/84], Loss: 0.1170\n",
      "Epoch [92/100], Step [37/84], Loss: 0.0934\n",
      "Epoch [92/100], Step [38/84], Loss: 0.1092\n",
      "Epoch [92/100], Step [39/84], Loss: 0.1234\n",
      "Epoch [92/100], Step [40/84], Loss: 0.1146\n",
      "Epoch [92/100], Step [41/84], Loss: 0.0966\n",
      "Epoch [92/100], Step [42/84], Loss: 0.1111\n",
      "Epoch [92/100], Step [43/84], Loss: 0.1028\n",
      "Epoch [92/100], Step [44/84], Loss: 0.1159\n",
      "Epoch [92/100], Step [45/84], Loss: 0.1097\n",
      "Epoch [92/100], Step [46/84], Loss: 0.0954\n",
      "Epoch [92/100], Step [47/84], Loss: 0.1070\n",
      "Epoch [92/100], Step [48/84], Loss: 0.0961\n",
      "Epoch [92/100], Step [49/84], Loss: 0.1119\n",
      "Epoch [92/100], Step [50/84], Loss: 0.1175\n",
      "Epoch [92/100], Step [51/84], Loss: 0.1130\n",
      "Epoch [92/100], Step [52/84], Loss: 0.1094\n",
      "Epoch [92/100], Step [53/84], Loss: 0.1272\n",
      "Epoch [92/100], Step [54/84], Loss: 0.1072\n",
      "Epoch [92/100], Step [55/84], Loss: 0.1039\n",
      "Epoch [92/100], Step [56/84], Loss: 0.1037\n",
      "Epoch [92/100], Step [57/84], Loss: 0.0989\n",
      "Epoch [92/100], Step [58/84], Loss: 0.1067\n",
      "Epoch [92/100], Step [59/84], Loss: 0.0881\n",
      "Epoch [92/100], Step [60/84], Loss: 0.0946\n",
      "Epoch [92/100], Step [61/84], Loss: 0.1014\n",
      "Epoch [92/100], Step [62/84], Loss: 0.1105\n",
      "Epoch [92/100], Step [63/84], Loss: 0.0992\n",
      "Epoch [92/100], Step [64/84], Loss: 0.1006\n",
      "Epoch [92/100], Step [65/84], Loss: 0.0876\n",
      "Epoch [92/100], Step [66/84], Loss: 0.1005\n",
      "Epoch [92/100], Step [67/84], Loss: 0.1254\n",
      "Epoch [92/100], Step [68/84], Loss: 0.0914\n",
      "Epoch [92/100], Step [69/84], Loss: 0.1276\n",
      "Epoch [92/100], Step [70/84], Loss: 0.0932\n",
      "Epoch [92/100], Step [71/84], Loss: 0.0998\n",
      "Epoch [92/100], Step [72/84], Loss: 0.1262\n",
      "Epoch [92/100], Step [73/84], Loss: 0.0878\n",
      "Epoch [92/100], Step [74/84], Loss: 0.1081\n",
      "Epoch [92/100], Step [75/84], Loss: 0.1112\n",
      "Epoch [92/100], Step [76/84], Loss: 0.1138\n",
      "Epoch [92/100], Step [77/84], Loss: 0.1191\n",
      "Epoch [92/100], Step [78/84], Loss: 0.1059\n",
      "Epoch [92/100], Step [79/84], Loss: 0.0967\n",
      "Epoch [92/100], Step [80/84], Loss: 0.0985\n",
      "Epoch [92/100], Step [81/84], Loss: 0.1011\n",
      "Epoch [92/100], Step [82/84], Loss: 0.1112\n",
      "Epoch [92/100], Step [83/84], Loss: 0.1091\n",
      "Epoch [92/100], Step [84/84], Loss: 0.1079\n",
      "Epoch [93/100], Step [1/84], Loss: 0.0990\n",
      "Epoch [93/100], Step [2/84], Loss: 0.1408\n",
      "Epoch [93/100], Step [3/84], Loss: 0.0929\n",
      "Epoch [93/100], Step [4/84], Loss: 0.1055\n",
      "Epoch [93/100], Step [5/84], Loss: 0.1137\n",
      "Epoch [93/100], Step [6/84], Loss: 0.1669\n",
      "Epoch [93/100], Step [7/84], Loss: 0.1064\n",
      "Epoch [93/100], Step [8/84], Loss: 0.0940\n",
      "Epoch [93/100], Step [9/84], Loss: 0.1172\n",
      "Epoch [93/100], Step [10/84], Loss: 0.1170\n",
      "Epoch [93/100], Step [11/84], Loss: 0.0990\n",
      "Epoch [93/100], Step [12/84], Loss: 0.1129\n",
      "Epoch [93/100], Step [13/84], Loss: 0.1144\n",
      "Epoch [93/100], Step [14/84], Loss: 0.1001\n",
      "Epoch [93/100], Step [15/84], Loss: 0.0990\n",
      "Epoch [93/100], Step [16/84], Loss: 0.0955\n",
      "Epoch [93/100], Step [17/84], Loss: 0.0970\n",
      "Epoch [93/100], Step [18/84], Loss: 0.1157\n",
      "Epoch [93/100], Step [19/84], Loss: 0.1746\n",
      "Epoch [93/100], Step [20/84], Loss: 0.1021\n",
      "Epoch [93/100], Step [21/84], Loss: 0.1229\n",
      "Epoch [93/100], Step [22/84], Loss: 0.1150\n",
      "Epoch [93/100], Step [23/84], Loss: 0.1081\n",
      "Epoch [93/100], Step [24/84], Loss: 0.1047\n",
      "Epoch [93/100], Step [25/84], Loss: 0.1124\n",
      "Epoch [93/100], Step [26/84], Loss: 0.1039\n",
      "Epoch [93/100], Step [27/84], Loss: 0.1098\n",
      "Epoch [93/100], Step [28/84], Loss: 0.1047\n",
      "Epoch [93/100], Step [29/84], Loss: 0.1221\n",
      "Epoch [93/100], Step [30/84], Loss: 0.0952\n",
      "Epoch [93/100], Step [31/84], Loss: 0.1215\n",
      "Epoch [93/100], Step [32/84], Loss: 0.1161\n",
      "Epoch [93/100], Step [33/84], Loss: 0.0970\n",
      "Epoch [93/100], Step [34/84], Loss: 0.1011\n",
      "Epoch [93/100], Step [35/84], Loss: 0.0964\n",
      "Epoch [93/100], Step [36/84], Loss: 0.0969\n",
      "Epoch [93/100], Step [37/84], Loss: 0.0946\n",
      "Epoch [93/100], Step [38/84], Loss: 0.1345\n",
      "Epoch [93/100], Step [39/84], Loss: 0.0939\n",
      "Epoch [93/100], Step [40/84], Loss: 0.1011\n",
      "Epoch [93/100], Step [41/84], Loss: 0.1036\n",
      "Epoch [93/100], Step [42/84], Loss: 0.1294\n",
      "Epoch [93/100], Step [43/84], Loss: 0.0994\n",
      "Epoch [93/100], Step [44/84], Loss: 0.1162\n",
      "Epoch [93/100], Step [45/84], Loss: 0.1035\n",
      "Epoch [93/100], Step [46/84], Loss: 0.1109\n",
      "Epoch [93/100], Step [47/84], Loss: 0.1150\n",
      "Epoch [93/100], Step [48/84], Loss: 0.1179\n",
      "Epoch [93/100], Step [49/84], Loss: 0.1219\n",
      "Epoch [93/100], Step [50/84], Loss: 0.0957\n",
      "Epoch [93/100], Step [51/84], Loss: 0.0938\n",
      "Epoch [93/100], Step [52/84], Loss: 0.1130\n",
      "Epoch [93/100], Step [53/84], Loss: 0.1112\n",
      "Epoch [93/100], Step [54/84], Loss: 0.0993\n",
      "Epoch [93/100], Step [55/84], Loss: 0.1000\n",
      "Epoch [93/100], Step [56/84], Loss: 0.1040\n",
      "Epoch [93/100], Step [57/84], Loss: 0.1110\n",
      "Epoch [93/100], Step [58/84], Loss: 0.1784\n",
      "Epoch [93/100], Step [59/84], Loss: 0.0949\n",
      "Epoch [93/100], Step [60/84], Loss: 0.1299\n",
      "Epoch [93/100], Step [61/84], Loss: 0.0987\n",
      "Epoch [93/100], Step [62/84], Loss: 0.1221\n",
      "Epoch [93/100], Step [63/84], Loss: 0.1051\n",
      "Epoch [93/100], Step [64/84], Loss: 0.0945\n",
      "Epoch [93/100], Step [65/84], Loss: 0.1092\n",
      "Epoch [93/100], Step [66/84], Loss: 0.0951\n",
      "Epoch [93/100], Step [67/84], Loss: 0.1168\n",
      "Epoch [93/100], Step [68/84], Loss: 0.1120\n",
      "Epoch [93/100], Step [69/84], Loss: 0.1239\n",
      "Epoch [93/100], Step [70/84], Loss: 0.0996\n",
      "Epoch [93/100], Step [71/84], Loss: 0.1014\n",
      "Epoch [93/100], Step [72/84], Loss: 0.0956\n",
      "Epoch [93/100], Step [73/84], Loss: 0.1035\n",
      "Epoch [93/100], Step [74/84], Loss: 0.0982\n",
      "Epoch [93/100], Step [75/84], Loss: 0.1096\n",
      "Epoch [93/100], Step [76/84], Loss: 0.0886\n",
      "Epoch [93/100], Step [77/84], Loss: 0.1265\n",
      "Epoch [93/100], Step [78/84], Loss: 0.1002\n",
      "Epoch [93/100], Step [79/84], Loss: 0.1062\n",
      "Epoch [93/100], Step [80/84], Loss: 0.1202\n",
      "Epoch [93/100], Step [81/84], Loss: 0.0846\n",
      "Epoch [93/100], Step [82/84], Loss: 0.0899\n",
      "Epoch [93/100], Step [83/84], Loss: 0.1158\n",
      "Epoch [93/100], Step [84/84], Loss: 0.0940\n",
      "Epoch [94/100], Step [1/84], Loss: 0.0856\n",
      "Epoch [94/100], Step [2/84], Loss: 0.0925\n",
      "Epoch [94/100], Step [3/84], Loss: 0.0876\n",
      "Epoch [94/100], Step [4/84], Loss: 0.0930\n",
      "Epoch [94/100], Step [5/84], Loss: 0.0967\n",
      "Epoch [94/100], Step [6/84], Loss: 0.0887\n",
      "Epoch [94/100], Step [7/84], Loss: 0.0921\n",
      "Epoch [94/100], Step [8/84], Loss: 0.1259\n",
      "Epoch [94/100], Step [9/84], Loss: 0.1276\n",
      "Epoch [94/100], Step [10/84], Loss: 0.0964\n",
      "Epoch [94/100], Step [11/84], Loss: 0.0984\n",
      "Epoch [94/100], Step [12/84], Loss: 0.0928\n",
      "Epoch [94/100], Step [13/84], Loss: 0.0951\n",
      "Epoch [94/100], Step [14/84], Loss: 0.0978\n",
      "Epoch [94/100], Step [15/84], Loss: 0.1113\n",
      "Epoch [94/100], Step [16/84], Loss: 0.1146\n",
      "Epoch [94/100], Step [17/84], Loss: 0.0969\n",
      "Epoch [94/100], Step [18/84], Loss: 0.0988\n",
      "Epoch [94/100], Step [19/84], Loss: 0.0854\n",
      "Epoch [94/100], Step [20/84], Loss: 0.1076\n",
      "Epoch [94/100], Step [21/84], Loss: 0.1005\n",
      "Epoch [94/100], Step [22/84], Loss: 0.0957\n",
      "Epoch [94/100], Step [23/84], Loss: 0.1193\n",
      "Epoch [94/100], Step [24/84], Loss: 0.0848\n",
      "Epoch [94/100], Step [25/84], Loss: 0.1221\n",
      "Epoch [94/100], Step [26/84], Loss: 0.1018\n",
      "Epoch [94/100], Step [27/84], Loss: 0.1069\n",
      "Epoch [94/100], Step [28/84], Loss: 0.0979\n",
      "Epoch [94/100], Step [29/84], Loss: 0.1106\n",
      "Epoch [94/100], Step [30/84], Loss: 0.1226\n",
      "Epoch [94/100], Step [31/84], Loss: 0.1088\n",
      "Epoch [94/100], Step [32/84], Loss: 0.1256\n",
      "Epoch [94/100], Step [33/84], Loss: 0.0949\n",
      "Epoch [94/100], Step [34/84], Loss: 0.1123\n",
      "Epoch [94/100], Step [35/84], Loss: 0.0906\n",
      "Epoch [94/100], Step [36/84], Loss: 0.0968\n",
      "Epoch [94/100], Step [37/84], Loss: 0.0897\n",
      "Epoch [94/100], Step [38/84], Loss: 0.1088\n",
      "Epoch [94/100], Step [39/84], Loss: 0.0925\n",
      "Epoch [94/100], Step [40/84], Loss: 0.1016\n",
      "Epoch [94/100], Step [41/84], Loss: 0.0916\n",
      "Epoch [94/100], Step [42/84], Loss: 0.0839\n",
      "Epoch [94/100], Step [43/84], Loss: 0.0987\n",
      "Epoch [94/100], Step [44/84], Loss: 0.1076\n",
      "Epoch [94/100], Step [45/84], Loss: 0.1108\n",
      "Epoch [94/100], Step [46/84], Loss: 0.0928\n",
      "Epoch [94/100], Step [47/84], Loss: 0.1095\n",
      "Epoch [94/100], Step [48/84], Loss: 0.0810\n",
      "Epoch [94/100], Step [49/84], Loss: 0.1045\n",
      "Epoch [94/100], Step [50/84], Loss: 0.1147\n",
      "Epoch [94/100], Step [51/84], Loss: 0.1002\n",
      "Epoch [94/100], Step [52/84], Loss: 0.0918\n",
      "Epoch [94/100], Step [53/84], Loss: 0.0860\n",
      "Epoch [94/100], Step [54/84], Loss: 0.0931\n",
      "Epoch [94/100], Step [55/84], Loss: 0.1021\n",
      "Epoch [94/100], Step [56/84], Loss: 0.1154\n",
      "Epoch [94/100], Step [57/84], Loss: 0.1025\n",
      "Epoch [94/100], Step [58/84], Loss: 0.1225\n",
      "Epoch [94/100], Step [59/84], Loss: 0.1111\n",
      "Epoch [94/100], Step [60/84], Loss: 0.1130\n",
      "Epoch [94/100], Step [61/84], Loss: 0.1233\n",
      "Epoch [94/100], Step [62/84], Loss: 0.0887\n",
      "Epoch [94/100], Step [63/84], Loss: 0.1022\n",
      "Epoch [94/100], Step [64/84], Loss: 0.0942\n",
      "Epoch [94/100], Step [65/84], Loss: 0.1026\n",
      "Epoch [94/100], Step [66/84], Loss: 0.1193\n",
      "Epoch [94/100], Step [67/84], Loss: 0.0926\n",
      "Epoch [94/100], Step [68/84], Loss: 0.0910\n",
      "Epoch [94/100], Step [69/84], Loss: 0.0923\n",
      "Epoch [94/100], Step [70/84], Loss: 0.1034\n",
      "Epoch [94/100], Step [71/84], Loss: 0.1138\n",
      "Epoch [94/100], Step [72/84], Loss: 0.1055\n",
      "Epoch [94/100], Step [73/84], Loss: 0.0959\n",
      "Epoch [94/100], Step [74/84], Loss: 0.0958\n",
      "Epoch [94/100], Step [75/84], Loss: 0.1012\n",
      "Epoch [94/100], Step [76/84], Loss: 0.1242\n",
      "Epoch [94/100], Step [77/84], Loss: 0.1300\n",
      "Epoch [94/100], Step [78/84], Loss: 0.0915\n",
      "Epoch [94/100], Step [79/84], Loss: 0.1084\n",
      "Epoch [94/100], Step [80/84], Loss: 0.1000\n",
      "Epoch [94/100], Step [81/84], Loss: 0.1414\n",
      "Epoch [94/100], Step [82/84], Loss: 0.1058\n",
      "Epoch [94/100], Step [83/84], Loss: 0.1099\n",
      "Epoch [94/100], Step [84/84], Loss: 0.1088\n",
      "Epoch [95/100], Step [1/84], Loss: 0.0845\n",
      "Epoch [95/100], Step [2/84], Loss: 0.0998\n",
      "Epoch [95/100], Step [3/84], Loss: 0.1026\n",
      "Epoch [95/100], Step [4/84], Loss: 0.1012\n",
      "Epoch [95/100], Step [5/84], Loss: 0.0920\n",
      "Epoch [95/100], Step [6/84], Loss: 0.1162\n",
      "Epoch [95/100], Step [7/84], Loss: 0.1058\n",
      "Epoch [95/100], Step [8/84], Loss: 0.0894\n",
      "Epoch [95/100], Step [9/84], Loss: 0.0942\n",
      "Epoch [95/100], Step [10/84], Loss: 0.1047\n",
      "Epoch [95/100], Step [11/84], Loss: 0.0997\n",
      "Epoch [95/100], Step [12/84], Loss: 0.0868\n",
      "Epoch [95/100], Step [13/84], Loss: 0.1135\n",
      "Epoch [95/100], Step [14/84], Loss: 0.0958\n",
      "Epoch [95/100], Step [15/84], Loss: 0.0900\n",
      "Epoch [95/100], Step [16/84], Loss: 0.0927\n",
      "Epoch [95/100], Step [17/84], Loss: 0.1457\n",
      "Epoch [95/100], Step [18/84], Loss: 0.0813\n",
      "Epoch [95/100], Step [19/84], Loss: 0.1012\n",
      "Epoch [95/100], Step [20/84], Loss: 0.0865\n",
      "Epoch [95/100], Step [21/84], Loss: 0.1167\n",
      "Epoch [95/100], Step [22/84], Loss: 0.0898\n",
      "Epoch [95/100], Step [23/84], Loss: 0.1255\n",
      "Epoch [95/100], Step [24/84], Loss: 0.1038\n",
      "Epoch [95/100], Step [25/84], Loss: 0.1294\n",
      "Epoch [95/100], Step [26/84], Loss: 0.1253\n",
      "Epoch [95/100], Step [27/84], Loss: 0.1211\n",
      "Epoch [95/100], Step [28/84], Loss: 0.1319\n",
      "Epoch [95/100], Step [29/84], Loss: 0.0963\n",
      "Epoch [95/100], Step [30/84], Loss: 0.1021\n",
      "Epoch [95/100], Step [31/84], Loss: 0.1136\n",
      "Epoch [95/100], Step [32/84], Loss: 0.1016\n",
      "Epoch [95/100], Step [33/84], Loss: 0.1147\n",
      "Epoch [95/100], Step [34/84], Loss: 0.1387\n",
      "Epoch [95/100], Step [35/84], Loss: 0.1150\n",
      "Epoch [95/100], Step [36/84], Loss: 0.0993\n",
      "Epoch [95/100], Step [37/84], Loss: 0.1187\n",
      "Epoch [95/100], Step [38/84], Loss: 0.1233\n",
      "Epoch [95/100], Step [39/84], Loss: 0.0989\n",
      "Epoch [95/100], Step [40/84], Loss: 0.0957\n",
      "Epoch [95/100], Step [41/84], Loss: 0.0974\n",
      "Epoch [95/100], Step [42/84], Loss: 0.1206\n",
      "Epoch [95/100], Step [43/84], Loss: 0.1156\n",
      "Epoch [95/100], Step [44/84], Loss: 0.1339\n",
      "Epoch [95/100], Step [45/84], Loss: 0.0993\n",
      "Epoch [95/100], Step [46/84], Loss: 0.0991\n",
      "Epoch [95/100], Step [47/84], Loss: 0.1028\n",
      "Epoch [95/100], Step [48/84], Loss: 0.1098\n",
      "Epoch [95/100], Step [49/84], Loss: 0.0961\n",
      "Epoch [95/100], Step [50/84], Loss: 0.1286\n",
      "Epoch [95/100], Step [51/84], Loss: 0.0838\n",
      "Epoch [95/100], Step [52/84], Loss: 0.0988\n",
      "Epoch [95/100], Step [53/84], Loss: 0.1098\n",
      "Epoch [95/100], Step [54/84], Loss: 0.1237\n",
      "Epoch [95/100], Step [55/84], Loss: 0.1118\n",
      "Epoch [95/100], Step [56/84], Loss: 0.1001\n",
      "Epoch [95/100], Step [57/84], Loss: 0.1072\n",
      "Epoch [95/100], Step [58/84], Loss: 0.1048\n",
      "Epoch [95/100], Step [59/84], Loss: 0.0993\n",
      "Epoch [95/100], Step [60/84], Loss: 0.1265\n",
      "Epoch [95/100], Step [61/84], Loss: 0.1033\n",
      "Epoch [95/100], Step [62/84], Loss: 0.0924\n",
      "Epoch [95/100], Step [63/84], Loss: 0.1033\n",
      "Epoch [95/100], Step [64/84], Loss: 0.0978\n",
      "Epoch [95/100], Step [65/84], Loss: 0.1285\n",
      "Epoch [95/100], Step [66/84], Loss: 0.1174\n",
      "Epoch [95/100], Step [67/84], Loss: 0.1231\n",
      "Epoch [95/100], Step [68/84], Loss: 0.1176\n",
      "Epoch [95/100], Step [69/84], Loss: 0.1192\n",
      "Epoch [95/100], Step [70/84], Loss: 0.0959\n",
      "Epoch [95/100], Step [71/84], Loss: 0.1048\n",
      "Epoch [95/100], Step [72/84], Loss: 0.1087\n",
      "Epoch [95/100], Step [73/84], Loss: 0.1125\n",
      "Epoch [95/100], Step [74/84], Loss: 0.0820\n",
      "Epoch [95/100], Step [75/84], Loss: 0.1071\n",
      "Epoch [95/100], Step [76/84], Loss: 0.1056\n",
      "Epoch [95/100], Step [77/84], Loss: 0.1129\n",
      "Epoch [95/100], Step [78/84], Loss: 0.0896\n",
      "Epoch [95/100], Step [79/84], Loss: 0.1139\n",
      "Epoch [95/100], Step [80/84], Loss: 0.1051\n",
      "Epoch [95/100], Step [81/84], Loss: 0.1007\n",
      "Epoch [95/100], Step [82/84], Loss: 0.1029\n",
      "Epoch [95/100], Step [83/84], Loss: 0.1083\n",
      "Epoch [95/100], Step [84/84], Loss: 0.1259\n",
      "Epoch [96/100], Step [1/84], Loss: 0.1016\n",
      "Epoch [96/100], Step [2/84], Loss: 0.0990\n",
      "Epoch [96/100], Step [3/84], Loss: 0.0927\n",
      "Epoch [96/100], Step [4/84], Loss: 0.1265\n",
      "Epoch [96/100], Step [5/84], Loss: 0.1147\n",
      "Epoch [96/100], Step [6/84], Loss: 0.0843\n",
      "Epoch [96/100], Step [7/84], Loss: 0.0951\n",
      "Epoch [96/100], Step [8/84], Loss: 0.1149\n",
      "Epoch [96/100], Step [9/84], Loss: 0.0906\n",
      "Epoch [96/100], Step [10/84], Loss: 0.0808\n",
      "Epoch [96/100], Step [11/84], Loss: 0.1272\n",
      "Epoch [96/100], Step [12/84], Loss: 0.0920\n",
      "Epoch [96/100], Step [13/84], Loss: 0.1097\n",
      "Epoch [96/100], Step [14/84], Loss: 0.1128\n",
      "Epoch [96/100], Step [15/84], Loss: 0.1016\n",
      "Epoch [96/100], Step [16/84], Loss: 0.1090\n",
      "Epoch [96/100], Step [17/84], Loss: 0.0946\n",
      "Epoch [96/100], Step [18/84], Loss: 0.0994\n",
      "Epoch [96/100], Step [19/84], Loss: 0.0892\n",
      "Epoch [96/100], Step [20/84], Loss: 0.1145\n",
      "Epoch [96/100], Step [21/84], Loss: 0.1056\n",
      "Epoch [96/100], Step [22/84], Loss: 0.1115\n",
      "Epoch [96/100], Step [23/84], Loss: 0.1002\n",
      "Epoch [96/100], Step [24/84], Loss: 0.0857\n",
      "Epoch [96/100], Step [25/84], Loss: 0.1401\n",
      "Epoch [96/100], Step [26/84], Loss: 0.0845\n",
      "Epoch [96/100], Step [27/84], Loss: 0.1036\n",
      "Epoch [96/100], Step [28/84], Loss: 0.0986\n",
      "Epoch [96/100], Step [29/84], Loss: 0.0990\n",
      "Epoch [96/100], Step [30/84], Loss: 0.1098\n",
      "Epoch [96/100], Step [31/84], Loss: 0.0996\n",
      "Epoch [96/100], Step [32/84], Loss: 0.0922\n",
      "Epoch [96/100], Step [33/84], Loss: 0.1099\n",
      "Epoch [96/100], Step [34/84], Loss: 0.1077\n",
      "Epoch [96/100], Step [35/84], Loss: 0.1153\n",
      "Epoch [96/100], Step [36/84], Loss: 0.0923\n",
      "Epoch [96/100], Step [37/84], Loss: 0.0985\n",
      "Epoch [96/100], Step [38/84], Loss: 0.1027\n",
      "Epoch [96/100], Step [39/84], Loss: 0.1071\n",
      "Epoch [96/100], Step [40/84], Loss: 0.0982\n",
      "Epoch [96/100], Step [41/84], Loss: 0.0885\n",
      "Epoch [96/100], Step [42/84], Loss: 0.0991\n",
      "Epoch [96/100], Step [43/84], Loss: 0.0886\n",
      "Epoch [96/100], Step [44/84], Loss: 0.1101\n",
      "Epoch [96/100], Step [45/84], Loss: 0.0903\n",
      "Epoch [96/100], Step [46/84], Loss: 0.0878\n",
      "Epoch [96/100], Step [47/84], Loss: 0.0964\n",
      "Epoch [96/100], Step [48/84], Loss: 0.0974\n",
      "Epoch [96/100], Step [49/84], Loss: 0.0758\n",
      "Epoch [96/100], Step [50/84], Loss: 0.1008\n",
      "Epoch [96/100], Step [51/84], Loss: 0.0951\n",
      "Epoch [96/100], Step [52/84], Loss: 0.0897\n",
      "Epoch [96/100], Step [53/84], Loss: 0.1089\n",
      "Epoch [96/100], Step [54/84], Loss: 0.0892\n",
      "Epoch [96/100], Step [55/84], Loss: 0.0978\n",
      "Epoch [96/100], Step [56/84], Loss: 0.0983\n",
      "Epoch [96/100], Step [57/84], Loss: 0.0898\n",
      "Epoch [96/100], Step [58/84], Loss: 0.1170\n",
      "Epoch [96/100], Step [59/84], Loss: 0.0996\n",
      "Epoch [96/100], Step [60/84], Loss: 0.0972\n",
      "Epoch [96/100], Step [61/84], Loss: 0.0945\n",
      "Epoch [96/100], Step [62/84], Loss: 0.1033\n",
      "Epoch [96/100], Step [63/84], Loss: 0.1280\n",
      "Epoch [96/100], Step [64/84], Loss: 0.0882\n",
      "Epoch [96/100], Step [65/84], Loss: 0.0882\n",
      "Epoch [96/100], Step [66/84], Loss: 0.1045\n",
      "Epoch [96/100], Step [67/84], Loss: 0.0919\n",
      "Epoch [96/100], Step [68/84], Loss: 0.0880\n",
      "Epoch [96/100], Step [69/84], Loss: 0.1591\n",
      "Epoch [96/100], Step [70/84], Loss: 0.1034\n",
      "Epoch [96/100], Step [71/84], Loss: 0.1274\n",
      "Epoch [96/100], Step [72/84], Loss: 0.1037\n",
      "Epoch [96/100], Step [73/84], Loss: 0.1103\n",
      "Epoch [96/100], Step [74/84], Loss: 0.1151\n",
      "Epoch [96/100], Step [75/84], Loss: 0.1549\n",
      "Epoch [96/100], Step [76/84], Loss: 0.1216\n",
      "Epoch [96/100], Step [77/84], Loss: 0.0945\n",
      "Epoch [96/100], Step [78/84], Loss: 0.1253\n",
      "Epoch [96/100], Step [79/84], Loss: 0.1005\n",
      "Epoch [96/100], Step [80/84], Loss: 0.0969\n",
      "Epoch [96/100], Step [81/84], Loss: 0.0972\n",
      "Epoch [96/100], Step [82/84], Loss: 0.0968\n",
      "Epoch [96/100], Step [83/84], Loss: 0.0841\n",
      "Epoch [96/100], Step [84/84], Loss: 0.1131\n",
      "Epoch [97/100], Step [1/84], Loss: 0.0865\n",
      "Epoch [97/100], Step [2/84], Loss: 0.0833\n",
      "Epoch [97/100], Step [3/84], Loss: 0.0943\n",
      "Epoch [97/100], Step [4/84], Loss: 0.0836\n",
      "Epoch [97/100], Step [5/84], Loss: 0.0870\n",
      "Epoch [97/100], Step [6/84], Loss: 0.1005\n",
      "Epoch [97/100], Step [7/84], Loss: 0.0884\n",
      "Epoch [97/100], Step [8/84], Loss: 0.1341\n",
      "Epoch [97/100], Step [9/84], Loss: 0.0803\n",
      "Epoch [97/100], Step [10/84], Loss: 0.1181\n",
      "Epoch [97/100], Step [11/84], Loss: 0.0887\n",
      "Epoch [97/100], Step [12/84], Loss: 0.0972\n",
      "Epoch [97/100], Step [13/84], Loss: 0.0913\n",
      "Epoch [97/100], Step [14/84], Loss: 0.0998\n",
      "Epoch [97/100], Step [15/84], Loss: 0.1072\n",
      "Epoch [97/100], Step [16/84], Loss: 0.1040\n",
      "Epoch [97/100], Step [17/84], Loss: 0.1230\n",
      "Epoch [97/100], Step [18/84], Loss: 0.1005\n",
      "Epoch [97/100], Step [19/84], Loss: 0.0971\n",
      "Epoch [97/100], Step [20/84], Loss: 0.0991\n",
      "Epoch [97/100], Step [21/84], Loss: 0.0852\n",
      "Epoch [97/100], Step [22/84], Loss: 0.0971\n",
      "Epoch [97/100], Step [23/84], Loss: 0.0841\n",
      "Epoch [97/100], Step [24/84], Loss: 0.1098\n",
      "Epoch [97/100], Step [25/84], Loss: 0.0912\n",
      "Epoch [97/100], Step [26/84], Loss: 0.1229\n",
      "Epoch [97/100], Step [27/84], Loss: 0.0925\n",
      "Epoch [97/100], Step [28/84], Loss: 0.1241\n",
      "Epoch [97/100], Step [29/84], Loss: 0.0958\n",
      "Epoch [97/100], Step [30/84], Loss: 0.0916\n",
      "Epoch [97/100], Step [31/84], Loss: 0.1263\n",
      "Epoch [97/100], Step [32/84], Loss: 0.0966\n",
      "Epoch [97/100], Step [33/84], Loss: 0.0819\n",
      "Epoch [97/100], Step [34/84], Loss: 0.1276\n",
      "Epoch [97/100], Step [35/84], Loss: 0.0878\n",
      "Epoch [97/100], Step [36/84], Loss: 0.1260\n",
      "Epoch [97/100], Step [37/84], Loss: 0.0889\n",
      "Epoch [97/100], Step [38/84], Loss: 0.0957\n",
      "Epoch [97/100], Step [39/84], Loss: 0.0990\n",
      "Epoch [97/100], Step [40/84], Loss: 0.1010\n",
      "Epoch [97/100], Step [41/84], Loss: 0.1017\n",
      "Epoch [97/100], Step [42/84], Loss: 0.0842\n",
      "Epoch [97/100], Step [43/84], Loss: 0.0925\n",
      "Epoch [97/100], Step [44/84], Loss: 0.0991\n",
      "Epoch [97/100], Step [45/84], Loss: 0.1103\n",
      "Epoch [97/100], Step [46/84], Loss: 0.0850\n",
      "Epoch [97/100], Step [47/84], Loss: 0.0831\n",
      "Epoch [97/100], Step [48/84], Loss: 0.0910\n",
      "Epoch [97/100], Step [49/84], Loss: 0.1001\n",
      "Epoch [97/100], Step [50/84], Loss: 0.0978\n",
      "Epoch [97/100], Step [51/84], Loss: 0.1036\n",
      "Epoch [97/100], Step [52/84], Loss: 0.0901\n",
      "Epoch [97/100], Step [53/84], Loss: 0.0893\n",
      "Epoch [97/100], Step [54/84], Loss: 0.0959\n",
      "Epoch [97/100], Step [55/84], Loss: 0.0853\n",
      "Epoch [97/100], Step [56/84], Loss: 0.0828\n",
      "Epoch [97/100], Step [57/84], Loss: 0.0992\n",
      "Epoch [97/100], Step [58/84], Loss: 0.1315\n",
      "Epoch [97/100], Step [59/84], Loss: 0.1133\n",
      "Epoch [97/100], Step [60/84], Loss: 0.1200\n",
      "Epoch [97/100], Step [61/84], Loss: 0.0866\n",
      "Epoch [97/100], Step [62/84], Loss: 0.1137\n",
      "Epoch [97/100], Step [63/84], Loss: 0.0994\n",
      "Epoch [97/100], Step [64/84], Loss: 0.1159\n",
      "Epoch [97/100], Step [65/84], Loss: 0.1222\n",
      "Epoch [97/100], Step [66/84], Loss: 0.0871\n",
      "Epoch [97/100], Step [67/84], Loss: 0.1266\n",
      "Epoch [97/100], Step [68/84], Loss: 0.1062\n",
      "Epoch [97/100], Step [69/84], Loss: 0.1013\n",
      "Epoch [97/100], Step [70/84], Loss: 0.0863\n",
      "Epoch [97/100], Step [71/84], Loss: 0.1288\n",
      "Epoch [97/100], Step [72/84], Loss: 0.0963\n",
      "Epoch [97/100], Step [73/84], Loss: 0.1079\n",
      "Epoch [97/100], Step [74/84], Loss: 0.0798\n",
      "Epoch [97/100], Step [75/84], Loss: 0.0927\n",
      "Epoch [97/100], Step [76/84], Loss: 0.0935\n",
      "Epoch [97/100], Step [77/84], Loss: 0.0841\n",
      "Epoch [97/100], Step [78/84], Loss: 0.0909\n",
      "Epoch [97/100], Step [79/84], Loss: 0.1060\n",
      "Epoch [97/100], Step [80/84], Loss: 0.1142\n",
      "Epoch [97/100], Step [81/84], Loss: 0.0985\n",
      "Epoch [97/100], Step [82/84], Loss: 0.0949\n",
      "Epoch [97/100], Step [83/84], Loss: 0.0868\n",
      "Epoch [97/100], Step [84/84], Loss: 0.1031\n",
      "Epoch [98/100], Step [1/84], Loss: 0.0818\n",
      "Epoch [98/100], Step [2/84], Loss: 0.0863\n",
      "Epoch [98/100], Step [3/84], Loss: 0.1043\n",
      "Epoch [98/100], Step [4/84], Loss: 0.0812\n",
      "Epoch [98/100], Step [5/84], Loss: 0.0998\n",
      "Epoch [98/100], Step [6/84], Loss: 0.0975\n",
      "Epoch [98/100], Step [7/84], Loss: 0.0822\n",
      "Epoch [98/100], Step [8/84], Loss: 0.0913\n",
      "Epoch [98/100], Step [9/84], Loss: 0.0917\n",
      "Epoch [98/100], Step [10/84], Loss: 0.0929\n",
      "Epoch [98/100], Step [11/84], Loss: 0.0880\n",
      "Epoch [98/100], Step [12/84], Loss: 0.1290\n",
      "Epoch [98/100], Step [13/84], Loss: 0.0899\n",
      "Epoch [98/100], Step [14/84], Loss: 0.0873\n",
      "Epoch [98/100], Step [15/84], Loss: 0.0874\n",
      "Epoch [98/100], Step [16/84], Loss: 0.1377\n",
      "Epoch [98/100], Step [17/84], Loss: 0.1015\n",
      "Epoch [98/100], Step [18/84], Loss: 0.0876\n",
      "Epoch [98/100], Step [19/84], Loss: 0.1138\n",
      "Epoch [98/100], Step [20/84], Loss: 0.0921\n",
      "Epoch [98/100], Step [21/84], Loss: 0.0817\n",
      "Epoch [98/100], Step [22/84], Loss: 0.0889\n",
      "Epoch [98/100], Step [23/84], Loss: 0.0908\n",
      "Epoch [98/100], Step [24/84], Loss: 0.1081\n",
      "Epoch [98/100], Step [25/84], Loss: 0.1050\n",
      "Epoch [98/100], Step [26/84], Loss: 0.1085\n",
      "Epoch [98/100], Step [27/84], Loss: 0.1124\n",
      "Epoch [98/100], Step [28/84], Loss: 0.0850\n",
      "Epoch [98/100], Step [29/84], Loss: 0.0884\n",
      "Epoch [98/100], Step [30/84], Loss: 0.0898\n",
      "Epoch [98/100], Step [31/84], Loss: 0.1211\n",
      "Epoch [98/100], Step [32/84], Loss: 0.0946\n",
      "Epoch [98/100], Step [33/84], Loss: 0.1124\n",
      "Epoch [98/100], Step [34/84], Loss: 0.0929\n",
      "Epoch [98/100], Step [35/84], Loss: 0.0977\n",
      "Epoch [98/100], Step [36/84], Loss: 0.1311\n",
      "Epoch [98/100], Step [37/84], Loss: 0.1207\n",
      "Epoch [98/100], Step [38/84], Loss: 0.0932\n",
      "Epoch [98/100], Step [39/84], Loss: 0.0854\n",
      "Epoch [98/100], Step [40/84], Loss: 0.1000\n",
      "Epoch [98/100], Step [41/84], Loss: 0.0938\n",
      "Epoch [98/100], Step [42/84], Loss: 0.1166\n",
      "Epoch [98/100], Step [43/84], Loss: 0.0904\n",
      "Epoch [98/100], Step [44/84], Loss: 0.0921\n",
      "Epoch [98/100], Step [45/84], Loss: 0.0875\n",
      "Epoch [98/100], Step [46/84], Loss: 0.1009\n",
      "Epoch [98/100], Step [47/84], Loss: 0.1016\n",
      "Epoch [98/100], Step [48/84], Loss: 0.1040\n",
      "Epoch [98/100], Step [49/84], Loss: 0.0823\n",
      "Epoch [98/100], Step [50/84], Loss: 0.0963\n",
      "Epoch [98/100], Step [51/84], Loss: 0.0931\n",
      "Epoch [98/100], Step [52/84], Loss: 0.0897\n",
      "Epoch [98/100], Step [53/84], Loss: 0.0966\n",
      "Epoch [98/100], Step [54/84], Loss: 0.0869\n",
      "Epoch [98/100], Step [55/84], Loss: 0.0843\n",
      "Epoch [98/100], Step [56/84], Loss: 0.1064\n",
      "Epoch [98/100], Step [57/84], Loss: 0.0888\n",
      "Epoch [98/100], Step [58/84], Loss: 0.1054\n",
      "Epoch [98/100], Step [59/84], Loss: 0.0924\n",
      "Epoch [98/100], Step [60/84], Loss: 0.0784\n",
      "Epoch [98/100], Step [61/84], Loss: 0.0894\n",
      "Epoch [98/100], Step [62/84], Loss: 0.0846\n",
      "Epoch [98/100], Step [63/84], Loss: 0.0943\n",
      "Epoch [98/100], Step [64/84], Loss: 0.1057\n",
      "Epoch [98/100], Step [65/84], Loss: 0.0922\n",
      "Epoch [98/100], Step [66/84], Loss: 0.0916\n",
      "Epoch [98/100], Step [67/84], Loss: 0.0864\n",
      "Epoch [98/100], Step [68/84], Loss: 0.1293\n",
      "Epoch [98/100], Step [69/84], Loss: 0.0896\n",
      "Epoch [98/100], Step [70/84], Loss: 0.0985\n",
      "Epoch [98/100], Step [71/84], Loss: 0.0752\n",
      "Epoch [98/100], Step [72/84], Loss: 0.1030\n",
      "Epoch [98/100], Step [73/84], Loss: 0.0905\n",
      "Epoch [98/100], Step [74/84], Loss: 0.1002\n",
      "Epoch [98/100], Step [75/84], Loss: 0.1036\n",
      "Epoch [98/100], Step [76/84], Loss: 0.0815\n",
      "Epoch [98/100], Step [77/84], Loss: 0.0769\n",
      "Epoch [98/100], Step [78/84], Loss: 0.0929\n",
      "Epoch [98/100], Step [79/84], Loss: 0.1159\n",
      "Epoch [98/100], Step [80/84], Loss: 0.0789\n",
      "Epoch [98/100], Step [81/84], Loss: 0.0918\n",
      "Epoch [98/100], Step [82/84], Loss: 0.0772\n",
      "Epoch [98/100], Step [83/84], Loss: 0.1202\n",
      "Epoch [98/100], Step [84/84], Loss: 0.0875\n",
      "Epoch [99/100], Step [1/84], Loss: 0.0795\n",
      "Epoch [99/100], Step [2/84], Loss: 0.0783\n",
      "Epoch [99/100], Step [3/84], Loss: 0.1194\n",
      "Epoch [99/100], Step [4/84], Loss: 0.1011\n",
      "Epoch [99/100], Step [5/84], Loss: 0.1235\n",
      "Epoch [99/100], Step [6/84], Loss: 0.0913\n",
      "Epoch [99/100], Step [7/84], Loss: 0.1066\n",
      "Epoch [99/100], Step [8/84], Loss: 0.0794\n",
      "Epoch [99/100], Step [9/84], Loss: 0.0931\n",
      "Epoch [99/100], Step [10/84], Loss: 0.0826\n",
      "Epoch [99/100], Step [11/84], Loss: 0.0954\n",
      "Epoch [99/100], Step [12/84], Loss: 0.0769\n",
      "Epoch [99/100], Step [13/84], Loss: 0.0833\n",
      "Epoch [99/100], Step [14/84], Loss: 0.0798\n",
      "Epoch [99/100], Step [15/84], Loss: 0.1217\n",
      "Epoch [99/100], Step [16/84], Loss: 0.0821\n",
      "Epoch [99/100], Step [17/84], Loss: 0.0968\n",
      "Epoch [99/100], Step [18/84], Loss: 0.1316\n",
      "Epoch [99/100], Step [19/84], Loss: 0.0914\n",
      "Epoch [99/100], Step [20/84], Loss: 0.1008\n",
      "Epoch [99/100], Step [21/84], Loss: 0.0913\n",
      "Epoch [99/100], Step [22/84], Loss: 0.0884\n",
      "Epoch [99/100], Step [23/84], Loss: 0.1260\n",
      "Epoch [99/100], Step [24/84], Loss: 0.0971\n",
      "Epoch [99/100], Step [25/84], Loss: 0.0743\n",
      "Epoch [99/100], Step [26/84], Loss: 0.0785\n",
      "Epoch [99/100], Step [27/84], Loss: 0.0884\n",
      "Epoch [99/100], Step [28/84], Loss: 0.0916\n",
      "Epoch [99/100], Step [29/84], Loss: 0.0951\n",
      "Epoch [99/100], Step [30/84], Loss: 0.0998\n",
      "Epoch [99/100], Step [31/84], Loss: 0.0938\n",
      "Epoch [99/100], Step [32/84], Loss: 0.1108\n",
      "Epoch [99/100], Step [33/84], Loss: 0.0899\n",
      "Epoch [99/100], Step [34/84], Loss: 0.0797\n",
      "Epoch [99/100], Step [35/84], Loss: 0.0929\n",
      "Epoch [99/100], Step [36/84], Loss: 0.0775\n",
      "Epoch [99/100], Step [37/84], Loss: 0.0947\n",
      "Epoch [99/100], Step [38/84], Loss: 0.0819\n",
      "Epoch [99/100], Step [39/84], Loss: 0.0903\n",
      "Epoch [99/100], Step [40/84], Loss: 0.0887\n",
      "Epoch [99/100], Step [41/84], Loss: 0.1532\n",
      "Epoch [99/100], Step [42/84], Loss: 0.1123\n",
      "Epoch [99/100], Step [43/84], Loss: 0.1061\n",
      "Epoch [99/100], Step [44/84], Loss: 0.1173\n",
      "Epoch [99/100], Step [45/84], Loss: 0.1395\n",
      "Epoch [99/100], Step [46/84], Loss: 0.0976\n",
      "Epoch [99/100], Step [47/84], Loss: 0.0902\n",
      "Epoch [99/100], Step [48/84], Loss: 0.0893\n",
      "Epoch [99/100], Step [49/84], Loss: 0.0934\n",
      "Epoch [99/100], Step [50/84], Loss: 0.1162\n",
      "Epoch [99/100], Step [51/84], Loss: 0.0939\n",
      "Epoch [99/100], Step [52/84], Loss: 0.0968\n",
      "Epoch [99/100], Step [53/84], Loss: 0.1013\n",
      "Epoch [99/100], Step [54/84], Loss: 0.1071\n",
      "Epoch [99/100], Step [55/84], Loss: 0.1149\n",
      "Epoch [99/100], Step [56/84], Loss: 0.1099\n",
      "Epoch [99/100], Step [57/84], Loss: 0.1061\n",
      "Epoch [99/100], Step [58/84], Loss: 0.0959\n",
      "Epoch [99/100], Step [59/84], Loss: 0.0942\n",
      "Epoch [99/100], Step [60/84], Loss: 0.0823\n",
      "Epoch [99/100], Step [61/84], Loss: 0.0947\n",
      "Epoch [99/100], Step [62/84], Loss: 0.0947\n",
      "Epoch [99/100], Step [63/84], Loss: 0.1034\n",
      "Epoch [99/100], Step [64/84], Loss: 0.1235\n",
      "Epoch [99/100], Step [65/84], Loss: 0.1093\n",
      "Epoch [99/100], Step [66/84], Loss: 0.1253\n",
      "Epoch [99/100], Step [67/84], Loss: 0.0857\n",
      "Epoch [99/100], Step [68/84], Loss: 0.1056\n",
      "Epoch [99/100], Step [69/84], Loss: 0.0962\n",
      "Epoch [99/100], Step [70/84], Loss: 0.1123\n",
      "Epoch [99/100], Step [71/84], Loss: 0.1203\n",
      "Epoch [99/100], Step [72/84], Loss: 0.1076\n",
      "Epoch [99/100], Step [73/84], Loss: 0.0908\n",
      "Epoch [99/100], Step [74/84], Loss: 0.0837\n",
      "Epoch [99/100], Step [75/84], Loss: 0.0972\n",
      "Epoch [99/100], Step [76/84], Loss: 0.0955\n",
      "Epoch [99/100], Step [77/84], Loss: 0.1006\n",
      "Epoch [99/100], Step [78/84], Loss: 0.0869\n",
      "Epoch [99/100], Step [79/84], Loss: 0.0916\n",
      "Epoch [99/100], Step [80/84], Loss: 0.0813\n",
      "Epoch [99/100], Step [81/84], Loss: 0.1014\n",
      "Epoch [99/100], Step [82/84], Loss: 0.1044\n",
      "Epoch [99/100], Step [83/84], Loss: 0.1037\n",
      "Epoch [99/100], Step [84/84], Loss: 0.0984\n",
      "Epoch [100/100], Step [1/84], Loss: 0.0968\n",
      "Epoch [100/100], Step [2/84], Loss: 0.0905\n",
      "Epoch [100/100], Step [3/84], Loss: 0.1041\n",
      "Epoch [100/100], Step [4/84], Loss: 0.0740\n",
      "Epoch [100/100], Step [5/84], Loss: 0.1133\n",
      "Epoch [100/100], Step [6/84], Loss: 0.1085\n",
      "Epoch [100/100], Step [7/84], Loss: 0.0877\n",
      "Epoch [100/100], Step [8/84], Loss: 0.0819\n",
      "Epoch [100/100], Step [9/84], Loss: 0.1159\n",
      "Epoch [100/100], Step [10/84], Loss: 0.0812\n",
      "Epoch [100/100], Step [11/84], Loss: 0.1091\n",
      "Epoch [100/100], Step [12/84], Loss: 0.0799\n",
      "Epoch [100/100], Step [13/84], Loss: 0.0989\n",
      "Epoch [100/100], Step [14/84], Loss: 0.0938\n",
      "Epoch [100/100], Step [15/84], Loss: 0.0802\n",
      "Epoch [100/100], Step [16/84], Loss: 0.0868\n",
      "Epoch [100/100], Step [17/84], Loss: 0.0846\n",
      "Epoch [100/100], Step [18/84], Loss: 0.0872\n",
      "Epoch [100/100], Step [19/84], Loss: 0.0825\n",
      "Epoch [100/100], Step [20/84], Loss: 0.0934\n",
      "Epoch [100/100], Step [21/84], Loss: 0.0816\n",
      "Epoch [100/100], Step [22/84], Loss: 0.0985\n",
      "Epoch [100/100], Step [23/84], Loss: 0.1034\n",
      "Epoch [100/100], Step [24/84], Loss: 0.0949\n",
      "Epoch [100/100], Step [25/84], Loss: 0.0809\n",
      "Epoch [100/100], Step [26/84], Loss: 0.0928\n",
      "Epoch [100/100], Step [27/84], Loss: 0.0757\n",
      "Epoch [100/100], Step [28/84], Loss: 0.0831\n",
      "Epoch [100/100], Step [29/84], Loss: 0.0915\n",
      "Epoch [100/100], Step [30/84], Loss: 0.0991\n",
      "Epoch [100/100], Step [31/84], Loss: 0.0752\n",
      "Epoch [100/100], Step [32/84], Loss: 0.1003\n",
      "Epoch [100/100], Step [33/84], Loss: 0.0848\n",
      "Epoch [100/100], Step [34/84], Loss: 0.0857\n",
      "Epoch [100/100], Step [35/84], Loss: 0.1016\n",
      "Epoch [100/100], Step [36/84], Loss: 0.0940\n",
      "Epoch [100/100], Step [37/84], Loss: 0.0764\n",
      "Epoch [100/100], Step [38/84], Loss: 0.0836\n",
      "Epoch [100/100], Step [39/84], Loss: 0.0887\n",
      "Epoch [100/100], Step [40/84], Loss: 0.0955\n",
      "Epoch [100/100], Step [41/84], Loss: 0.0882\n",
      "Epoch [100/100], Step [42/84], Loss: 0.0859\n",
      "Epoch [100/100], Step [43/84], Loss: 0.0831\n",
      "Epoch [100/100], Step [44/84], Loss: 0.0834\n",
      "Epoch [100/100], Step [45/84], Loss: 0.0857\n",
      "Epoch [100/100], Step [46/84], Loss: 0.0814\n",
      "Epoch [100/100], Step [47/84], Loss: 0.0844\n",
      "Epoch [100/100], Step [48/84], Loss: 0.0986\n",
      "Epoch [100/100], Step [49/84], Loss: 0.0790\n",
      "Epoch [100/100], Step [50/84], Loss: 0.0822\n",
      "Epoch [100/100], Step [51/84], Loss: 0.0976\n",
      "Epoch [100/100], Step [52/84], Loss: 0.0905\n",
      "Epoch [100/100], Step [53/84], Loss: 0.1194\n",
      "Epoch [100/100], Step [54/84], Loss: 0.0997\n",
      "Epoch [100/100], Step [55/84], Loss: 0.1060\n",
      "Epoch [100/100], Step [56/84], Loss: 0.1031\n",
      "Epoch [100/100], Step [57/84], Loss: 0.0874\n",
      "Epoch [100/100], Step [58/84], Loss: 0.1035\n",
      "Epoch [100/100], Step [59/84], Loss: 0.1022\n",
      "Epoch [100/100], Step [60/84], Loss: 0.0988\n",
      "Epoch [100/100], Step [61/84], Loss: 0.0993\n",
      "Epoch [100/100], Step [62/84], Loss: 0.1071\n",
      "Epoch [100/100], Step [63/84], Loss: 0.0843\n",
      "Epoch [100/100], Step [64/84], Loss: 0.1149\n",
      "Epoch [100/100], Step [65/84], Loss: 0.1073\n",
      "Epoch [100/100], Step [66/84], Loss: 0.1231\n",
      "Epoch [100/100], Step [67/84], Loss: 0.1019\n",
      "Epoch [100/100], Step [68/84], Loss: 0.1101\n",
      "Epoch [100/100], Step [69/84], Loss: 0.0735\n",
      "Epoch [100/100], Step [70/84], Loss: 0.0805\n",
      "Epoch [100/100], Step [71/84], Loss: 0.0940\n",
      "Epoch [100/100], Step [72/84], Loss: 0.0823\n",
      "Epoch [100/100], Step [73/84], Loss: 0.0958\n",
      "Epoch [100/100], Step [74/84], Loss: 0.0849\n",
      "Epoch [100/100], Step [75/84], Loss: 0.1035\n",
      "Epoch [100/100], Step [76/84], Loss: 0.1007\n",
      "Epoch [100/100], Step [77/84], Loss: 0.0855\n",
      "Epoch [100/100], Step [78/84], Loss: 0.0951\n",
      "Epoch [100/100], Step [79/84], Loss: 0.1143\n",
      "Epoch [100/100], Step [80/84], Loss: 0.1363\n",
      "Epoch [100/100], Step [81/84], Loss: 0.0771\n",
      "Epoch [100/100], Step [82/84], Loss: 0.0992\n",
      "Epoch [100/100], Step [83/84], Loss: 0.1430\n",
      "Epoch [100/100], Step [84/84], Loss: 0.0861\n",
      "TRAIN 1 program run complete\n",
      "Train datas: {'Average Training Accuracy': [0.8594927409338573, 0.8813577076745412, 0.8872069706992495, 0.8902094099256724, 0.8938146924215644, 0.894657377212767, 0.8970872485448447, 0.9019066644093343, 0.9043870047917443, 0.9063233572339255, 0.9060738881429031, 0.9118224249945749, 0.9139006175692117, 0.9131182110498821, 0.9145698244609531, 0.9176477704729352, 0.9202674986824158, 0.920133772350493, 0.9236699967157277, 0.9245545523507255, 0.9250695667569596, 0.9275877513582744, 0.9279764266241163, 0.9289591047498914, 0.93020514836387, 0.9321949973938954, 0.9310418991815478, 0.9311228555346293, 0.9351446060907271, 0.935441698346819, 0.9377489241342694, 0.9397293272472563, 0.9397126455155632, 0.9413057660299632, 0.9423647381010505, 0.9438950674874446, 0.9438656095474488, 0.9462764376685728, 0.9474802168588787, 0.9478269910055497, 0.9498030041891432, 0.9499812958732485, 0.9512618836902439, 0.9511089627704924, 0.9521929725768078, 0.9524422902909537, 0.9529269384959388, 0.9541120075044176, 0.9552732800680493, 0.9547482142372733, 0.9540684715149894, 0.9557594723171656, 0.9554667094397166, 0.9563288915724983, 0.9557638016958088, 0.9555015563964842, 0.9566443912566653, 0.9559720357259113, 0.9563206263950891, 0.9557890210832868, 0.9570284949408638, 0.9561397915794733, 0.9564570472353984, 0.9551412188817587, 0.9550517854236421, 0.9527983892531617, 0.9432824452718096, 0.9464275117904423, 0.9485679808117092, 0.9491285293821302, 0.9493022192092168, 0.9505404215010392, 0.9524098956395707, 0.9522636050269719, 0.9531238798111209, 0.9507079351515996, 0.9526983594137522, 0.9539842151460198, 0.9536487337142701, 0.9537886967734691, 0.9543351672944572, 0.9568802364288816, 0.9553313785129124, 0.9554853893461683, 0.9574553171793619, 0.9565940130324593, 0.9593672071184433, 0.9583524370950366, 0.9558754875546404, 0.9590809837220207, 0.9605489458356586, 0.9589938209170383, 0.9599138895670571, 0.9630014328729539, 0.9595850989932108, 0.9617490011548243, 0.9628605010017511, 0.9640516553606309, 0.9624896579318574, 0.9640214102608824], 'Average Training Loss': [0.5996736408699126, 0.5358631720855123, 0.510739657495703, 0.491531317787511, 0.4721097747484843, 0.4576728390086265, 0.4406915803750356, 0.4233525512473924, 0.40971024547304424, 0.3956515033330236, 0.38490093286548344, 0.3672019658344133, 0.355913292439211, 0.3468567711256799, 0.33757697471550535, 0.325075901335194, 0.3156466480521929, 0.3078905180806205, 0.2958609862696557, 0.2898504595671381, 0.28249730453604743, 0.2745503882567088, 0.2696302646682376, 0.2626899610317889, 0.2570216256592955, 0.24986588582396507, 0.24758304353980792, 0.24425716432077543, 0.23614666522258804, 0.23367388351332574, 0.2269851134291717, 0.22170302537935122, 0.21886243813094639, 0.21480423105614527, 0.2116065768613702, 0.20705832416812578, 0.2057950407976196, 0.20070900803520567, 0.1977404583068121, 0.19631528747933252, 0.19116401494968505, 0.19079204098809333, 0.18822385956134116, 0.1862940690701916, 0.18486497551202774, 0.1834912427834102, 0.18237532249518804, 0.1798222286715394, 0.17723642467033296, 0.1776477343269757, 0.1776868197179976, 0.17440949557792573, 0.17398905044510252, 0.1720380336046219, 0.17264942115261442, 0.17272829299881345, 0.1698981197107406, 0.17013337498619444, 0.16821069855775153, 0.167939526339372, 0.16603195135082519, 0.165717808618432, 0.1638900987094357, 0.164535325552736, 0.1631394839357762, 0.16578481098016104, 0.17917159873814809, 0.17286419815250806, 0.1678857955904234, 0.16521219235090984, 0.16302805926118577, 0.16037977132059278, 0.15525325529632114, 0.1529141669826848, 0.14988054086764654, 0.1518115997314453, 0.14700085173050562, 0.14306504944605486, 0.14249424095309915, 0.13855448515997046, 0.13704911938735417, 0.1308000631453026, 0.1310995603423743, 0.12960568318764368, 0.12473091500855628, 0.12457216389122464, 0.118321618065238, 0.11822831524269921, 0.12103767382601897, 0.11381853247682254, 0.11034169243205161, 0.11157092061780748, 0.10924268327653408, 0.10289577580988407, 0.10705569359873023, 0.10262745636559668, 0.09961524197743052, 0.0961409415162745, 0.09852934806119829, 0.09401748418098405], 'Average Training Precision': [0.6787509766333962, 0.8209769450727998, 0.8486076556374252, 0.8557612977424381, 0.8670093239245988, 0.8666751430812273, 0.8760533458537755, 0.8779369569353906, 0.8797949304548593, 0.8827359665509843, 0.8816763670708797, 0.8927268532535997, 0.8891440954287159, 0.8959633797134463, 0.8905620742925825, 0.8981698772909972, 0.8950316636014722, 0.8985537520868719, 0.9072085148323216, 0.9008350533466063, 0.9077045731119681, 0.9089555930342248, 0.9037792344008148, 0.910723494057304, 0.912493754671817, 0.9177613709736737, 0.9117077841520281, 0.912675376546967, 0.9168500849473269, 0.9166521743989197, 0.9183049377415234, 0.9215626192575609, 0.9223234209847504, 0.9259612769251543, 0.926306898285676, 0.9263488843216162, 0.9272516349953196, 0.9294015985636003, 0.9302201099179626, 0.9299704431762004, 0.9370958324323733, 0.9320614759413016, 0.9331976068302307, 0.9360347541775567, 0.9334916850677034, 0.9375380491910309, 0.9365470631506871, 0.9388013630883189, 0.9409335836722742, 0.9365388016378317, 0.9390118357151409, 0.9405826562377988, 0.9403506562795029, 0.9426813616563822, 0.9402962428788433, 0.9385584192708036, 0.9410946050261958, 0.9409433528039196, 0.9424456667488964, 0.9424287082908155, 0.9372453350419974, 0.9434955569652249, 0.940257615342532, 0.9403060082002066, 0.940161304952263, 0.9350006818379998, 0.9288140400442828, 0.9312402404976606, 0.931991510408353, 0.9322181627886911, 0.9325396643432706, 0.934009372490018, 0.9336884704236385, 0.9371211920750288, 0.9378098619322033, 0.9331839611823207, 0.9357829108731721, 0.9374280615362955, 0.9352882621536032, 0.9367745325953665, 0.9352253235694483, 0.9381808937064454, 0.9398408084886666, 0.9383633601368427, 0.940316587254806, 0.9383466764164273, 0.9399558061979961, 0.93968676830004, 0.9410952278723844, 0.9424865484248625, 0.9424982368611144, 0.9422460779376622, 0.9436169221800135, 0.9450236189162846, 0.944524101041692, 0.94373627192774, 0.9452631868518847, 0.9474102893113635, 0.9456589050416678, 0.9476688907918588], 'Average Training Sensitivity': [0.6787509766333962, 0.8209769450727998, 0.8486076556374252, 0.8557612977424381, 0.8670093239245988, 0.8666751430812273, 0.8760533458537755, 0.8779369569353906, 0.8797949304548593, 0.8827359665509843, 0.8816763670708797, 0.8927268532535997, 0.8891440954287159, 0.8959633797134463, 0.8905620742925825, 0.8981698772909972, 0.8950316636014722, 0.8985537520868719, 0.9072085148323216, 0.9008350533466063, 0.9077045731119681, 0.9089555930342248, 0.9037792344008148, 0.910723494057304, 0.912493754671817, 0.9177613709736737, 0.9117077841520281, 0.912675376546967, 0.9168500849473269, 0.9166521743989197, 0.9183049377415234, 0.9215626192575609, 0.9223234209847504, 0.9259612769251543, 0.926306898285676, 0.9263488843216162, 0.9272516349953196, 0.9294015985636003, 0.9302201099179626, 0.9299704431762004, 0.9370958324323733, 0.9320614759413016, 0.9331976068302307, 0.9360347541775567, 0.9334916850677034, 0.9375380491910309, 0.9365470631506871, 0.9388013630883189, 0.9409335836722742, 0.9365388016378317, 0.9390118357151409, 0.9405826562377988, 0.9403506562795029, 0.9426813616563822, 0.9402962428788433, 0.9385584192708036, 0.9410946050261958, 0.9409433528039196, 0.9424456667488964, 0.9424287082908155, 0.9372453350419974, 0.9434955569652249, 0.940257615342532, 0.9403060082002066, 0.940161304952263, 0.9350006818379998, 0.9288140400442828, 0.9312402404976606, 0.931991510408353, 0.9322181627886911, 0.9325396643432706, 0.934009372490018, 0.9336884704236385, 0.9371211920750288, 0.9378098619322033, 0.9331839611823207, 0.9357829108731721, 0.9374280615362955, 0.9352882621536032, 0.9367745325953665, 0.9352253235694483, 0.9381808937064454, 0.9398408084886666, 0.9383633601368427, 0.940316587254806, 0.9383466764164273, 0.9399558061979961, 0.93968676830004, 0.9410952278723844, 0.9424865484248625, 0.9424982368611144, 0.9422460779376622, 0.9436169221800135, 0.9450236189162846, 0.944524101041692, 0.94373627192774, 0.9452631868518847, 0.9474102893113635, 0.9456589050416678, 0.9476688907918588], 'Average Training Specificity': [0.925184847236743, 0.9743534417739319, 0.9782318607425327, 0.9791919432044567, 0.9805478230506924, 0.9800980390701044, 0.9816134227171082, 0.9812039575853332, 0.9815690951760552, 0.9816849431988247, 0.9814450845590235, 0.9825727770802258, 0.9818401325755278, 0.9832534576081619, 0.9821916988699323, 0.9828527425660246, 0.9821964907003937, 0.9829884347972507, 0.9841669047535336, 0.98266604044049, 0.9843961998805353, 0.9840585801232407, 0.9832517245967443, 0.9840715683987249, 0.9846402503937798, 0.9851010118327269, 0.9844739362015874, 0.984654538403124, 0.9849177099768286, 0.9852688798922605, 0.9849790717763809, 0.9856593746417349, 0.9855508406088374, 0.986114166822061, 0.986231547373578, 0.9863303247729831, 0.9864993830929029, 0.9863660798421483, 0.9866935577217846, 0.9866417926326624, 0.9877638857635167, 0.9869284438223948, 0.987401817223847, 0.9874324582355747, 0.9871865009819445, 0.9877598145256188, 0.9876589371739909, 0.9877976136745268, 0.9881943028305451, 0.9876173285468064, 0.987878593389031, 0.9882854027736803, 0.987951878904617, 0.9884851728881849, 0.9881465124368802, 0.9879228264610548, 0.9880479982435662, 0.9886135357194473, 0.9884031391940039, 0.9885383139730569, 0.9876033956359458, 0.988728007243209, 0.9880028191735567, 0.9879937040156419, 0.9879931465795784, 0.9871698908376609, 0.9866290352951452, 0.9864605323146807, 0.9866852444105506, 0.9869431173426987, 0.9866987173368759, 0.9872573610899901, 0.9869405085547823, 0.9875445299382933, 0.9875690217481602, 0.9866573540707337, 0.9870317155057412, 0.9876713992628413, 0.986973818925585, 0.9872633924786065, 0.9869187387416618, 0.9878202948396547, 0.9878919626076785, 0.9876422310407598, 0.9877666685156196, 0.9874919885591137, 0.9877167254741169, 0.9880214295303524, 0.9879667161664438, 0.9881923015954622, 0.9878975480756768, 0.9879292492761509, 0.988402283451347, 0.9885491316251915, 0.9885428275458134, 0.9882305505965479, 0.9886127377423742, 0.9890122527806384, 0.9885653457596343, 0.9889492005084299], 'Average Training F1': [0.6231607775211615, 0.6218243893874745, 0.6400781354644297, 0.6506406203314611, 0.6652163636508627, 0.6673721181855745, 0.6779847591028045, 0.6963995790070356, 0.7064579910039716, 0.7112721411886532, 0.7110734370597458, 0.7328473077361991, 0.7411209869754146, 0.7363160716834815, 0.7439535750627032, 0.753929614917558, 0.7642206011581278, 0.76310305258416, 0.7740732646204006, 0.7788455540200756, 0.7801262970129178, 0.7879713005316724, 0.7888769570756842, 0.7927994625236702, 0.7970052226169836, 0.8066193736970746, 0.8016889817001992, 0.8004700296720899, 0.8135331788648253, 0.813097519201872, 0.8216010387809864, 0.8279461964363283, 0.8278629518836782, 0.8329225157946721, 0.8378596904299743, 0.8399480749554672, 0.8405675803387711, 0.8488709859242449, 0.85099372172378, 0.8524525245022381, 0.8601881801422918, 0.8585617114834188, 0.8622142720304863, 0.8630671519696921, 0.8656869663081904, 0.8674621513716667, 0.8677602844599497, 0.8724555745945184, 0.8757802558659288, 0.8738030457023749, 0.8724261722689965, 0.876993077205427, 0.8766905113795312, 0.8796023009550593, 0.8777555665417313, 0.876129743673721, 0.8797179137649441, 0.8766707524386861, 0.8788821962468684, 0.8777448095188718, 0.8798263825049982, 0.8778737570181202, 0.8793453092127205, 0.8753457663887851, 0.8742590103133406, 0.8678276577808846, 0.838258869816347, 0.8498567059128184, 0.8547916806052192, 0.856607792571783, 0.8576294364310009, 0.8610050286549903, 0.866534285923802, 0.8656177319081532, 0.8699346280325491, 0.8634822815015296, 0.8682874158857601, 0.8720202020720018, 0.870774562157375, 0.871365156281219, 0.8724130966764331, 0.8798573199298859, 0.8752379145151155, 0.8758991039112937, 0.8831296484116141, 0.8797485914387722, 0.8880871883560537, 0.8839288490532256, 0.8766864492048787, 0.8875538027398946, 0.8916776225103598, 0.8868925584688521, 0.8900980316275025, 0.8979976603688962, 0.8888239558847444, 0.8958999835287142, 0.897913455902762, 0.9009800336067538, 0.8966458258609735, 0.9016596819189767], 'Average Training JS': [0.45741475469069587, 0.4534158033379474, 0.4727562692249543, 0.4842548257626692, 0.5001819309866307, 0.5028141389437527, 0.5144821133836606, 0.5359343871864406, 0.5486788299285873, 0.5543847736727044, 0.5538248294524858, 0.5798753772333656, 0.5902738617819642, 0.5843304089833777, 0.5939967252566806, 0.6067640576778537, 0.6199290040035909, 0.6188697778533455, 0.633034845887631, 0.6392285985900834, 0.6408764086660304, 0.6517207462699172, 0.6528793157160504, 0.6585389945704672, 0.6642976889246219, 0.6773405191235344, 0.6702576523059369, 0.6688418351787595, 0.6869112456349099, 0.6865684875502931, 0.6983355424909709, 0.707315909810621, 0.7074722439406288, 0.7149233710026924, 0.7220715405764387, 0.72533499519186, 0.7261574666809483, 0.7382863587880641, 0.7418629990257298, 0.7437183867032534, 0.7556388235401195, 0.7533471850295711, 0.7587912800449623, 0.7600263353669382, 0.763938313790898, 0.7667041961691354, 0.767245499951082, 0.7743361675938697, 0.779799235841891, 0.7765836107323358, 0.7744687507563126, 0.7815650184581929, 0.7810496193900013, 0.7856685672387324, 0.7827430965985852, 0.7802890629966028, 0.7858750498468917, 0.7811263993113213, 0.7847229338647344, 0.7828877879381241, 0.7866549056312658, 0.7830997479844006, 0.7852541526214122, 0.7790244283640171, 0.7777081714633531, 0.7674319957837464, 0.722958432366005, 0.740113199024211, 0.7471807764960131, 0.7499946467237715, 0.7514504821507203, 0.7568642347312499, 0.7653328243741816, 0.7641726687052071, 0.7706278825312557, 0.7605103288493587, 0.7678914004923681, 0.7736264961353265, 0.7717761644703761, 0.7726845951160684, 0.7745004754512598, 0.7860783172473711, 0.7786897642730375, 0.7800196055421748, 0.7912980567451025, 0.7861363517358181, 0.7991699421441658, 0.7927780270383519, 0.7814738801773833, 0.798329027867101, 0.8051097288880849, 0.7975581953649308, 0.8025191878481824, 0.8153881897872715, 0.8003816753047553, 0.8119264463820454, 0.8152256879055729, 0.8202992173126966, 0.8132268727986363, 0.8215942958023177], 'Average Training Dice Coefficient': [0.6231612662923777, 0.6218248558859021, 0.64007860277568, 0.6506410891995973, 0.6652168336451643, 0.6673725889214313, 0.6779852315203492, 0.696400055096673, 0.7064584680546794, 0.7112726201600434, 0.7110739149075321, 0.7328477902724753, 0.7411214707181019, 0.7363165539684927, 0.7439540588488559, 0.7539300997281196, 0.7642210886998813, 0.7631035387042128, 0.774073752234643, 0.7788460430202041, 0.7801267853479517, 0.787971790269925, 0.7888774471838758, 0.7927999524940723, 0.7970057126571207, 0.8066198647138331, 0.8016894732277484, 0.8004705206129249, 0.8135336715367787, 0.8130980115273765, 0.8216015321188818, 0.827946690366391, 0.827863445619545, 0.8329230097620771, 0.837860184833007, 0.8399485697269585, 0.840568075197266, 0.8488714813956368, 0.8509942173543837, 0.8524530203469156, 0.8601886760981452, 0.8585622078687533, 0.8622147684801993, 0.8630676483437092, 0.8656874631548623, 0.8674626479844202, 0.8677607812111037, 0.8724560716573402, 0.875780752979677, 0.8738035428033425, 0.8724266692185904, 0.8769935744663988, 0.876691008588071, 0.8796027982361375, 0.8777560638800446, 0.8761302410662954, 0.8797184112365143, 0.876671249641916, 0.8788826934063692, 0.8777453066203049, 0.8798268800531168, 0.8778742541165988, 0.879345806666175, 0.8753462634527651, 0.8742595070755099, 0.8678281546502087, 0.8382593641123032, 0.8498572013689251, 0.8547921765911562, 0.8566082885537479, 0.8576299326066184, 0.8610055248553714, 0.8665347827237694, 0.8656182282074625, 0.8699351247764922, 0.863482777822799, 0.8682879126507184, 0.8720206991519175, 0.8707750589797951, 0.8713656531253943, 0.8724135935887347, 0.879857817471027, 0.8752384116439833, 0.8758996007493914, 0.8831301460196127, 0.8797490888662198, 0.888087686393392, 0.8839293466392264, 0.8766869460482635, 0.8875543006731048, 0.8916781205871752, 0.8868930561424081, 0.8900985294953113, 0.8979981587625503, 0.8888244537671323, 0.8959004816983209, 0.8979139542985677, 0.9009805320922178, 0.896646323715345, 0.9016601802110416], 'Epochs': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]}\n"
     ]
    }
   ],
   "source": [
    "def get_val_metrics(mod, loader):\n",
    "    mod.eval()\n",
    "    n_total_steps = len(train_loader)\n",
    "    los_vall = 0\n",
    "    accuracy_vall = 0\n",
    "    sensitivity_vall = 0\n",
    "    specificity_vall = 0\n",
    "    precision_vall = 0\n",
    "    f1_vall = 0\n",
    "    js_vall = 0\n",
    "    dc_vall = 0\n",
    "    print('Entering the loop')\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data.type(opt.dtype)), Variable(target.type(opt.dtype))\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        los_vall += loss.item()\n",
    "        accuracy_vall += get_accuracy(output, target)\n",
    "        sensitivity_vall += get_sensitivity(output, target)\n",
    "        specificity_vall += get_specificity(output, target)\n",
    "        precision_vall += get_precision(output, target)\n",
    "        f1_vall += get_F1(output, target)\n",
    "        js_vall += get_JS(output, target)\n",
    "        dc_vall += get_DC(output, target)\n",
    "    los_vall = los_vall / n_total_steps\n",
    "    accuracy_vall = accuracy_vall / n_total_steps\n",
    "    sensitivity_vall = sensitivity_vall / n_total_steps\n",
    "    specificity_vall = specificity_vall / n_total_steps\n",
    "    precision_vall = precision_vall / n_total_steps\n",
    "    f1_vall = f1_vall / n_total_steps\n",
    "    js_vall = js_vall / n_total_steps\n",
    "    dc_vall = dc_vall / n_total_steps\n",
    "    return los_vall, accuracy_vall, sensitivity_vall, specificity_vall, precision_vall, f1_vall, js_vall, dc_vall\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "opt = Option()\n",
    "# model = R2U_Net(input_channels=3, nclasses=1)\n",
    "model = TransUNet(img_dim=128,\n",
    "                  in_channels=3,\n",
    "                  out_channels=128,\n",
    "                  head_num=4,\n",
    "                  mlp_dim=512,\n",
    "                  block_num=8,\n",
    "                  patch_dim=16,\n",
    "                  class_num=1)\n",
    "model.to(device)\n",
    "# train_loader = get_val_loader()\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 50, 0.00001)\n",
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "train_loader = get_train_loader()\n",
    "val_loader = get_val_loader()\n",
    "\n",
    "los = 0\n",
    "num_batches = 0\n",
    "accuracy_train = 0\n",
    "sensitivity_train = 0\n",
    "specificity_train = 0\n",
    "precision_train = 0\n",
    "f1_train = 0\n",
    "js_train = 0\n",
    "dc_train = 0\n",
    "\n",
    "los_val = 0\n",
    "accuracy_train_val = 0\n",
    "sensitivity_train_val = 0\n",
    "specificity_train_val = 0\n",
    "precision_train_val = 0\n",
    "f1_train_val = 0\n",
    "js_train_val = 0\n",
    "dc_train_val = 0\n",
    "\n",
    "avg_loss = []\n",
    "avg_accuracy_train = []\n",
    "avg_sensitivity_train = []\n",
    "avg_specificity_train = []\n",
    "avg_precision_train = []\n",
    "avg_f1_train = []\n",
    "avg_js_train = []\n",
    "avg_dc_train = []\n",
    "epoch_store = []\n",
    "\n",
    "avg_loss_val = []\n",
    "avg_accuracy_val = []\n",
    "avg_sensitivity_val = []\n",
    "avg_specificity_val = []\n",
    "avg_precision_val = []\n",
    "avg_f1_val = []\n",
    "avg_js_val = []\n",
    "avg_dc_val = []\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "print('Entering the loop')\n",
    "for epoch in range(0, opt.epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # data = sample_batched['image']\n",
    "        # target = sample_batched['mask']\n",
    "        data, target = Variable(data.type(opt.dtype)), Variable(target.type(opt.dtype))\n",
    "        # print(f'Data max is: {data.max()}')\n",
    "        # print(f'Target max is: {target.max()}')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         output = torch.sigmoid(output)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{opt.epochs}], Step [{batch_idx + 1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "        los += loss.item()\n",
    "        accuracy_train += get_accuracy(output, target)\n",
    "        sensitivity_train += get_sensitivity(output, target)\n",
    "        specificity_train += get_specificity(output, target)\n",
    "        precision_train += get_precision(output, target)\n",
    "        f1_train += get_F1(output, target)\n",
    "        js_train += get_JS(output, target)\n",
    "        dc_train += get_DC(output, target)\n",
    "#         break\n",
    "    scheduler.step()\n",
    "    avg_loss.append(los / n_total_steps)\n",
    "    avg_accuracy_train.append(accuracy_train / n_total_steps)\n",
    "    avg_precision_train.append(precision_train / n_total_steps)\n",
    "    avg_sensitivity_train.append(sensitivity_train / n_total_steps)\n",
    "    avg_specificity_train.append(specificity_train / n_total_steps)\n",
    "    avg_f1_train.append(f1_train / n_total_steps)\n",
    "    avg_js_train.append(js_train / n_total_steps)\n",
    "    avg_dc_train.append(dc_train / n_total_steps)\n",
    "    epoch_store.append(epoch + 1)\n",
    "    los = 0\n",
    "    accuracy_train = 0\n",
    "    sensitivity_train = 0\n",
    "    specificity_train = 0\n",
    "    precision_train = 0\n",
    "    f1_train = 0\n",
    "    js_train = 0\n",
    "    dc_train = 0\n",
    "   \n",
    "    # For validation set:\n",
    "    \n",
    "    \n",
    "#     break\n",
    "datas = {'Average Training Accuracy': avg_accuracy_train,\n",
    "         'Average Training Loss': avg_loss,\n",
    "         'Average Training Precision': avg_precision_train,\n",
    "         'Average Training Sensitivity': avg_precision_train,\n",
    "         'Average Training Specificity': avg_specificity_train,\n",
    "         'Average Training F1': avg_f1_train,\n",
    "         'Average Training JS': avg_js_train,\n",
    "         'Average Training Dice Coefficient': avg_dc_train,\n",
    "         'Epochs': epoch_store\n",
    "         }\n",
    "\n",
    "df = pd.DataFrame(datas)\n",
    "df.to_csv(\"Attn_UNET_MODEL_METRICS_train.csv\")\n",
    "if opt.save_model:\n",
    "    torch.save(model.state_dict(), 'Attn_UNET_Model_2.pt')\n",
    "\n",
    "print('TRAIN 1 program run complete')\n",
    "print(f'Train datas: {datas}')\n",
    "\n",
    "# For validation DATA\n",
    "# los_val,accuracy_train_val ,sensitivity_train_val ,specificity_train_val ,precision_train_val ,f1_train_val ,js_train_val ,dc_train_val  = get_val_metrics(loader=val_loader,mod=model)\n",
    "# avg_loss_val.append(los_val)\n",
    "# avg_accuracy_val.append(accuracy_train_val)\n",
    "# avg_sensitivity_val.append(sensitivity_train_val)\n",
    "# avg_specificity_val.append(specificity_train_val)\n",
    "# avg_precision_val.append(precision_train_val)\n",
    "# avg_f1_val.append(f1_train_val)\n",
    "# avg_js_val.append(js_train_val)\n",
    "# avg_dc_val.append(dc_train_val)\n",
    "# #For VALIDATION DATA\n",
    "# datas_val = {'Average Val Accuracy': avg_accuracy_train,\n",
    "#          'Average Val Loss': avg_loss,\n",
    "#          'Average Val Precision': avg_precision_train,\n",
    "#          'Average Val Sensitivity': avg_precision_train,\n",
    "#          'Average Val Specificity': avg_specificity_train,\n",
    "#          'Average Val F1': avg_f1_train,\n",
    "#          'Average Val JS': avg_js_train,\n",
    "#          'Average Val Dice Coefficient': avg_dc_train,\n",
    "#          }\n",
    "\n",
    "# df = pd.DataFrame(datas_val)\n",
    "# df.to_csv(\"Attn_UNET_MODEL_METRICS_VAL.csv\")\n",
    "# print('VAL saving complete')\n",
    "# print(f'VAL data:{datas_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51902226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T08:31:07.321564Z",
     "iopub.status.busy": "2024-05-06T08:31:07.321216Z",
     "iopub.status.idle": "2024-05-06T08:31:08.476320Z",
     "shell.execute_reply": "2024-05-06T08:31:08.475415Z"
    },
    "papermill": {
     "duration": 1.931797,
     "end_time": "2024-05-06T08:31:08.478269",
     "exception": false,
     "start_time": "2024-05-06T08:31:06.546472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 1, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_list = ['/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000000/image.png',\n",
    "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000001/image.png',\n",
    "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000004/image.png']\n",
    "\n",
    "masks_list = ['/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000000/mask.png',\n",
    "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000001/mask.png',\n",
    "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000004/mask.png'\n",
    "]\n",
    "\n",
    "transform0 = A.Compose([\n",
    "    # A.Resize(width=256, height=256),\n",
    "    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "    A.Normalize(\n",
    "        mean=[0.0, 0.0, 0.0],\n",
    "        std=[1.0, 1.0, 1.0],\n",
    "        max_pixel_value=255.0,\n",
    "    ),\n",
    "    # ToTensorV2()\n",
    "])\n",
    "\n",
    "def img_mask_preprocessing(img_list=[],mask_list=[],ismask=False):\n",
    "    final_data_list = []\n",
    "    if ismask:\n",
    "        for index in range(len(img_list)):\n",
    "            img = Image.open(img_list[index])\n",
    "            mask1 = Image.open(mask_list[index]).convert('L')\n",
    "            img = np.array(img, dtype=np.uint8)\n",
    "            mask1 = np.asarray(mask1)\n",
    "            k = np.expand_dims(mask1, axis=-1)\n",
    "            mask = np.array(k, dtype=np.float32)\n",
    "            transformed = transform0(image=img, mask=mask)\n",
    "            p = transformed['image']\n",
    "            q = transformed['mask']\n",
    "            p = torch.from_numpy(p)\n",
    "            q = torch.from_numpy(q)\n",
    "            p = p.type(opt.dtype)\n",
    "            q = q.type(opt.dtype)\n",
    "            p = torch.reshape(p, (3, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "            q = torch.reshape(q, (1, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "            q = q/255.0    \n",
    "            final_data_list.append((p,q))  \n",
    "    else:\n",
    "        for index in range(len(img_list)):\n",
    "            img = Image.open(img_list[index])\n",
    "            img = np.array(img, dtype=np.float32)\n",
    "            transformed = transform0(image=img)\n",
    "            p = transformed['image']\n",
    "            p = torch.from_numpy(p)\n",
    "            p = p.type(opt.dtype)\n",
    "            p = torch.reshape(p, (3, IMAGE_WIDTH, IMAGE_HEIGHT)) \n",
    "            final_data_list.append(p)\n",
    "    \n",
    "#     unprocessed_image = []\n",
    "#     for i in range(len(img_list)):\n",
    "#         img = Image.open(img_list[index])\n",
    "#         img = np.array(img, dtype=np.uint8)\n",
    "#         unprocessed_image.append(img)\n",
    "    return final_data_list\n",
    "\n",
    "def plotting(ismask_=True,model=model):\n",
    "    data = img_mask_preprocessing(img_list=image_list,mask_list=masks_list,ismask=ismask_)\n",
    "    if ismask_:\n",
    "        fig,axes = plt.subplots(2,3)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1,3)\n",
    "    \n",
    "#     for idx, item in enumerate(images):\n",
    "#         item = item.cpu(\n",
    "#         axes[0, idx].imshow(item)\n",
    "#         axes[0, idx].set_title(('image'+str(idx+1)))\n",
    "        \n",
    "    for idx, k in enumerate(data):\n",
    "        if ismask_:\n",
    "            item,_ = k\n",
    "        else:\n",
    "            item = k\n",
    "        item = torch.unsqueeze(item,0)\n",
    "        # item = torch.unsqueeze(torch.squeeze(item),0).detach().cpu()\n",
    "        output = model(item)\n",
    "        output = output>0.3\n",
    "        print(output.size())\n",
    "        output = torch.unsqueeze(torch.squeeze(output),0).detach().cpu()\n",
    "        if ismask_:\n",
    "            axes[0, idx].imshow(output.permute(1,2,0))\n",
    "            axes[0, idx].set_title(('gen_mask'+str(idx+1)))\n",
    "        else:\n",
    "            axes[0,idx].imshow(output.permute(1,2,0))\n",
    "            axes[0,idx].set_title(('gen_mask'+str(idx+1)))\n",
    "    \n",
    "    if ismask_:\n",
    "        for idx, k in enumerate(data):\n",
    "            if ismask_:\n",
    "                _,item = k\n",
    "            item = item.detach().cpu()\n",
    "            axes[1, idx].imshow(item.permute(1,2,0))\n",
    "            axes[1, idx].set_title(('ground-truth'+str(idx+1)))\n",
    "    plt.show()\n",
    "    # plt.savefig('saved_plots/generated_masks.png')\n",
    "\n",
    "plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847f00d",
   "metadata": {
    "papermill": {
     "duration": 0.661201,
     "end_time": "2024-05-06T08:31:09.807467",
     "exception": false,
     "start_time": "2024-05-06T08:31:09.146266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4577258,
     "sourceId": 7814006,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34486.212236,
   "end_time": "2024-05-06T08:31:12.903893",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-05T22:56:26.691657",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
