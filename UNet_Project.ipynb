{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7814006,
          "sourceType": "datasetVersion",
          "datasetId": 4577258
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "UNet Project",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vibhoooo/Bio-Image-Segmentation/blob/main/UNet_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'unet-project-dataset-isic-2017:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4577258%2F7814006%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240709%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240709T203733Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5f02753f1150a69bae6602556e6fa877aae566f02c1f43120f076942961e61430c19d4857974ec2473cd7350845c9feab392c76c687a02891c34d978796a37361df90253b85d85a2b6d00896a5dac55cbd1225c91f09113f92bdc5ab8a79e60f12c7d604d6620b3fe0dc86379370343ec9ac19c8854ae2765dd04b99b7cfab6905551981ddaa938fa75011f2d3793138a5eb5c37085464110094e098ab6013fd2bbb0ea64473050d534abbe556f515dff9c956586c76670a14d5ec0450ff4e2dbc8694288c38be5b0597303bb88cac4368d2481798e6b0ae5ebd8f0324b6095b61a415e2bd94cbe73cb4bc0f719b98938952e9ce10cd41b5044b27c15f1491ff'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "c0q13e3gN02L"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:02.724237Z",
          "iopub.execute_input": "2024-05-05T22:54:02.724702Z",
          "iopub.status.idle": "2024-05-05T22:54:02.731277Z",
          "shell.execute_reply.started": "2024-05-05T22:54:02.724665Z",
          "shell.execute_reply": "2024-05-05T22:54:02.730077Z"
        },
        "trusted": true,
        "id": "h5kXGyWEN02P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "OmaJB6C4N02Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "import torch\n",
        "from skimage.morphology import label\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:02.733163Z",
          "iopub.execute_input": "2024-05-05T22:54:02.733582Z",
          "iopub.status.idle": "2024-05-05T22:54:15.067374Z",
          "shell.execute_reply.started": "2024-05-05T22:54:02.733538Z",
          "shell.execute_reply": "2024-05-05T22:54:15.06634Z"
        },
        "trusted": true,
        "id": "KBb5mZL5N02R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UTILS**"
      ],
      "metadata": {
        "id": "5DZWSLB1N02R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "UNet\n",
        "Common utility functions and classes\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Base Configuration class\n",
        "# Don't use this class directly. Instead, sub-class it and override\n",
        "\n",
        "class Config():\n",
        "    name = None\n",
        "\n",
        "    img_width = 256\n",
        "    img_height = 256\n",
        "\n",
        "    img_channel = 3\n",
        "\n",
        "    batch_size = 16\n",
        "\n",
        "    learning_rate = 1e-3\n",
        "    learning_momentum = 0.9\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    shuffle = False\n",
        "\n",
        "    def __init__(self):\n",
        "        self.IMAGE_SHAPE = np.array([\n",
        "            self.img_width, self.img_height, self.img_channel\n",
        "        ])\n",
        "\n",
        "    def display(self):\n",
        "        \"\"\"Display Configuration values\"\"\"\n",
        "        print(\"\\nConfigurations:\")\n",
        "        for a in dir(self):\n",
        "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
        "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "# Configurations\n",
        "\n",
        "class Option(Config):\n",
        "    \"\"\"Configuration for training on Kaggle Data Science Bowl 2018\n",
        "    Derived from the base Config class and overrides specific values\n",
        "    \"\"\"\n",
        "    name = \"DSB2018\"\n",
        "    img_height = 256\n",
        "    img_width = 256\n",
        "    # root dir of training and validation set\n",
        "    root_dir = 'combined'\n",
        "\n",
        "    # root dir of testing set\n",
        "    test_dir = 'testing_data'\n",
        "\n",
        "    # save segmenting results (prediction masks) to this folder\n",
        "    results_dir = 'results'\n",
        "\n",
        "    num_workers = 1  # number of threads for data loading\n",
        "    shuffle = True  # shuffle the data set\n",
        "    batch_size = 24  # GTX1060 3G Memory\n",
        "    epochs = 100  #number of epochs to train\n",
        "    is_train = True  # True for training, False for making prediction\n",
        "    save_model = True  # True for saving the model, False for not saving the model\n",
        "\n",
        "    n_gpu = 1  # number of GPUs\n",
        "\n",
        "    learning_rate = 0.0001  # learning rate\n",
        "    weight_decay = 1e-4  # weight decay\n",
        "\n",
        "    pin_memory = True  # use pinned (page-locked) memory. when using CUDA, set to True\n",
        "\n",
        "    is_cuda = torch.cuda.is_available()  # True --> GPU\n",
        "    num_gpus = torch.cuda.device_count()  # number of GPUs\n",
        "    checkpoint_dir = \"./checkpoint\"  # dir to save checkpoints\n",
        "    dtype = torch.cuda.FloatTensor if is_cuda else torch.Tensor  # data type\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Dataset orgnization:\n",
        "Read images and masks, combine separated mask into one\n",
        "Write images and combined masks into specific folder\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Utils(object):\n",
        "    \"\"\"\n",
        "    Initialize image parameters from DSB2018Config class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stage1_train_src, stage1_train_dest, stage1_test_src, stage1_test_dest):\n",
        "        self.opt = Option\n",
        "        self.stage1_train_src = stage1_train_src\n",
        "        self.stage1_train_dest = stage1_train_dest\n",
        "        self.stage1_test_src = stage1_test_src\n",
        "        self.stage1_test_dest = stage1_test_dest\n",
        "\n",
        "    # Combine all separated masks into one mask\n",
        "    def assemble_masks(self, path):\n",
        "        # mask = np.zeros((self.config.IMG_HEIGHT, self.config.IMG_WIDTH), dtype=np.uint8)\n",
        "        mask = None\n",
        "        for i, mask_file in enumerate(next(os.walk(os.path.join(path, 'masks')))[2]):\n",
        "            mask_ = Image.open(os.path.join(path, 'masks', mask_file)).convert(\"RGB\")\n",
        "            # mask_ = mask_.resize((self.config.IMG_HEIGHT, self.config.IMG_WIDTH))\n",
        "            mask_ = np.asarray(mask_)\n",
        "            if i == 0:\n",
        "                mask = mask_\n",
        "                continue\n",
        "            mask = mask | mask_\n",
        "        # mask = np.expand_dims(mask, axis=-1)\n",
        "        return mask\n",
        "\n",
        "    # read all training data and save them to other folder\n",
        "    def prepare_training_data(self):\n",
        "        # get imageId\n",
        "        train_ids = next(os.walk(self.stage1_train_src))[1]\n",
        "\n",
        "        # read training data\n",
        "        X_train = []\n",
        "        Y_train = []\n",
        "        print('reading training data starts...')\n",
        "        sys.stdout.flush()\n",
        "        for n, id_ in tqdm(enumerate(train_ids)):\n",
        "            path = os.path.join(self.stage1_train_src, id_)\n",
        "            dest = os.path.join(self.stage1_train_dest, id_)\n",
        "            img = Image.open(os.path.join(path, 'images', id_ + '.png')).convert(\"RGB\")\n",
        "            mask = self.assemble_masks(path)\n",
        "            os.mkdir(dest)\n",
        "            img.save(os.path.join(dest, 'image.png'))\n",
        "            Image.fromarray(mask).save(os.path.join(dest, 'mask.png'))\n",
        "\n",
        "        print('reading training data done...')\n",
        "\n",
        "    # read testing data and save them to other folder\n",
        "    def prepare_testing_data(self):\n",
        "        # get imageId\n",
        "        test_ids = next(os.walk(self.stage1_test_src))[1]\n",
        "        # read training data\n",
        "        print('reading testing data starts...')\n",
        "        sys.stdout.flush()\n",
        "        for n, id_ in tqdm(enumerate(test_ids)):\n",
        "            path = os.path.join(self.stage1_test_src, id_, 'images', id_ + '.png')\n",
        "            dest = os.path.join(self.stage1_test_dest, id_)\n",
        "            if not os.path.exists(dest):\n",
        "                os.mkdir(dest)\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img.save(os.path.join(dest, 'image.png'))\n",
        "\n",
        "        print('reading testing data done...')\n",
        "\n",
        "\n",
        "def compute_iou(predictions, img_ids, val_loader):\n",
        "    \"\"\"\n",
        "    compute IOU between two combined masks, this does not follow kaggle's evaluation\n",
        "    :return: IOU, between 0 and 1\n",
        "    \"\"\"\n",
        "    ious = []\n",
        "    for i in range(0, len(img_ids)):\n",
        "        pred = predictions[i]\n",
        "        img_id = img_ids[i]\n",
        "        mask_path = os.path.join(Option.root_dir, img_id, 'mask.png')\n",
        "        mask = np.asarray(Image.open(mask_path).convert('L'), dtype=np.bool_)\n",
        "        union = np.sum(np.logical_or(mask, pred))\n",
        "        intersection = np.sum(np.logical_and(mask, pred))\n",
        "        iou = intersection / union\n",
        "        ious.append(iou)\n",
        "    df = pd.DataFrame({'img_id': img_ids, 'iou': ious})\n",
        "    df.to_csv('IOU.csv', index=False)\n",
        "\n",
        "\n",
        "# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
        "def rle_encoding(x):\n",
        "    dots = np.where(x.T.flatten() == 1)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "\n",
        "def prob_to_rles(x, cutoff=0.5):\n",
        "    lab_img = label(x > cutoff)\n",
        "    for i in range(1, lab_img.max() + 1):\n",
        "        yield rle_encoding(lab_img == i)\n",
        "\n",
        "\n",
        "def encode_and_save(preds_test_upsampled, test_ids):\n",
        "    \"\"\"\n",
        "    Use run-length-encoding encode the prediction masks and save to csv file for submitting\n",
        "    :param preds_test_upsampled: list, for each elements, numpy array (Width, Height)\n",
        "    :param test_ids: list, for each elements, image id\n",
        "    :return:\n",
        "        save to csv file\n",
        "    \"\"\"\n",
        "    # save as imgs\n",
        "    for i in range(0, len(test_ids)):\n",
        "        path = os.path.join(Option.results_dir, test_ids[i])\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "        # Image.fromarray(preds_test_upsampled[i]).save(os.path.join(path,'prediction.png'))\n",
        "        plt.imsave(os.path.join(path, 'prediction.png'), preds_test_upsampled[i], cmap='gray')\n",
        "    # save as encoding\n",
        "    new_test_ids = []\n",
        "    rles = []\n",
        "    for n, id_ in enumerate(test_ids):\n",
        "        rle = list(prob_to_rles(preds_test_upsampled[n]))\n",
        "        rles.extend(rle)\n",
        "        new_test_ids.extend([id_] * len(rle))\n",
        "\n",
        "    sub = pd.DataFrame()\n",
        "    sub['ImageId'] = new_test_ids\n",
        "    sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n",
        "    sub.to_csv('sub-dsbowl2018.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:15.069796Z",
          "iopub.execute_input": "2024-05-05T22:54:15.070156Z",
          "iopub.status.idle": "2024-05-05T22:54:15.102635Z",
          "shell.execute_reply.started": "2024-05-05T22:54:15.070127Z",
          "shell.execute_reply": "2024-05-05T22:54:15.101657Z"
        },
        "trusted": true,
        "id": "tHsgLsSXN02R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA**"
      ],
      "metadata": {
        "id": "pXNB29_9N02S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMAGE_HEIGHT = 256\n",
        "# IMAGE_WIDTH = 256\n",
        "\n",
        "IMAGE_HEIGHT = 128\n",
        "IMAGE_WIDTH = 128\n",
        "\n",
        "opt = Option()\n",
        "\n",
        "transform0 = A.Compose([\n",
        "    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    # ToTensorV2()\n",
        "])\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
        "    A.HorizontalFlip(p=0.25),\n",
        "    A.VerticalFlip(p=0.25),\n",
        "    A.Rotate(p=0.25),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "class Dataset_Class(Dataset):\n",
        "    def __init__(self, root_dir, transform=transform0):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.folder_list = os.listdir(self.root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.folder_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(os.path.join(self.root_dir + '/' + self.folder_list[index] + '/' + \"image.png\")).convert(\"RGB\")\n",
        "        mask1 = Image.open(os.path.join(self.root_dir + '/' + self.folder_list[index] + '/' + \"mask.png\")).convert('L')\n",
        "        image = np.array(img, dtype=np.float32)\n",
        "        mask = np.array(mask1, dtype=np.float32)\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        # mask1 = np.asarray(mask1)\n",
        "        # k = np.expand_dims(mask1, axis=-1)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        image = torch.from_numpy(image)\n",
        "        mask = torch.from_numpy(mask)\n",
        "        image = image.type(opt.dtype)\n",
        "        mask = mask.type(opt.dtype)\n",
        "        image = torch.reshape(image, (3, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "        mask = torch.reshape(mask, (1, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "        mask = mask/255.0\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "def get_train_loader(root_training_dir='/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data'):\n",
        "    data = Dataset_Class(root_dir=root_training_dir)\n",
        "\n",
        "    BATCH_SIZE = opt.batch_size\n",
        "    dataloader_train = DataLoader(\n",
        "        data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    print(\"TRAIN DATALOADER DONE\")\n",
        "    return dataloader_train\n",
        "\n",
        "\n",
        "def get_val_loader(root_val_dir='/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/validation_data'):\n",
        "\n",
        "    data = Dataset_Class(root_dir=root_val_dir)\n",
        "\n",
        "    BATCH_SIZE = opt.batch_size\n",
        "    dataloader_train = DataLoader(\n",
        "        data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    print(\"VAL DATALOADER DONE\")\n",
        "    return dataloader_train\n",
        "\n",
        "\n",
        "# def get_test_loader(root_test_dir='testing_data'):\n",
        "\n",
        "#     data = Dataset_Class(root_dir=root_test_dir)\n",
        "#     BATCH_SIZE = opt.batch_size\n",
        "#     dataloader_train = DataLoader(\n",
        "#         data, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:15.104165Z",
          "iopub.execute_input": "2024-05-05T22:54:15.104642Z",
          "iopub.status.idle": "2024-05-05T22:54:15.121832Z",
          "shell.execute_reply.started": "2024-05-05T22:54:15.104608Z",
          "shell.execute_reply": "2024-05-05T22:54:15.12083Z"
        },
        "trusted": true,
        "id": "2iUeSJfhN02T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODELS**"
      ],
      "metadata": {
        "id": "ZX29JatCN02T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Functions\n",
        "''' when filter kernel= 3x3, padding=1 makes in&out matrix same size'''\n",
        "\n",
        "class MHCA(nn.Module):\n",
        "    def __init__(self, n_feats, ratio):\n",
        "        \"\"\"\n",
        "        MHCA spatial-channel attention module.\n",
        "        :param n_feats: The number of filter of the input.\n",
        "        :param ratio: Channel reduction ratio.\n",
        "        \"\"\"\n",
        "        super(MHCA, self).__init__()\n",
        "\n",
        "        out_channels = int(n_feats // ratio)\n",
        "\n",
        "        head_1 = [\n",
        "            nn.Conv2d(in_channels=n_feats, out_channels=out_channels, kernel_size=1, padding=0, bias=True),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=out_channels, out_channels=n_feats, kernel_size=1, padding=0, bias=True)\n",
        "        ]\n",
        "\n",
        "        kernel_size_sam = 3\n",
        "        head_2 = [\n",
        "            nn.Conv2d(in_channels=n_feats, out_channels=out_channels, kernel_size=kernel_size_sam, padding=0, bias=True),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=out_channels, out_channels=n_feats, kernel_size=kernel_size_sam, padding=0, bias=True)\n",
        "        ]\n",
        "\n",
        "        kernel_size_sam_2 = 5\n",
        "        head_3 = [\n",
        "            nn.Conv2d(in_channels=n_feats, out_channels=out_channels, kernel_size=kernel_size_sam_2, padding=0, bias=True),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=out_channels, out_channels=n_feats, kernel_size=kernel_size_sam_2, padding=0, bias=True)\n",
        "        ]\n",
        "\n",
        "        self.head_1 = nn.Sequential(*head_1)\n",
        "        self.head_2 = nn.Sequential(*head_2)\n",
        "        self.head_3 = nn.Sequential(*head_3)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res_h1 = self.head_1(x)\n",
        "        res_h2 = self.head_2(x)\n",
        "        res_h3 = self.head_3(x)\n",
        "        m_c = self.sigmoid(res_h1 + res_h2 + res_h3)\n",
        "        res = x * m_c\n",
        "        return res\n",
        "\n",
        "def conv_bn_relu(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "\n",
        "def down_pooling():\n",
        "    return nn.MaxPool2d(2)\n",
        "\n",
        "\n",
        "def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "# UNet class\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(\n",
        "            self, input_channels=3, nclasses=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNet, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        in_channels = input_channels\n",
        "        out_channels = nclasses\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class UNet2(nn.Module):\n",
        "    def __init__(self, input_channels, nclasses):\n",
        "        super().__init__()\n",
        "        # go down\n",
        "        self.conv1 = conv_bn_relu(input_channels, 16)\n",
        "        self.conv2 = conv_bn_relu(16, 32)\n",
        "        self.conv3 = conv_bn_relu(32, 64)\n",
        "        self.conv4 = conv_bn_relu(64, 128)\n",
        "        self.conv5 = conv_bn_relu(128, 256)\n",
        "        self.down_pooling = nn.MaxPool2d(2)\n",
        "\n",
        "        # go up\n",
        "        self.up_pool6 = up_pooling(256, 128)\n",
        "        self.conv6 = conv_bn_relu(256, 128)\n",
        "        self.up_pool7 = up_pooling(128, 64)\n",
        "        self.conv7 = conv_bn_relu(128, 64)\n",
        "        self.up_pool8 = up_pooling(64, 32)\n",
        "        self.conv8 = conv_bn_relu(64, 32)\n",
        "        self.up_pool9 = up_pooling(32, 16)\n",
        "        self.conv9 = conv_bn_relu(32, 16)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(16, nclasses, 1)\n",
        "\n",
        "        # test weight init\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # normalize input data\n",
        "        x = x / 255.\n",
        "        # go down\n",
        "        x1 = self.conv1(x)\n",
        "        print(\"X1 shape: \", x1.size())\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "        print(\"X2 Shape: \", x2.size())\n",
        "        p2 = self.down_pooling(x2)\n",
        "        x3 = self.conv3(p2)\n",
        "        print(\"X3 Shape: \", x3.size())\n",
        "        p3 = self.down_pooling(x3)\n",
        "        x4 = self.conv4(p3)\n",
        "        print(\"X4 Shape: \", x4.size())\n",
        "        p4 = self.down_pooling(x4)\n",
        "        x5 = self.conv5(p4)\n",
        "        print(\"X5 Shape: \", x5.size())\n",
        "\n",
        "        # go up\n",
        "        p6 = self.up_pool6(x5)\n",
        "        x6 = torch.cat([p6, x4], dim=1)\n",
        "        x6 = self.conv6(x6)\n",
        "\n",
        "        p7 = self.up_pool7(x6)\n",
        "        x7 = torch.cat([p7, x3], dim=1)\n",
        "        x7 = self.conv7(x7)\n",
        "\n",
        "        p8 = self.up_pool8(x7)\n",
        "        x8 = torch.cat([p8, x2], dim=1)\n",
        "        x8 = self.conv8(x8)\n",
        "\n",
        "        p9 = self.up_pool9(x8)\n",
        "        x9 = torch.cat([p9, x1], dim=1)\n",
        "        x9 = self.conv9(x9)\n",
        "\n",
        "        output = self.conv10(x9)\n",
        "        output = F.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Code for Attention UNET\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up_conv(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(up_conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention_block(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(Attention_block, self).__init__()\n",
        "        self.W_g = nn.Sequential(\n",
        "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(F_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "\n",
        "        return x * psi\n",
        "\n",
        "\n",
        "class AttU_Net(nn.Module):\n",
        "    def __init__(self, input_channels=3, nclasses=1):\n",
        "        super(AttU_Net, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = conv_block(ch_in=input_channels, ch_out=64)\n",
        "        self.Conv2 = conv_block(ch_in=64, ch_out=128)\n",
        "        self.Conv3 = conv_block(ch_in=128, ch_out=256)\n",
        "        self.Conv4 = conv_block(ch_in=256, ch_out=512)\n",
        "        self.Conv5 = conv_block(ch_in=512, ch_out=1024)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
        "        self.Att5 = Attention_block(F_g=512, F_l=512, F_int=256)\n",
        "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
        "        self.Att4 = Attention_block(F_g=256, F_l=256, F_int=128)\n",
        "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
        "\n",
        "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
        "        self.Att3 = Attention_block(F_g=128, F_l=128, F_int=64)\n",
        "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
        "        self.Att2 = Attention_block(F_g=64, F_l=64, F_int=32)\n",
        "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(64, nclasses, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.Conv1(x)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.Conv2(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.Conv3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.Conv4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.Conv5(x5)\n",
        "\n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        x4 = self.Att5(g=d5, x=x4)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        d5 = self.Up_conv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        x3 = self.Att4(g=d4, x=x3)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Up_conv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        x2 = self.Att3(g=d3, x=x2)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Up_conv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        x1 = self.Att2(g=d2, x=x1)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Up_conv2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "\n",
        "        return d1\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Code for R2_UNET\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Recurrent_block(nn.Module):\n",
        "    def __init__(self, ch_out, t=2):\n",
        "        super(Recurrent_block, self).__init__()\n",
        "        self.t = t\n",
        "        self.ch_out = ch_out\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.t):\n",
        "\n",
        "            if i == 0:\n",
        "                x1 = self.conv(x)\n",
        "\n",
        "            x1 = self.conv(x + x1)\n",
        "        return x1\n",
        "\n",
        "\n",
        "class RRCNN_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, t=2):\n",
        "        super(RRCNN_block, self).__init__()\n",
        "        self.RCNN = nn.Sequential(\n",
        "            Recurrent_block(ch_out, t=t),\n",
        "            Recurrent_block(ch_out, t=t)\n",
        "        )\n",
        "        self.Conv_1x1 = nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Conv_1x1(x)\n",
        "        x1 = self.RCNN(x)\n",
        "        return x + x1\n",
        "\n",
        "\n",
        "class R2U_Net(nn.Module):\n",
        "    def __init__(self, input_channels=3, nclasses=1, t=2):\n",
        "        super(R2U_Net, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "        self.RRCNN1 = RRCNN_block(ch_in=input_channels, ch_out=64, t=t)\n",
        "\n",
        "        self.RRCNN2 = RRCNN_block(ch_in=64, ch_out=128, t=t)\n",
        "\n",
        "        self.RRCNN3 = RRCNN_block(ch_in=128, ch_out=256, t=t)\n",
        "\n",
        "        self.RRCNN4 = RRCNN_block(ch_in=256, ch_out=512, t=t)\n",
        "\n",
        "        self.RRCNN5 = RRCNN_block(ch_in=512, ch_out=1024, t=t)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
        "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512, t=t)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
        "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256, t=t)\n",
        "\n",
        "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
        "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128, t=t)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
        "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64, t=t)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(64, nclasses, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.RRCNN1(x)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.RRCNN2(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.RRCNN3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.RRCNN4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.RRCNN5(x5)\n",
        "\n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        d5 = self.Up_RRCNN5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Up_RRCNN4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Up_RRCNN3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Up_RRCNN2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "\n",
        "        return d1"
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:15.124142Z",
          "iopub.execute_input": "2024-05-05T22:54:15.124447Z",
          "iopub.status.idle": "2024-05-05T22:54:15.201374Z",
          "shell.execute_reply.started": "2024-05-05T22:54:15.124421Z",
          "shell.execute_reply": "2024-05-05T22:54:15.200366Z"
        },
        "trusted": true,
        "id": "AbWVhK29N02U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trans-UNet"
      ],
      "metadata": {
        "id": "1EHgZ8M-N02V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Modules\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_k = nn.Linear(dim, inner_dim , bias=False)\n",
        "        self.to_v = nn.Linear(dim, inner_dim , bias = False)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x_qkv):\n",
        "        b, n, _, h = *x_qkv.shape, self.heads\n",
        "\n",
        "        k = self.to_k(x_qkv)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h = h)\n",
        "\n",
        "        v = self.to_v(x_qkv)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h = h)\n",
        "\n",
        "        q = self.to_q(x_qkv[:, 0].unsqueeze(1))\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h = h)\n",
        "\n",
        "\n",
        "\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Vision Transformer\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, head_num):\n",
        "        super().__init__()\n",
        "\n",
        "        self.head_num = head_num\n",
        "        self.dk = (embedding_dim // head_num) ** (1 / 2)\n",
        "\n",
        "        self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
        "        self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        qkv = self.qkv_layer(x)\n",
        "\n",
        "        query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.head_num))\n",
        "        energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask, -np.inf)\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n",
        "\n",
        "        x = rearrange(x, \"b h t d -> b t (h d)\")\n",
        "        x = self.out_attention(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, embedding_dim, mlp_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp_layers = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(mlp_dim, embedding_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp_layers(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, head_num, mlp_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
        "        self.mlp = MLP(embedding_dim, mlp_dim)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _x = self.multi_head_attention(x)\n",
        "        _x = self.dropout(_x)\n",
        "        x = x + _x\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        _x = self.mlp(x)\n",
        "        x = x + _x\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_blocks = nn.ModuleList(\n",
        "            [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer_block in self.layer_blocks:\n",
        "            x = layer_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_dim, in_channels, embedding_dim, head_num, mlp_dim,\n",
        "                 block_num, patch_dim, classification=True, num_classes=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_dim = patch_dim\n",
        "        self.classification = classification\n",
        "        self.num_tokens = (img_dim // patch_dim) ** 2\n",
        "        print(f'Num of tokens: {self.num_tokens}')\n",
        "        self.token_dim = in_channels * (patch_dim ** 2)\n",
        "        print(f'Token dimension: {self.token_dim}')\n",
        "\n",
        "        self.projection = nn.Linear(self.token_dim, embedding_dim)\n",
        "        self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n",
        "        print(f'Embedding shape: {self.embedding.shape}')\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n",
        "\n",
        "        if self.classification:\n",
        "            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        img_patches = rearrange(x,\n",
        "                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
        "                                patch_x=self.patch_dim, patch_y=self.patch_dim)\n",
        "\n",
        "        batch_size, tokens, _ = img_patches.shape\n",
        "\n",
        "        project = self.projection(img_patches)\n",
        "        token = repeat(self.cls_token, 'b ... -> (b batch_size) ...',\n",
        "                       batch_size=batch_size)\n",
        "\n",
        "        patches = torch.cat([token, project], dim=1)\n",
        "        patches += self.embedding[:tokens + 1, :]\n",
        "\n",
        "        x = self.dropout(patches)\n",
        "        x = self.transformer(x)\n",
        "        x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Trans-UNet\n",
        "\"\"\"\n",
        "class EncoderBottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        width = int(out_channels * (base_width / 64))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
        "        self.norm1 = nn.BatchNorm2d(width)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n",
        "        self.norm2 = nn.BatchNorm2d(width)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(width, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.norm3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_down = self.downsample(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "        x = x + x_down\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderBottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, scale_factor=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, x_concat=None):\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        if x_concat is not None:\n",
        "            x = torch.cat([x_concat, x], dim=1)\n",
        "\n",
        "        x = self.layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
        "        self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
        "        self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
        "\n",
        "        self.vit_img_dim = img_dim // patch_dim\n",
        "        print(f'vit_img_dim = img_dim//patch_dim\\n={img_dim}/{patch_dim} = {self.vit_img_dim}')\n",
        "        self.vit = ViT(self.vit_img_dim, out_channels * 8, out_channels * 8,\n",
        "                       head_num, mlp_dim, block_num, patch_dim=1, classification=False)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm2 = nn.BatchNorm2d(512)\n",
        "\n",
        "    def forward(self, x):\n",
        "#         print(f'Shape of x: {x.shape}')\n",
        "        x = self.conv1(x)\n",
        "#         print(f'Shape of x after conv1: {x.shape}')\n",
        "        x = self.norm1(x)\n",
        "#         print(f'Shape after norm1: {x.shape}')\n",
        "        x1 = self.relu(x)\n",
        "\n",
        "        x2 = self.encoder1(x1)\n",
        "#         print(f'Shape of x after encoder1: {x.shape}')\n",
        "        x3 = self.encoder2(x2)\n",
        "#         print(f'Shape of x after encoder2: {x.shape}')\n",
        "        x = self.encoder3(x3)\n",
        "#         print(f'Shape of x after encoder3: {x.shape}')\n",
        "        x = self.vit(x)\n",
        "#         print(f'Shape of x after vit: {x.shape}')\n",
        "        x = rearrange(x, \"b (x y) c -> b c x y\", x=self.vit_img_dim, y=self.vit_img_dim)\n",
        "#         print(f'Shape of x after vit rearrangement: {x.shape}')\n",
        "        x = self.conv2(x)\n",
        "#         print(f'Shape of x after conv2 on vit-rearranged output: {x.shape}')\n",
        "        x = self.norm2(x)\n",
        "#         print(f'Shape of x after norm2: {x.shape}')\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x, x1, x2, x3\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, out_channels, class_num):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n",
        "        self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n",
        "        self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n",
        "        self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, x1, x2, x3):\n",
        "#         print('Entered Decoder')\n",
        "        x = self.decoder1(x, x3)\n",
        "        x = self.decoder2(x, x2)\n",
        "        x = self.decoder3(x, x1)\n",
        "        x = self.decoder4(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransUNet(nn.Module):\n",
        "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(img_dim, in_channels, out_channels,\n",
        "                               head_num, mlp_dim, block_num, patch_dim)\n",
        "\n",
        "        self.decoder = Decoder(out_channels, class_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x1, x2, x3 = self.encoder(x)\n",
        "        x = self.decoder(x, x1, x2, x3)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:15.203186Z",
          "iopub.execute_input": "2024-05-05T22:54:15.203565Z",
          "iopub.status.idle": "2024-05-05T22:54:15.545745Z",
          "shell.execute_reply.started": "2024-05-05T22:54:15.203528Z",
          "shell.execute_reply": "2024-05-05T22:54:15.544829Z"
        },
        "trusted": true,
        "id": "BhhUM2M7N02V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Definition"
      ],
      "metadata": {
        "id": "DrVhnbqzN02W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(SR, GT, threshold=0.5):\n",
        "    SR = SR > threshold\n",
        "    GT = GT == torch.max(GT)\n",
        "    corr = torch.sum(SR == GT)\n",
        "    tensor_size = SR.size(0) * SR.size(1) * SR.size(2) * SR.size(3)\n",
        "    acc = float(corr) / float(tensor_size)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def get_sensitivity(SR, GT, threshold=0.5):\n",
        "    # Sensitivity == Recall\n",
        "    SR = SR > threshold\n",
        "    GT = GT > threshold\n",
        "\n",
        "    # TP : True Positive\n",
        "    # FN : False Negative\n",
        "    TP = torch.logical_and(SR, GT)\n",
        "    FN = torch.logical_and((SR == 0), (GT == 1))\n",
        "\n",
        "    SE = float(torch.sum(TP)) / (float(torch.sum(TP + FN)) + 1e-6)\n",
        "\n",
        "    return SE\n",
        "\n",
        "\n",
        "def get_specificity(SR, GT, threshold=0.5):\n",
        "    SR = SR > threshold\n",
        "    GT = GT > threshold\n",
        "\n",
        "    # TN : True Negative\n",
        "    # FP : False Positive\n",
        "    TN = torch.logical_and((SR == 0), (GT == 0))\n",
        "    FP = torch.logical_and((SR == 1), (GT == 0))\n",
        "\n",
        "    SP = float(torch.sum(TN)) / (float(torch.sum(TN + FP)) + 1e-6)\n",
        "\n",
        "    return SP\n",
        "\n",
        "\n",
        "def get_precision(SR, GT, threshold=0.5):\n",
        "    SR = SR > threshold\n",
        "    GT = GT > threshold\n",
        "\n",
        "    # TP : True Positive\n",
        "    # FP : False Positive\n",
        "    TP = torch.logical_and(SR, GT)\n",
        "    P = SR\n",
        "\n",
        "    PC = float(torch.sum(TP)) / (float(torch.sum(P)) + 1e-6)\n",
        "\n",
        "    return PC\n",
        "\n",
        "\n",
        "def get_F1(SR, GT, threshold=0.5):\n",
        "    # Sensitivity == Recall\n",
        "    SE = get_sensitivity(SR, GT, threshold=threshold)\n",
        "    PC = get_precision(SR, GT, threshold=threshold)\n",
        "\n",
        "    F1 = 2 * SE * PC / (SE + PC + 1e-6)\n",
        "\n",
        "    return F1\n",
        "\n",
        "\n",
        "def get_JS(SR, GT, threshold=0.5):\n",
        "    # JS : Jaccard similarity\n",
        "    SR = SR > threshold\n",
        "    GT = GT > threshold\n",
        "    Inter = torch.sum(torch.logical_and(SR, GT))\n",
        "    Union = torch.sum(torch.logical_or(SR, GT))\n",
        "\n",
        "    JS = float(Inter) / (float(Union) + 1e-6)\n",
        "\n",
        "    return JS\n",
        "\n",
        "\n",
        "def get_DC(SR, GT, threshold=0.5):\n",
        "    # DC : Dice Coefficient\n",
        "    SR = SR > threshold\n",
        "    GT = GT > threshold\n",
        "    Inter = torch.sum(torch.logical_and(SR, GT)).item()\n",
        "    Union = torch.sum(torch.logical_or(SR, GT)).item()\n",
        "    DC = float(2 * Inter) / (float(Inter + Union) + 1e-6)\n",
        "\n",
        "    return DC"
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:15.547009Z",
          "iopub.execute_input": "2024-05-05T22:54:15.547432Z",
          "iopub.status.idle": "2024-05-05T22:54:15.563207Z",
          "shell.execute_reply.started": "2024-05-05T22:54:15.547394Z",
          "shell.execute_reply": "2024-05-05T22:54:15.562385Z"
        },
        "trusted": true,
        "id": "CsIvE4ovN02W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAIN**"
      ],
      "metadata": {
        "id": "CcXz7SA8N02W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_val_metrics(mod, loader):\n",
        "    mod.eval()\n",
        "    n_total_steps = len(train_loader)\n",
        "    los_vall = 0\n",
        "    accuracy_vall = 0\n",
        "    sensitivity_vall = 0\n",
        "    specificity_vall = 0\n",
        "    precision_vall = 0\n",
        "    f1_vall = 0\n",
        "    js_vall = 0\n",
        "    dc_vall = 0\n",
        "    print('Entering the loop')\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data.type(opt.dtype)), Variable(target.type(opt.dtype))\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        los_vall += loss.item()\n",
        "        accuracy_vall += get_accuracy(output, target)\n",
        "        sensitivity_vall += get_sensitivity(output, target)\n",
        "        specificity_vall += get_specificity(output, target)\n",
        "        precision_vall += get_precision(output, target)\n",
        "        f1_vall += get_F1(output, target)\n",
        "        js_vall += get_JS(output, target)\n",
        "        dc_vall += get_DC(output, target)\n",
        "    los_vall = los_vall / n_total_steps\n",
        "    accuracy_vall = accuracy_vall / n_total_steps\n",
        "    sensitivity_vall = sensitivity_vall / n_total_steps\n",
        "    specificity_vall = specificity_vall / n_total_steps\n",
        "    precision_vall = precision_vall / n_total_steps\n",
        "    f1_vall = f1_vall / n_total_steps\n",
        "    js_vall = js_vall / n_total_steps\n",
        "    dc_vall = dc_vall / n_total_steps\n",
        "    return los_vall, accuracy_vall, sensitivity_vall, specificity_vall, precision_vall, f1_vall, js_vall, dc_vall\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "opt = Option()\n",
        "# model = R2U_Net(input_channels=3, nclasses=1)\n",
        "model = TransUNet(img_dim=128,\n",
        "                  in_channels=3,\n",
        "                  out_channels=128,\n",
        "                  head_num=4,\n",
        "                  mlp_dim=512,\n",
        "                  block_num=8,\n",
        "                  patch_dim=16,\n",
        "                  class_num=1)\n",
        "model.to(device)\n",
        "# train_loader = get_val_loader()\n",
        "optimizer = optim.Adam(model.parameters(), lr=opt.learning_rate)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 50, 0.00001)\n",
        "criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "train_loader = get_train_loader()\n",
        "val_loader = get_val_loader()\n",
        "\n",
        "los = 0\n",
        "num_batches = 0\n",
        "accuracy_train = 0\n",
        "sensitivity_train = 0\n",
        "specificity_train = 0\n",
        "precision_train = 0\n",
        "f1_train = 0\n",
        "js_train = 0\n",
        "dc_train = 0\n",
        "\n",
        "los_val = 0\n",
        "accuracy_train_val = 0\n",
        "sensitivity_train_val = 0\n",
        "specificity_train_val = 0\n",
        "precision_train_val = 0\n",
        "f1_train_val = 0\n",
        "js_train_val = 0\n",
        "dc_train_val = 0\n",
        "\n",
        "avg_loss = []\n",
        "avg_accuracy_train = []\n",
        "avg_sensitivity_train = []\n",
        "avg_specificity_train = []\n",
        "avg_precision_train = []\n",
        "avg_f1_train = []\n",
        "avg_js_train = []\n",
        "avg_dc_train = []\n",
        "epoch_store = []\n",
        "\n",
        "avg_loss_val = []\n",
        "avg_accuracy_val = []\n",
        "avg_sensitivity_val = []\n",
        "avg_specificity_val = []\n",
        "avg_precision_val = []\n",
        "avg_f1_val = []\n",
        "avg_js_val = []\n",
        "avg_dc_val = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "print('Entering the loop')\n",
        "for epoch in range(0, opt.epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # data = sample_batched['image']\n",
        "        # target = sample_batched['mask']\n",
        "        data, target = Variable(data.type(opt.dtype)), Variable(target.type(opt.dtype))\n",
        "        # print(f'Data max is: {data.max()}')\n",
        "        # print(f'Target max is: {target.max()}')\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "#         output = torch.sigmoid(output)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch [{epoch + 1}/{opt.epochs}], Step [{batch_idx + 1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "        los += loss.item()\n",
        "        accuracy_train += get_accuracy(output, target)\n",
        "        sensitivity_train += get_sensitivity(output, target)\n",
        "        specificity_train += get_specificity(output, target)\n",
        "        precision_train += get_precision(output, target)\n",
        "        f1_train += get_F1(output, target)\n",
        "        js_train += get_JS(output, target)\n",
        "        dc_train += get_DC(output, target)\n",
        "#         break\n",
        "    scheduler.step()\n",
        "    avg_loss.append(los / n_total_steps)\n",
        "    avg_accuracy_train.append(accuracy_train / n_total_steps)\n",
        "    avg_precision_train.append(precision_train / n_total_steps)\n",
        "    avg_sensitivity_train.append(sensitivity_train / n_total_steps)\n",
        "    avg_specificity_train.append(specificity_train / n_total_steps)\n",
        "    avg_f1_train.append(f1_train / n_total_steps)\n",
        "    avg_js_train.append(js_train / n_total_steps)\n",
        "    avg_dc_train.append(dc_train / n_total_steps)\n",
        "    epoch_store.append(epoch + 1)\n",
        "    los = 0\n",
        "    accuracy_train = 0\n",
        "    sensitivity_train = 0\n",
        "    specificity_train = 0\n",
        "    precision_train = 0\n",
        "    f1_train = 0\n",
        "    js_train = 0\n",
        "    dc_train = 0\n",
        "\n",
        "    # For validation set:\n",
        "\n",
        "\n",
        "#     break\n",
        "datas = {'Average Training Accuracy': avg_accuracy_train,\n",
        "         'Average Training Loss': avg_loss,\n",
        "         'Average Training Precision': avg_precision_train,\n",
        "         'Average Training Sensitivity': avg_precision_train,\n",
        "         'Average Training Specificity': avg_specificity_train,\n",
        "         'Average Training F1': avg_f1_train,\n",
        "         'Average Training JS': avg_js_train,\n",
        "         'Average Training Dice Coefficient': avg_dc_train,\n",
        "         'Epochs': epoch_store\n",
        "         }\n",
        "\n",
        "df = pd.DataFrame(datas)\n",
        "df.to_csv(\"Attn_UNET_MODEL_METRICS_train.csv\")\n",
        "if opt.save_model:\n",
        "    torch.save(model.state_dict(), 'Attn_UNET_Model_2.pt')\n",
        "\n",
        "print('TRAIN 1 program run complete')\n",
        "print(f'Train datas: {datas}')\n",
        "\n",
        "# For validation DATA\n",
        "# los_val,accuracy_train_val ,sensitivity_train_val ,specificity_train_val ,precision_train_val ,f1_train_val ,js_train_val ,dc_train_val  = get_val_metrics(loader=val_loader,mod=model)\n",
        "# avg_loss_val.append(los_val)\n",
        "# avg_accuracy_val.append(accuracy_train_val)\n",
        "# avg_sensitivity_val.append(sensitivity_train_val)\n",
        "# avg_specificity_val.append(specificity_train_val)\n",
        "# avg_precision_val.append(precision_train_val)\n",
        "# avg_f1_val.append(f1_train_val)\n",
        "# avg_js_val.append(js_train_val)\n",
        "# avg_dc_val.append(dc_train_val)\n",
        "# #For VALIDATION DATA\n",
        "# datas_val = {'Average Val Accuracy': avg_accuracy_train,\n",
        "#          'Average Val Loss': avg_loss,\n",
        "#          'Average Val Precision': avg_precision_train,\n",
        "#          'Average Val Sensitivity': avg_precision_train,\n",
        "#          'Average Val Specificity': avg_specificity_train,\n",
        "#          'Average Val F1': avg_f1_train,\n",
        "#          'Average Val JS': avg_js_train,\n",
        "#          'Average Val Dice Coefficient': avg_dc_train,\n",
        "#          }\n",
        "\n",
        "# df = pd.DataFrame(datas_val)\n",
        "# df.to_csv(\"Attn_UNET_MODEL_METRICS_VAL.csv\")\n",
        "# print('VAL saving complete')\n",
        "# print(f'VAL data:{datas_val}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-05T22:54:15.564464Z",
          "iopub.execute_input": "2024-05-05T22:54:15.564762Z",
          "iopub.status.idle": "2024-05-05T22:56:14.245159Z",
          "shell.execute_reply.started": "2024-05-05T22:54:15.564733Z",
          "shell.execute_reply": "2024-05-05T22:56:14.243906Z"
        },
        "trusted": true,
        "id": "WyXurrv9N02W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "image_list = ['/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000000/image.png',\n",
        "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000001/image.png',\n",
        "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000004/image.png']\n",
        "\n",
        "masks_list = ['/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000000/mask.png',\n",
        "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000001/mask.png',\n",
        "    '/kaggle/input/unet-project-dataset-isic-2017/Total Dataset/training_data/ISIC_0000004/mask.png'\n",
        "]\n",
        "\n",
        "transform0 = A.Compose([\n",
        "    # A.Resize(width=256, height=256),\n",
        "    A.Resize(width=IMAGE_WIDTH, height=IMAGE_HEIGHT),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    # ToTensorV2()\n",
        "])\n",
        "\n",
        "def img_mask_preprocessing(img_list=[],mask_list=[],ismask=False):\n",
        "    final_data_list = []\n",
        "    if ismask:\n",
        "        for index in range(len(img_list)):\n",
        "            img = Image.open(img_list[index])\n",
        "            mask1 = Image.open(mask_list[index]).convert('L')\n",
        "            img = np.array(img, dtype=np.uint8)\n",
        "            mask1 = np.asarray(mask1)\n",
        "            k = np.expand_dims(mask1, axis=-1)\n",
        "            mask = np.array(k, dtype=np.float32)\n",
        "            transformed = transform0(image=img, mask=mask)\n",
        "            p = transformed['image']\n",
        "            q = transformed['mask']\n",
        "            p = torch.from_numpy(p)\n",
        "            q = torch.from_numpy(q)\n",
        "            p = p.type(opt.dtype)\n",
        "            q = q.type(opt.dtype)\n",
        "            p = torch.reshape(p, (3, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "            q = torch.reshape(q, (1, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "            q = q/255.0\n",
        "            final_data_list.append((p,q))\n",
        "    else:\n",
        "        for index in range(len(img_list)):\n",
        "            img = Image.open(img_list[index])\n",
        "            img = np.array(img, dtype=np.float32)\n",
        "            transformed = transform0(image=img)\n",
        "            p = transformed['image']\n",
        "            p = torch.from_numpy(p)\n",
        "            p = p.type(opt.dtype)\n",
        "            p = torch.reshape(p, (3, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
        "            final_data_list.append(p)\n",
        "\n",
        "#     unprocessed_image = []\n",
        "#     for i in range(len(img_list)):\n",
        "#         img = Image.open(img_list[index])\n",
        "#         img = np.array(img, dtype=np.uint8)\n",
        "#         unprocessed_image.append(img)\n",
        "    return final_data_list\n",
        "\n",
        "def plotting(ismask_=True,model=model):\n",
        "    data = img_mask_preprocessing(img_list=image_list,mask_list=masks_list,ismask=ismask_)\n",
        "    if ismask_:\n",
        "        fig,axes = plt.subplots(2,3)\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1,3)\n",
        "\n",
        "#     for idx, item in enumerate(images):\n",
        "#         item = item.cpu(\n",
        "#         axes[0, idx].imshow(item)\n",
        "#         axes[0, idx].set_title(('image'+str(idx+1)))\n",
        "\n",
        "    for idx, k in enumerate(data):\n",
        "        if ismask_:\n",
        "            item,_ = k\n",
        "        else:\n",
        "            item = k\n",
        "        item = torch.unsqueeze(item,0)\n",
        "        # item = torch.unsqueeze(torch.squeeze(item),0).detach().cpu()\n",
        "        output = model(item)\n",
        "        output = output>0.3\n",
        "        print(output.size())\n",
        "        output = torch.unsqueeze(torch.squeeze(output),0).detach().cpu()\n",
        "        if ismask_:\n",
        "            axes[0, idx].imshow(output.permute(1,2,0))\n",
        "            axes[0, idx].set_title(('gen_mask'+str(idx+1)))\n",
        "        else:\n",
        "            axes[0,idx].imshow(output.permute(1,2,0))\n",
        "            axes[0,idx].set_title(('gen_mask'+str(idx+1)))\n",
        "\n",
        "    if ismask_:\n",
        "        for idx, k in enumerate(data):\n",
        "            if ismask_:\n",
        "                _,item = k\n",
        "            item = item.detach().cpu()\n",
        "            axes[1, idx].imshow(item.permute(1,2,0))\n",
        "            axes[1, idx].set_title(('ground-truth'+str(idx+1)))\n",
        "    plt.show()\n",
        "    # plt.savefig('saved_plots/generated_masks.png')\n",
        "\n",
        "plotting()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-05T22:56:14.246069Z",
          "iopub.status.idle": "2024-05-05T22:56:14.246431Z",
          "shell.execute_reply.started": "2024-05-05T22:56:14.24626Z",
          "shell.execute_reply": "2024-05-05T22:56:14.246275Z"
        },
        "trusted": true,
        "id": "5pqnoNn2N02X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zp2JqczZN02X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}